{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.realuser import RealUserEnvironment\n",
    "from server.nlu import NLU\n",
    "from typing import Tuple\n",
    "from data.dataset import GraphDataset, ReimburseGraphDataset, DataAugmentationLevel\n",
    "from data.parsers.parserValueProvider import ReimbursementRealValueBackend\n",
    "from data.parsers.answerTemplateParser import AnswerTemplateParser\n",
    "from data.parsers.systemTemplateParser import SystemTemplateParser\n",
    "from data.parsers.logicParser import LogicTemplateParser\n",
    "from utils.utils import AutoSkipMode\n",
    "from algorithm.dqn.dqn import CustomDQN\n",
    "import torch\n",
    "from data.cache import Cache\n",
    "from gymnasium import spaces, Env\n",
    "from encoding.state import StateEncoding\n",
    "\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "from hydra.core.config_store import ConfigStore\n",
    "from config import register_configs\n",
    "\n",
    "cs = ConfigStore.instance()\n",
    "register_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_name = \"reimburse_generated_v3_terminalobs\"\n",
    "ckpt_path = '/mount/arbeitsdaten/asr-2/vaethdk/cts_newcodebase_weights/reimburse_gen_v3_best'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_env(data: GraphDataset) -> RealUserEnvironment:\n",
    "    # setup data & parsers\n",
    "    answerParser = AnswerTemplateParser()\n",
    "    logicParser = LogicTemplateParser()\n",
    "    sysParser = SystemTemplateParser()\n",
    "    valueBackend = ReimbursementRealValueBackend(a1_laender=data.a1_countries, data=data.hotel_costs)\n",
    "    nlu = NLU()\n",
    "\n",
    "    # setup env\n",
    "    env = RealUserEnvironment(user_id=0,dataset=data, nlu=nlu,\n",
    "                        sys_token=\"SYSTEM\", usr_token=\"USER\", sep_token=\"\",\n",
    "                        max_steps=50, max_reward=150, user_patience=2,\n",
    "                        answer_parser=answerParser, logic_parser=logicParser, value_backend=valueBackend,\n",
    "                        auto_skip=AutoSkipMode.NONE, stop_on_invalid_skip=False)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_class(path:str):\n",
    "    from pydoc import locate\n",
    "    class_instance = locate(path)\n",
    "    return class_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## NOTE: assumes already unzipped checkpoint!\n",
    "from config import DialogLogLevel, WandbLogLevel\n",
    "from algorithm.dqn.her import HindsightExperienceReplayWrapper\n",
    "import gymnasium as gym\n",
    "\n",
    "def load_model(ckpt_path: str, cfg_name: str, device: str, data: GraphDataset) -> Tuple[CustomDQN, StateEncoding]:\n",
    "    # load config\n",
    "    cfg_path = \"./conf/\"\n",
    "\n",
    "    with initialize(version_base=None, config_path=cfg_path):\n",
    "        # parse config\n",
    "        print(\"Parsing config...\")\n",
    "        cfg = compose(config_name=cfg_name)\n",
    "        # print(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "        # disable logging\n",
    "        cfg.experiment.logging.dialog_log = DialogLogLevel.NONE\n",
    "        cfg.experiment.logging.wandb_log = WandbLogLevel.NONE\n",
    "        cfg.experiment.logging.log_interval = 9999999\n",
    "        cfg.experiment.logging.keep_checkpoints = 9\n",
    "\n",
    "        # load encodings\n",
    "        print(\"Loading encodings...\")\n",
    "        state_cfg = cfg.experiment.state\n",
    "        action_cfg = cfg.experiment.actions\n",
    "        cache = Cache(device=device, data=data, state_config=state_cfg, torch_compile=False)\n",
    "        encoding = StateEncoding(cache=cache, state_config=state_cfg, action_config=action_cfg, data=data)\n",
    "\n",
    "        # setup spaces\n",
    "        action_space = gym.spaces.Discrete(encoding.space_dims.num_actions)\n",
    "        if encoding.action_config.in_state_space == True:\n",
    "            # state space: max. node degree (#actions) x state dim\n",
    "            observation_space = gym.spaces.Box(low=float('-inf'), high=float('inf'), shape=(encoding.space_dims.num_actions, encoding.space_dims.state_vector,)) #, dtype=np.float32)\n",
    "        else:\n",
    "            observation_space = gym.spaces.Box(low=float('-inf'), high=float('inf'), shape=(encoding.space_dims.state_vector,)) #, dtype=np.float32)\n",
    "\n",
    "        class CustomEnv(Env):\n",
    "            def __init__(self, observation_space, action_space) -> None:\n",
    "                self.observation_space = observation_space\n",
    "                self.action_space = action_space\n",
    "        dummy_env = CustomEnv(observation_space=observation_space, action_space=action_space)\n",
    "\n",
    "        # setup model\n",
    "        print(\"Settung up model...\")\n",
    "        net_arch = OmegaConf.to_container(cfg.experiment.policy.net_arch)\n",
    "        net_arch['state_dims'] = encoding.space_dims # patch arguments\n",
    "        optim = OmegaConf.to_container(cfg.experiment.optimizer)\n",
    "        optim_class = to_class(optim.pop('class_path'))\n",
    "        lr = optim.pop('lr')\n",
    "        print(\"Optim ARGS:\", optim_class, lr, optim)\n",
    "        policy_kwargs = {\n",
    "            \"activation_fn\": to_class(cfg.experiment.policy.activation_fn),   \n",
    "            \"net_arch\": net_arch,\n",
    "            \"torch_compile\": cfg.experiment.torch_compile,\n",
    "            \"optimizer_class\": optim_class,\n",
    "            \"optimizer_kwargs\": optim\n",
    "        }\n",
    "        replay_buffer_kwargs = {\n",
    "            \"num_train_envs\": cfg.experiment.environment.num_train_envs,\n",
    "            \"batch_size\": cfg.experiment.algorithm.dqn.batch_size,\n",
    "            \"dataset\": data,\n",
    "            \"append_ask_action\": False,\n",
    "            # \"state_encoding\": state_encoding,\n",
    "            \"auto_skip\": AutoSkipMode.NONE,\n",
    "            \"normalize_rewards\": True,\n",
    "            \"stop_when_reaching_goal\": cfg.experiment.environment.stop_when_reaching_goal,\n",
    "            \"stop_on_invalid_skip\": cfg.experiment.environment.stop_on_invalid_skip,\n",
    "            \"max_steps\": cfg.experiment.environment.max_steps,\n",
    "            \"user_patience\": cfg.experiment.environment.user_patience,\n",
    "            \"sys_token\": cfg.experiment.environment.sys_token,\n",
    "            \"usr_token\": cfg.experiment.environment.usr_token,\n",
    "            \"sep_token\": cfg.experiment.environment.sep_token,\n",
    "            \"alpha\": cfg.experiment.algorithm.dqn.buffer.backend.alpha,\n",
    "            \"beta\": cfg.experiment.algorithm.dqn.buffer.backend.beta,\n",
    "            \"use_lap\": cfg.experiment.algorithm.dqn.buffer.backend.use_lap,\n",
    "            \"noise\": 0.0\n",
    "        }\n",
    "        replay_buffer_class = HindsightExperienceReplayWrapper\n",
    "        dqn_target_cls =  to_class(cfg.experiment.algorithm.dqn.targets._target_)\n",
    "        dqn_target_args = {'gamma': cfg.experiment.algorithm.dqn.gamma}\n",
    "        dqn_target_args.update(cfg.experiment.algorithm.dqn.targets) \n",
    "        model = CustomDQN(policy=to_class(cfg.experiment.policy._target_), policy_kwargs=policy_kwargs,\n",
    "                    target=dqn_target_cls(**dqn_target_args),\n",
    "                    seed=cfg.experiment.seed,\n",
    "                    env=dummy_env, \n",
    "                    batch_size=cfg.experiment.algorithm.dqn.batch_size,\n",
    "                    verbose=1, device=cfg.experiment.device,  \n",
    "                    learning_rate=lr, \n",
    "                    exploration_initial_eps=cfg.experiment.algorithm.dqn.eps_start, exploration_final_eps=cfg.experiment.algorithm.dqn.eps_end, exploration_fraction=cfg.experiment.algorithm.dqn.exploration_fraction,\n",
    "                    buffer_size=1, \n",
    "                    learning_starts=cfg.experiment.algorithm.dqn.warmup_turns,\n",
    "                    gamma=cfg.experiment.algorithm.dqn.gamma,\n",
    "                    train_freq=1, # how many rollouts to perform before training once (one rollout = num_train_envs steps)\n",
    "                    gradient_steps=max(cfg.experiment.environment.num_train_envs // cfg.experiment.training.every_steps, 1),\n",
    "                    target_update_interval=cfg.experiment.algorithm.dqn.target_network_update_frequency * cfg.experiment.environment.num_train_envs,\n",
    "                    max_grad_norm=cfg.experiment.algorithm.dqn.max_grad_norm,\n",
    "                    tensorboard_log=None,\n",
    "                    replay_buffer_class=replay_buffer_class,\n",
    "                    optimize_memory_usage=False,\n",
    "                    replay_buffer_kwargs=replay_buffer_kwargs,\n",
    "                    action_masking=cfg.experiment.actions.action_masking,\n",
    "                    actions_in_state_space=cfg.experiment.actions.in_state_space\n",
    "                ) \n",
    "        \n",
    "        # restore weights\n",
    "        print(\"Restoring weights...\")\n",
    "        ckpt_params = torch.load(f\"{ckpt_path}/policy.pth\", map_location=device)\n",
    "        model.policy.load_state_dict(ckpt_params)\n",
    "        model.policy.set_training_mode(False)\n",
    "        model.policy.eval()\n",
    "    return model, encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Dataset Statistics =====\n",
      "- files:  en/reimburse/test_graph.json en/reimburse/test_answers.json\n",
      "- synonyms: True\n",
      "- depth: 20  - degree: 13\n",
      "- answers: 162\n",
      "- questions: 173\n",
      "- loaded original data: True\n",
      "- loaded generated data: False\n",
      "- question limit: 0  - maximum loaded:  4\n",
      "- answer limit: 0  - maximum loaded:  4\n"
     ]
    }
   ],
   "source": [
    "data = ReimburseGraphDataset('en/reimburse/test_graph.json', 'en/reimburse/test_answers.json', use_answer_synonyms=True, augmentation=DataAugmentationLevel.NONE, resource_dir='resources')\n",
    "user_env = load_env(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing config...\n",
      "Loading encodings...\n",
      "Loading Embedding (caching: False) encoding.text.sbert.SentenceEmbeddings ...\n",
      "Building tree embedding for nodes...\n",
      "Done\n",
      "Space dimensions: StateDims(state_vector=3932, action_vector=1, state_action_subvector=783, num_actions=14)\n",
      "Settung up model...\n",
      "Optim ARGS: <class 'torch.optim.adam.Adam'> 0.0001 {}\n",
      "Using cuda:0 device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "HER BUFFER BACKEND PrioritizedLAPReplayBuffer\n",
      "HER ENV!! TOKENS: SYSTEM: USER: \n",
      "ARCHITECUTRE OptimizedModule(\n",
      "  (_orig_mod): CustomDuelingQNetworkWithIntentPrediction(\n",
      "    (shared_net): ModuleList(\n",
      "      (0): Linear(in_features=3149, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (action_input_net): ModuleList(\n",
      "      (0): Linear(in_features=783, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (value_net): ModuleList(\n",
      "      (0): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=1024, out_features=1, bias=True)\n",
      "    )\n",
      "    (advantage_net): ModuleList(\n",
      "      (0): Linear(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): Linear(in_features=1024, out_features=1, bias=True)\n",
      "    )\n",
      "    (intent_head): Sequential(\n",
      "      (0): Linear(in_features=4096, out_features=256, bias=True)\n",
      "      (1): SELU()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "      (3): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "ARCHITECUTRE OptimizedModule(\n",
      "  (_orig_mod): CustomDuelingQNetworkWithIntentPrediction(\n",
      "    (shared_net): ModuleList(\n",
      "      (0): Linear(in_features=3149, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (action_input_net): ModuleList(\n",
      "      (0): Linear(in_features=783, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (value_net): ModuleList(\n",
      "      (0): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=1024, out_features=1, bias=True)\n",
      "    )\n",
      "    (advantage_net): ModuleList(\n",
      "      (0): Linear(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): Linear(in_features=1024, out_features=1, bias=True)\n",
      "    )\n",
      "    (intent_head): Sequential(\n",
      "      (0): Linear(in_features=4096, out_features=256, bias=True)\n",
      "      (1): SELU()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "      (3): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Restoring weights...\n"
     ]
    }
   ],
   "source": [
    "model, state_encoding = load_model(ckpt_path=ckpt_path, cfg_name=cfg_name, device='cpu', data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_action(obs: dict) -> Tuple[int, bool]:\n",
    "    # encode observation\n",
    "    s = state_encoding.batch_encode(observation=[obs], sys_token=\"SYSTEM\", usr_token=\"USER\", sep_token=\"\", noise=0.0) \n",
    "    # predict action & intent\n",
    "    action, intent = model.predict(observation=s, deterministic=True)\n",
    "    action = int(action)\n",
    "    intent = intent.item()\n",
    "\n",
    "    return action, intent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What topic do you have questions about? You can either click on an answer from the suggested topics or enter your own text.\n",
      "  (policy: action 3, is faq: True)\n",
      "SKIPPING\n",
      "-> TO Travel Safety/ Travel Risk Management: What would you like to know about? Emergency help \n",
      " What to c\n",
      "  (done: False)\n",
      "  (policy: action 1, is faq: True)\n",
      "SKIPPING\n",
      "-> TO Content: Why was it introduced: To show the services provided both internally and from MDMedicus \n",
      " N\n",
      "  (done: False)\n",
      "  (policy: action 1, is faq: True)\n",
      "SKIPPING\n",
      "-> TO Do you have any further questions?\n",
      "  (done: False)\n",
      "  (policy: action 1, is faq: True)\n",
      "SKIPPING\n",
      "-> TO What topic do you have questions about? You can either click on an answer from the suggested topics \n",
      "  (done: False)\n",
      "  (policy: action 1, is faq: True)\n",
      "SKIPPING\n",
      "-> TO Are you going on an intracity trip or a business trip?\n",
      "  (done: False)\n",
      "  (policy: action 1, is faq: True)\n",
      "SKIPPING\n",
      "-> TO Did you get written permission from your supervisor?\n",
      "  (done: False)\n",
      "  (policy: action 2, is faq: True)\n",
      "SKIPPING\n",
      "-> TO What country are you traveling to?\n",
      "  (done: False)\n",
      "  (policy: action 0, is faq: True)\n",
      "ASKING What country are you traveling to?\n",
      "  (done: False)\n",
      "  (policy: action 1, is faq: True)\n",
      "SKIPPING\n",
      "-> TO What city are you traveling to?\n",
      "  (done: False)\n",
      "  (policy: action 1, is faq: True)\n",
      "SKIPPING\n",
      "-> TO {{COUNTRY\n",
      "  (done: False)\n",
      "  (policy: action 1, is faq: True)\n",
      "SKIPPING\n",
      "-> TO Please check the current COVID-19 travel warnings travel restrictions from the foreign ministry and \n",
      "  (done: False)\n",
      "  (policy: action 1, is faq: True)\n",
      "SKIPPING\n",
      "-> TO COVID-19: Business trips should be reduced to an absolute minimum and are only allowed when they are\n",
      "  (done: False)\n",
      "  (policy: action 1, is faq: True)\n",
      "SKIPPING\n",
      "-> TO Are you planning to extend your trip with private travel?\n",
      "  (done: False)\n",
      "  (policy: action 0, is faq: True)\n",
      "ASKING Are you planning to extend your trip with private travel?\n",
      "  (done: False)\n",
      "  (policy: action 1, is faq: True)\n",
      "SKIPPING\n",
      "-> TO {{PRIVATE_EXTENSION\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'connected_node'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/mount/arbeitsdaten/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/realusertest.ipynb Cell 9\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmadagaskarweihe/mount/arbeitsdaten/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/realusertest.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m action, intent \u001b[39m=\u001b[39m next_action(obs)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmadagaskarweihe/mount/arbeitsdaten/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/realusertest.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m  (policy: action \u001b[39m\u001b[39m{\u001b[39;00maction\u001b[39m}\u001b[39;00m\u001b[39m, is faq: \u001b[39m\u001b[39m{\u001b[39;00mintent\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bmadagaskarweihe/mount/arbeitsdaten/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/realusertest.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m obs, reward, done \u001b[39m=\u001b[39m user_env\u001b[39m.\u001b[39;49mstep(action\u001b[39m=\u001b[39;49maction)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmadagaskarweihe/mount/arbeitsdaten/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/realusertest.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m  (done: \u001b[39m\u001b[39m{\u001b[39;00mdone\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/mount/arbeitsdaten41/projekte/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/environment/base.py:323\u001b[0m, in \u001b[0;36mBaseEnv.step\u001b[0;34m(self, action, replayed_user_utterance)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39m# handle logic node auto-transitioning here\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m--> 323\u001b[0m     reward, logic_done, did_handle_logic_node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_logic_and_varupdate_nodes(reward)\n\u001b[1;32m    324\u001b[0m     \u001b[39mif\u001b[39;00m did_handle_logic_node:\n\u001b[1;32m    325\u001b[0m         done \u001b[39m=\u001b[39m logic_done\n",
      "File \u001b[0;32m/mount/arbeitsdaten41/projekte/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/environment/base.py:428\u001b[0m, in \u001b[0;36mBaseEnv.handle_logic_and_varupdate_nodes\u001b[0;34m(self, reward)\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    427\u001b[0m         default_answer \u001b[39m=\u001b[39m answer\n\u001b[0;32m--> 428\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_node \u001b[39m=\u001b[39m default_answer\u001b[39m.\u001b[39;49mconnected_node\n\u001b[1;32m    429\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_node:\n\u001b[1;32m    430\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_coverage[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_node\u001b[39m.\u001b[39mkey] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'connected_node'"
     ]
    }
   ],
   "source": [
    "obs = user_env.reset()\n",
    "done = False\n",
    "\n",
    "while not done and user_env.current_user_utterance != \"exit\":\n",
    "    action, intent = next_action(obs)\n",
    "    print(f\"  (policy: action {action}, is faq: {intent})\")\n",
    "    obs, reward, done = user_env.step(action=action)\n",
    "    print(f\"  (done: {done})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RealUserEnvironment' object has no attribute 'current_episode_log'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/mount/arbeitsdaten/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/realusertest.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bkapweihe/mount/arbeitsdaten/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/realusertest.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m user_env\u001b[39m.\u001b[39;49mcurrent_episode_log\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RealUserEnvironment' object has no attribute 'current_episode_log'"
     ]
    }
   ],
   "source": [
    "user_env.current_episode_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DialogNode.LOGIC(key: 16378316272591567, answers: 2, questions: 0)\n",
       "        - connected_node: None\n",
       "        - text: {{PRIVATE_EXTENSION\n",
       "        "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_env.current_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cts_en",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
