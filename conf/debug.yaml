defaults:
  - config_schema
  # dataset choice: train, test, joint (train + test)
  - dataset@experiment.training.dataset : train
  - dataset@experiment.validation.dataset: train
  
  # model, algorithm & optimizer
  - optimizer@experiment.optimizer: adam
  - net_arch@experiment.policy.net_arch: dqn/decoupled_dueling_dqn_with_intentprediction
  - algorithm@experiment.algorithm: dqn/dqn

  # embeddings
  - embeddings/text/model@experiment.state.node_text: roberta_sentencesim
  - embeddings/text/model@experiment.state.initial_user_utterance: roberta_sentencesim
  - embeddings/text/model@experiment.state.current_user_utterance: roberta_sentencesim
  - embeddings/text/model@experiment.state.action_text: roberta_sentencesim
  - embeddings/text/model@experiment.state.dialog_history: roberta_sentencesim

  # overwrite defaults loaded above with the following content from this file
  - _self_

experiment:
  device: cuda:0
  seed: 12345678
  cudnn_deterministic: false
  logging:
    # keep_checkpoints: 5
    wandb_log: NONE
    dialog_log: NONE
    log_interval: 1000
  training:
    noise: 0.3
    every_steps: 2
    steps: 10000
    dataset:
      use_answer_synonyms: true
  validation:
    noise: 0.0
    every_steps: 10000
    dialogs: 5
    dataset:
      use_answer_synonyms: true
  policy:
    _target_: algorithm.dqn.dqn.CustomDQNPolicy
    activation_fn: torch.nn.SELU
  optimizer:
    lr: 0.0001
  # model:
  #   hidden_layer_sizes: [1024, 1024]
  algorithm:
    dqn:
      batch_size: 2
      warmup_turns: 0
  environment:
    guided_free_ratio: 1.0
    auto_skip: NONE
    normalize_rewards: true
    max_steps: 50
    user_patience: 3
    stop_when_reaching_goal: true # stop after goal node was both reached + presented (ASK)
    stop_on_invalid_skip: false # stop when taking a wrong path
    num_train_envs: 1
    num_val_envs: 5
    num_test_envs: 0
    sys_token: "[SYS]"
    usr_token: "[USR]"
    sep_token: ""
    goal_distance_mode: FULL_DISTANCE # FULL_DISTANCE, INCREMENT_EVERY_N_EPISODES
    goal_distance_increment: 1000 # increment distance by 1 after N episodes - will be ignored if goal distance mode is different from INCREMENT_EVERY_N_EPISODES
  actions:
    in_state_space: true
    action_masking: true
  state:
    # TODO: make all of these classes, so we can easily plug everything together as a sequence of maps from property -> class -> sub-vector?
    last_system_action: true
    beliefstate: true
    node_position: true
    node_type: true
    node_text:
      active: true
    dialog_history:
      active: true
    current_user_utterance:
      active: true
    initial_user_utterance:
      active: true
    action_text:
      active: true
    action_position: true
    # TODO attentions    
    # TODO user_intent_prediction

  _target_: training.trainer.Trainer
