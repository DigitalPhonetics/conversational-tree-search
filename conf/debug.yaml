defaults:
  - config_schema
  # dataset choice: train, test, joint (train + test)
  - dataset@experiment.training.dataset : train
  - dataset@experiment.validation.dataset: train
  - dataset@experiment.testing.dataset: test
  
  # model, algorithm & optimizer
  - optimizer@experiment.optimizer: adam
  - optimizer/scheduler@experiment.optimizer.scheduler: halfcosine
  - net_arch@experiment.policy.net_arch: dqn/decoupled_dueling_dqn_with_intentprediction
  - algorithm@experiment.algorithm: dqn/dqn
  - algorithm/dqn/targets@experiment.algorithm.dqn.targets: muenchausen

  # embeddings
  - embeddings/text/model@experiment.state.node_text: mpnet_base
  - embeddings/text/model@experiment.state.initial_user_utterance: mpnet_base
  - embeddings/text/model@experiment.state.current_user_utterance: mpnet_base
  - embeddings/text/model@experiment.state.action_text: mpnet_base
  - embeddings/text/model@experiment.state.dialog_history: mpnet_base

  # overwrite defaults loaded above with the following content from this file
  - _self_

hydra:
  run:
    dir: ${run_dir:}
experiment:
  device: cuda:0
  seed: 12345678
  cudnn_deterministic: true
  torch_compile: false
  logging:
    keep_checkpoints: 0
    wandb_log: ONLINE #  NONE
    dialog_log: NONE
    log_interval: 2
  training:
    noise: 0.1
    every_steps: 2
    dataset:
      use_answer_synonyms: true
      augmentation: NONE
  validation:
    noise: 0.0
    every_steps: 500
    dialogs: 5
    dataset:
      use_answer_synonyms: true
      augmentation: NONE
  testing:
    noise: 0.0
    every_steps: 15000
    dialogs: 500
    dataset:
      use_answer_synonyms: true
  # testing:
  policy:
    _target_: algorithm.dqn.dqn.CustomDQNPolicy
    activation_fn: torch.nn.SELU
    net_arch:
      shared_layer_sizes: [4096, 4096, 4096]
      value_layer_sizes: [2048, 1024]
      advantage_layer_sizes: [4096, 2048, 1024]
      dropout_rate: 0.25
      normalization_layers: false
      intent_loss_weight: 0.1
      q_value_clipping: 10.0
  optimizer:
    scheduler:
      start_lr: 0.001
      end_lr: 0.0001
  # model:
  #   hidden_layer_sizes: [1024, 1024]
  algorithm:
    dqn:
      timesteps_per_reset: 250 # single timesteps (across all vec envs)
      reset_exploration_times: 3
      batch_size: 2
      target_network_update_frequency: 1 # train 85 times per patch -> 1 step for target update frequency == 85 live network updates
      max_grad_norm: 1.0
      gamma: 0.99
      warmup_turns: 2
      buffer:
        backend:
          buffer_size: 100
  environment:
    guided_free_ratio: 0.5
    auto_skip: NONE
    normalize_rewards: true
    max_steps: 50
    user_patience: 3
    stop_when_reaching_goal: true # stop after goal node was both reached + presented (ASK)
    stop_on_invalid_skip: false # stop when taking a wrong path
    num_train_envs: 2
    num_val_envs: 2
    num_test_envs: 2
    sys_token: "SYSTEM:"
    usr_token: "USER:"
    sep_token: ""
    goal_distance_mode: FULL_DISTANCE # FULL_DISTANCE, INCREMENT_EVERY_N_EPISODES
    goal_distance_increment: 100 # increment distance by 1 after N * #TRAIN_ENVS episodes - will be ignored if goal distance mode is different from INCREMENT_EVERY_N_EPISODES
  actions:
    in_state_space: true
    action_masking: true
  state:
    # TODO: make all of these classes, so we can easily plug everything together as a sequence of maps from property -> class -> sub-vector?
    last_system_action: true
    beliefstate: true
    node_position: true
    node_type: true
    node_text:
      active: true
    dialog_history:
      active: true
    current_user_utterance:
      active: true
    initial_user_utterance:
      active: true
    action_text:
      active: true
    action_position: true
    # TODO attentions    
    # TODO user_intent_prediction

  _target_: training.trainer.Trainer
