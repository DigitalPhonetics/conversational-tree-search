{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs/scratch/users/vaethdk/adviser_reisekosten/.env/lib64/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from data.dataset import GraphDataset, DataAugmentationLevel, DialogNode, NodeType\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension not installed.\n",
      "The safetensors archive passed at /mount/arbeitsdaten/asr-2/vaethdk/resources/weights/TheBloke--upstage-llama-30b-instruct-2048-GPTQ/gptq_model-4bit-32g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 23.70 GiB total capacity; 22.56 GiB already allocated; 4.69 MiB free; 22.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m use_triton \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m      6\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name_or_path,\n\u001b[1;32m      7\u001b[0m                                           use_fast\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m                                           cache_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/mount/arbeitsdaten/asr-2/vaethdk/resources/weights/\u001b[39m\u001b[39m\"\u001b[39m,)\n\u001b[0;32m----> 9\u001b[0m model \u001b[39m=\u001b[39m AutoGPTQForCausalLM\u001b[39m.\u001b[39;49mfrom_quantized(\u001b[39m\"\u001b[39;49m\u001b[39m/mount/arbeitsdaten/asr-2/vaethdk/resources/weights/TheBloke--upstage-llama-30b-instruct-2048-GPTQ\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m         \u001b[39m# model_basename=model_basename,\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m         \u001b[39m# revision=\"gptq-4bit-32g-actorder_True\",\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m         use_safetensors\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     13\u001b[0m         trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     14\u001b[0m         device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcuda:0\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     15\u001b[0m         use_triton\u001b[39m=\u001b[39;49muse_triton,\n\u001b[1;32m     16\u001b[0m         quantize_config\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[0;32m/fs/scratch/users/vaethdk/adviser_reisekosten/.env/lib64/python3.10/site-packages/auto_gptq/modeling/auto.py:105\u001b[0m, in \u001b[0;36mAutoGPTQForCausalLM.from_quantized\u001b[0;34m(cls, model_name_or_path, device_map, max_memory, device, low_cpu_mem_usage, use_triton, inject_fused_attention, inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors, trust_remote_code, warmup_triton, trainable, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39m# TODO: do we need this filtering of kwargs? @PanQiWei is there a reason we can't just pass all kwargs?\u001b[39;00m\n\u001b[1;32m    100\u001b[0m keywords \u001b[39m=\u001b[39m {\n\u001b[1;32m    101\u001b[0m     key: kwargs[key]\n\u001b[1;32m    102\u001b[0m     \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(signature(quant_func)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mkeys()) \u001b[39m+\u001b[39m huggingface_kwargs\n\u001b[1;32m    103\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m kwargs\n\u001b[1;32m    104\u001b[0m }\n\u001b[0;32m--> 105\u001b[0m \u001b[39mreturn\u001b[39;00m quant_func(\n\u001b[1;32m    106\u001b[0m     model_name_or_path\u001b[39m=\u001b[39;49mmodel_name_or_path,\n\u001b[1;32m    107\u001b[0m     device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m    108\u001b[0m     max_memory\u001b[39m=\u001b[39;49mmax_memory,\n\u001b[1;32m    109\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    110\u001b[0m     low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m    111\u001b[0m     use_triton\u001b[39m=\u001b[39;49muse_triton,\n\u001b[1;32m    112\u001b[0m     inject_fused_attention\u001b[39m=\u001b[39;49minject_fused_attention,\n\u001b[1;32m    113\u001b[0m     inject_fused_mlp\u001b[39m=\u001b[39;49minject_fused_mlp,\n\u001b[1;32m    114\u001b[0m     use_cuda_fp16\u001b[39m=\u001b[39;49muse_cuda_fp16,\n\u001b[1;32m    115\u001b[0m     quantize_config\u001b[39m=\u001b[39;49mquantize_config,\n\u001b[1;32m    116\u001b[0m     model_basename\u001b[39m=\u001b[39;49mmodel_basename,\n\u001b[1;32m    117\u001b[0m     use_safetensors\u001b[39m=\u001b[39;49muse_safetensors,\n\u001b[1;32m    118\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49mtrust_remote_code,\n\u001b[1;32m    119\u001b[0m     warmup_triton\u001b[39m=\u001b[39;49mwarmup_triton,\n\u001b[1;32m    120\u001b[0m     trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[1;32m    121\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkeywords\n\u001b[1;32m    122\u001b[0m )\n",
      "File \u001b[0;32m/fs/scratch/users/vaethdk/adviser_reisekosten/.env/lib64/python3.10/site-packages/auto_gptq/modeling/_base.py:874\u001b[0m, in \u001b[0;36mBaseGPTQForCausalLM.from_quantized\u001b[0;34m(cls, model_name_or_path, device_map, max_memory, device, low_cpu_mem_usage, use_triton, torch_dtype, inject_fused_attention, inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors, trust_remote_code, warmup_triton, trainable, **kwargs)\u001b[0m\n\u001b[1;32m    872\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m hasn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt fused attention module yet, will skip inject fused attention.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    873\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 874\u001b[0m         \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mfused_attn_module_type\u001b[39m.\u001b[39;49minject_to_model(\n\u001b[1;32m    875\u001b[0m             model,\n\u001b[1;32m    876\u001b[0m             use_triton\u001b[39m=\u001b[39;49muse_triton,\n\u001b[1;32m    877\u001b[0m             group_size\u001b[39m=\u001b[39;49mquantize_config\u001b[39m.\u001b[39;49mgroup_size,\n\u001b[1;32m    878\u001b[0m             use_cuda_fp16\u001b[39m=\u001b[39;49muse_cuda_fp16,\n\u001b[1;32m    879\u001b[0m             desc_act\u001b[39m=\u001b[39;49mquantize_config\u001b[39m.\u001b[39;49mdesc_act,\n\u001b[1;32m    880\u001b[0m             trainable\u001b[39m=\u001b[39;49mtrainable\n\u001b[1;32m    881\u001b[0m         )\n\u001b[1;32m    882\u001b[0m \u001b[39mif\u001b[39;00m inject_fused_mlp:\n\u001b[1;32m    883\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mfused_mlp_module_type \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/fs/scratch/users/vaethdk/adviser_reisekosten/.env/lib64/python3.10/site-packages/auto_gptq/nn_modules/fused_llama_attn.py:152\u001b[0m, in \u001b[0;36mFusedLlamaAttentionForQuantizedModel.inject_to_model\u001b[0;34m(cls, model, use_triton, group_size, use_cuda_fp16, desc_act, trainable, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m k_proj \u001b[39m=\u001b[39m m\u001b[39m.\u001b[39mk_proj\n\u001b[1;32m    150\u001b[0m v_proj \u001b[39m=\u001b[39m m\u001b[39m.\u001b[39mv_proj\n\u001b[0;32m--> 152\u001b[0m qweights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat([q_proj\u001b[39m.\u001b[39;49mqweight, k_proj\u001b[39m.\u001b[39;49mqweight, v_proj\u001b[39m.\u001b[39;49mqweight], dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    153\u001b[0m qzeros \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([q_proj\u001b[39m.\u001b[39mqzeros, k_proj\u001b[39m.\u001b[39mqzeros, v_proj\u001b[39m.\u001b[39mqzeros], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    154\u001b[0m scales \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([q_proj\u001b[39m.\u001b[39mscales, k_proj\u001b[39m.\u001b[39mscales, v_proj\u001b[39m.\u001b[39mscales], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 23.70 GiB total capacity; 22.56 GiB already allocated; 4.69 MiB free; 22.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model_name_or_path = \"TheBloke/upstage-llama-30b-instruct-2048-GPTQ\"\n",
    "model_basename = \"gptq_model-4bit--1g\"\n",
    "\n",
    "use_triton = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\n",
    "                                          use_fast=True,\n",
    "                                          cache_dir=\"/mount/arbeitsdaten/asr-2/vaethdk/resources/weights/\",)\n",
    "model = AutoGPTQForCausalLM.from_quantized(\"/mount/arbeitsdaten/asr-2/vaethdk/resources/weights/TheBloke--upstage-llama-30b-instruct-2048-GPTQ\",\n",
    "        # model_basename=model_basename,\n",
    "        # revision=\"gptq-4bit-32g-actorder_True\",\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=False,\n",
    "        device=\"cuda:0\",\n",
    "        use_triton=use_triton,\n",
    "        quantize_config=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(system: str, user: str, questions: List[str]) -> Tuple[str, List[Tuple[int, int]]]:\n",
    "    \"\"\" Returns:\n",
    "        Generated prompt,\n",
    "        List of start/end index of each question within the tokenized prompt,\n",
    "    \"\"\"\n",
    "    incremental_prompt = f\"\"\"\n",
    "### System:\n",
    "{system}\n",
    "\n",
    "### User:\n",
    "{user}\n",
    "\n",
    "### Assistant:\n",
    "\"\"\"\n",
    "    token_ranges = []\n",
    "    for i, question in enumerate(questions):\n",
    "        incremental_prompt += f\"{i+1}. \"\n",
    "        question_start_idx = tokenizer(incremental_prompt, return_tensors='pt').input_ids.size(-1) - 1\n",
    "        incremental_prompt += question\n",
    "        question_end_idx = tokenizer(incremental_prompt, return_tensors='pt').input_ids.size(-1)\n",
    "        token_ranges.append((question_start_idx, question_end_idx))\n",
    "        incremental_prompt += \"\\n\"\n",
    "    return incremental_prompt, token_ranges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"You are a helpful assistant creating a list of FAQ-style questions from given facts.\n",
    "Only generate questions that can be answered by the given facts, without any external knowledge.\n",
    "Remove some information, especially nouns and named entities, between generated questions.\n",
    "Use casual language.\n",
    "Order the generated paraphrases in a numbered list.\"\"\"\n",
    "\n",
    "def user(answer_text: str, num_paraphrases: int) -> str:\n",
    "    return f'Generate {num_paraphrases} FAQ-style questions from the fact: \"{answer_text}\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_token_probablities(node: DialogNode) -> Tuple[List[torch.FloatTensor], List[torch.LongTensor]]:\n",
    "    \"\"\" \n",
    "    Returns \n",
    "        A list of token probabilities (one tensor per question)\n",
    "        A list of token indices\n",
    "    \"\"\"\n",
    "    # generate prompt\n",
    "    prompt, token_ranges = generate_prompt(system=system, user=user(answer_text=node.text, num_paraphrases=len(node.questions)), questions=[q.text for q in node.questions])\n",
    "    with torch.no_grad():\n",
    "        # forward prompt\n",
    "        inputs = tokenizer(prompt, return_tensors='pt').input_ids.to(DEVICE)\n",
    "        logits = model(input_ids=inputs, labels=inputs).logits.squeeze(0) # tokens x vocabulary\n",
    "        # get probabilities from logits\n",
    "        probs = logits.softmax(-1).to('cpu') # tokens\n",
    "        # extract token probabilities for questions only\n",
    "        question_probs = [probs[start:end] for start, end in token_ranges]\n",
    "        token_indices = [inputs[start:end].to('cpu') for start, end in token_ranges]\n",
    "    return question_probs, token_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- not using synonyms\n",
      "===== Dataset Statistics =====\n",
      "- files:  resources/en/train_graph.json resources/en/train_answers.json\n",
      "- synonyms: False\n",
      "- depth: 20  - degree: 13\n",
      "- answers: 73\n",
      "- questions: 279\n",
      "- loaded original data: True\n",
      "- loaded generated data: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/80 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"addmm_impl_cpu_\" not implemented for 'Half'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m tqdm(human_data\u001b[39m.\u001b[39mnodes_by_type[NodeType\u001b[39m.\u001b[39mINFO]):\n\u001b[1;32m      4\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(node\u001b[39m.\u001b[39mquestions) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m----> 5\u001b[0m         question_probs, token_indices \u001b[39m=\u001b[39m calculate_token_probablities(node)\n\u001b[1;32m      6\u001b[0m         question_probs[node\u001b[39m.\u001b[39mkey] \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mp\u001b[39m\u001b[39m\"\u001b[39m: question_probs, \u001b[39m\"\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m\"\u001b[39m: token_indices}\n\u001b[1;32m      7\u001b[0m torch\u001b[39m.\u001b[39msave(question_probs, \u001b[39m\"\u001b[39m\u001b[39mhuman_question_probs.pt\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 12\u001b[0m, in \u001b[0;36mcalculate_token_probablities\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     10\u001b[0m     \u001b[39m# forward prompt\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     inputs \u001b[39m=\u001b[39m tokenizer(prompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39minput_ids\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[0;32m---> 12\u001b[0m     logits \u001b[39m=\u001b[39m model(input_ids\u001b[39m=\u001b[39;49minputs, labels\u001b[39m=\u001b[39;49minputs)\u001b[39m.\u001b[39mlogits\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m) \u001b[39m# tokens x vocabulary\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[39m# get probabilities from logits\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     probs \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39msoftmax(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m# tokens\u001b[39;00m\n",
      "File \u001b[0;32m/fs/scratch/users/vaethdk/adviser_reisekosten/.env/lib64/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/fs/scratch/users/vaethdk/adviser_reisekosten/.env/lib64/python3.10/site-packages/auto_gptq/modeling/_base.py:433\u001b[0m, in \u001b[0;36mBaseGPTQForCausalLM.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 433\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/fs/scratch/users/vaethdk/adviser_reisekosten/.env/lib64/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/fs/scratch/users/vaethdk/adviser_reisekosten/.env/lib64/python3.10/site-packages/transformers/models/llama/modeling_llama.py:806\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    803\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m    805\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 806\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m    807\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    808\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    809\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    810\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    811\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    812\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    813\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    814\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    815\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    816\u001b[0m )\n\u001b[1;32m    818\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    819\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpretraining_tp \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/fs/scratch/users/vaethdk/adviser_reisekosten/.env/lib64/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/fs/scratch/users/vaethdk/adviser_reisekosten/.env/lib64/python3.10/site-packages/transformers/models/llama/modeling_llama.py:693\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    685\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    686\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    687\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    691\u001b[0m     )\n\u001b[1;32m    692\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 693\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    694\u001b[0m         hidden_states,\n\u001b[1;32m    695\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    696\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    697\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    698\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    699\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    700\u001b[0m     )\n\u001b[1;32m    702\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    704\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/fs/scratch/users/vaethdk/adviser_reisekosten/.env/lib64/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/fs/scratch/users/vaethdk/adviser_reisekosten/.env/lib64/python3.10/site-packages/transformers/models/llama/modeling_llama.py:408\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    405\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    407\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    409\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    410\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    411\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    412\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    413\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    414\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    415\u001b[0m )\n\u001b[1;32m    416\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    418\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/fs/scratch/users/vaethdk/adviser_reisekosten/.env/lib64/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/fs/scratch/users/vaethdk/adviser_reisekosten/.env/lib64/python3.10/site-packages/auto_gptq/nn_modules/fused_llama_attn.py:53\u001b[0m, in \u001b[0;36mFusedLlamaAttentionForQuantizedModel.forward\u001b[0;34m(self, hidden_states, past_key_value, attention_mask, position_ids, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Input shape: Batch x Time x Channel\"\"\"\u001b[39;00m\n\u001b[1;32m     51\u001b[0m bsz, q_len, _ \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39msize()\n\u001b[0;32m---> 53\u001b[0m qkv_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mqkv_proj(hidden_states)\n\u001b[1;32m     54\u001b[0m query_states, key_states, value_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msplit(qkv_states, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     56\u001b[0m query_states \u001b[39m=\u001b[39m query_states\u001b[39m.\u001b[39mview(bsz, q_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m/fs/scratch/users/vaethdk/adviser_reisekosten/.env/lib64/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/fs/scratch/users/vaethdk/adviser_reisekosten/.env/lib64/python3.10/site-packages/auto_gptq/nn_modules/qlinear/qlinear_cuda.py:265\u001b[0m, in \u001b[0;36mQuantLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    263\u001b[0m             weights\u001b[39m.\u001b[39mappend(scale_i[g_idx_i\u001b[39m.\u001b[39mlong()] \u001b[39m*\u001b[39m (weight_i \u001b[39m-\u001b[39m zeros_i[g_idx_i\u001b[39m.\u001b[39mlong()]))\n\u001b[1;32m    264\u001b[0m         weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(weights,dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 265\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(x\u001b[39m.\u001b[39;49mhalf(), weights)\n\u001b[1;32m    266\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mhalf()\u001b[39m.\u001b[39mreshape(out_shape)\n\u001b[1;32m    267\u001b[0m out \u001b[39m=\u001b[39m out \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m out\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"addmm_impl_cpu_\" not implemented for 'Half'"
     ]
    }
   ],
   "source": [
    "human_data = GraphDataset('resources/en/train_graph.json', 'resources/en/train_answers.json', False, augmentation=DataAugmentationLevel.NONE)\n",
    "question_probs = {}\n",
    "for node in tqdm(human_data.nodes_by_type[NodeType.INFO]):\n",
    "    if len(node.questions) > 0:\n",
    "        question_probs, token_indices = calculate_token_probablities(node)\n",
    "        question_probs[node.key] = {\"p\": question_probs, \"tokens\": token_indices}\n",
    "torch.save(question_probs, \"human_question_probs.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loading questions from  resources/en/generated/train_questions.json\n",
      "- not using synonyms\n",
      "===== Dataset Statistics =====\n",
      "- files:  resources/en/train_graph.json resources/en/train_answers.json\n",
      "- synonyms: False\n",
      "- depth: 20  - degree: 13\n",
      "- answers: 73\n",
      "- questions: 1012\n",
      "- loaded original data: False\n",
      "- loaded generated data: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [02:17<00:00,  1.72s/it]\n"
     ]
    }
   ],
   "source": [
    "generated_data = GraphDataset('resources/en/train_graph.json', 'resources/en/train_answers.json', False, augmentation=DataAugmentationLevel.ARTIFICIAL_ONLY)\n",
    "question_probs = {}\n",
    "for node in tqdm(generated_data.nodes_by_type[NodeType.INFO]):\n",
    "    if len(node.questions) > 0:\n",
    "        question_probs[node.key] = calculate_token_probablities(node)\n",
    "torch.save(question_probs, \"generated_question_probs.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate pairwise simiarity between questions within one node \n",
    "# - for each dataset\n",
    "# - compare how \"diverse\" the questions are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32000])\n"
     ]
    }
   ],
   "source": [
    "# Calculate perplexity of human data \n",
    "human_probs = torch.load('human_question_probs.pt', map_location=lambda storage, loc: storage)\n",
    "human_perplexities = []\n",
    "for node_key in human_probs:\n",
    "    question_probs = human_probs[node_key] # list with probabilities per question\n",
    "    for p_question in question_probs:\n",
    "        print(p_question.size())\n",
    "        ppl = torch.exp(-(1./p_question.size(0)) * torch.log(p_question).sum(-1))\n",
    "        human_perplexities.append(ppl)\n",
    "        break\n",
    "    break\n",
    "human_perplexities = torch.cat(human_perplexities).tolist()\n",
    "\n",
    "# Calculate perplexity of generated data\n",
    "# - compare distributions\n",
    "\n",
    "# Also, compare lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7f722b2ce740>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAekAAAHqCAYAAAAgWrY5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAjwElEQVR4nO3de1TUdf7H8RcXAXcLyBuIYV7W0tRiA0Gsjqmc2LSLJzuZmZpr2UXNxK00L3SnrSwrLdZ2yzrJ6tqWp4yjq5jdpFTUygtubaWmzqgZjGkCwvf3Rz9nlxwRxmF4E8/HOXN+y3c+X+b9/f7Mp9+5QIjjOI4AAIA5oQ09AAAA8I1IAwBgFJEGAMAoIg0AgFFEGgAAo4g0AABGEWkAAIwi0gAAGEWkA8BxHHk8HvFzYQAAgUSkA+DQoUOKiYnRoUOHGnoUAMCvCJEGAMAoIg0AgFFEGgAAo4g0AABGEWkAAIwi0gAAGEWkAQAwikgDAGAUkQYAwCgiDQCAUUQaAACjiDQAAEYRaQAAjCLSAAAYRaQBADCKSAMAYBSRBgDAKCINAIBRRBoAAKOINAAARhFpAACMItIAABhFpAEAMIpIAwBgFJEGAMAoIg0AgFFEGgAAo4g0AABGEWkAAIwi0gAAGEWkAQAwikgDAGAUkQYAwCgiDQCAUUQaAACjiDQAAEYRaQAAjCLSAAAYRaQBADCKSAMAYBSRBgDAKCINAIBRRBoAAKOINAAARhFpAACMItIAABhFpAEAMIpIAwBgFJEGAMAoIg0AgFFEGgAAo4g0AABGEWkAAIwi0gAAGEWkAQAwqtFFeu7cuerQoYOioqKUlpamtWvX1rh+8eLF6tq1q6KiotSzZ0/l5+efdO3tt9+ukJAQzZ49O8BTAwBQd40q0osWLVJWVpays7O1YcMGXXjhhcrMzNS+fft8rl+zZo2GDRumMWPGaOPGjRo8eLAGDx6szZs3n7D2rbfe0ieffKKEhIT6PgwAAGolxHEcp6GHqK20tDT16tVLc+bMkSRVVVUpMTFREyZM0JQpU05YP3ToUB0+fFhLly71buvdu7eSkpKUm5vr3bZ7926lpaVp+fLlGjRokO6++27dfffdtZ7L4/EoJiZGpaWlio6O9v8AAQD4H43mSrq8vFxFRUXKyMjwbgsNDVVGRoYKCwt97lNYWFhtvSRlZmZWW19VVaURI0bonnvuUffu3etneAAA/BDe0APU1oEDB1RZWam4uLhq2+Pi4lRcXOxzH5fL5XO9y+Xyfv3nP/9Z4eHhuuuuu2o9S1lZmcrKyrxfezyeWu8LAEBtNZor6fpQVFSkZ599VvPnz1dISEit98vJyVFMTIz3lpiYWI9TAgCaqkYT6VatWiksLExut7vadrfbrfj4eJ/7xMfH17j+ww8/1L59+9S+fXuFh4crPDxcO3bs0OTJk9WhQ4eTzjJ16lSVlpZ6b7t27Tq9gwMAwIdGE+mIiAglJyeroKDAu62qqkoFBQVKT0/3uU96enq19ZK0YsUK7/oRI0bo888/16ZNm7y3hIQE3XPPPVq+fPlJZ4mMjFR0dHS1GwAAgdZoXpOWpKysLI0aNUopKSlKTU3V7NmzdfjwYY0ePVqSNHLkSLVr1045OTmSpIkTJ6pv376aNWuWBg0apIULF2r9+vWaN2+eJKlly5Zq2bJltcdo1qyZ4uPjdd555wX34AAA+IVGFemhQ4dq//79mjlzplwul5KSkrRs2TLvm8N27typ0ND/PjnQp08f5eXlafr06br//vvVpUsXLVmyRD169GioQwAAoNYa1eekreJz0gCA+tBoXpMGAKCpIdIAABhFpAEAMIpIAwBgFJEGAMAoIg0AgFFEGgAAo4g0AABGEWkAAIwi0gAAGEWkAQAwikgDAGAUkQYAwCgiDQCAUUQaAACjiDQAAEYRaQAAjCLSAAAYRaQBADCKSAMAYBSRBgDAKCINAIBRRBoAAKOINAAARhFpAACMItIAABhFpAEAMIpIAwBgFJEGAMAoIg0AgFFEGgAAo4g0AABGEWkAAIwi0gAAGEWkAQAwikgDAGAUkQYAwCgiDQCAUUQaAACjiDQAAEYRaQAAjCLSAAAYRaQBADCKSAMAYBSRBgDAKCINAIBRRBoAAKOINAAARhFpAACMItIAABhFpAEAMIpIAwBgFJEGAMAoIg0AgFFEGgAAo4g0AABGEWkAAIwi0gAAGEWkAQAwikgDAGAUkQYAwCgiDQCAUUQaAACjiDQAAEYRaQAAjCLSAAAYRaQBADCKSAMAYBSRBgDAKCINAIBRjS7Sc+fOVYcOHRQVFaW0tDStXbu2xvWLFy9W165dFRUVpZ49eyo/P997X0VFhe677z717NlTv/3tb5WQkKCRI0dqz5499X0YAACcUqOK9KJFi5SVlaXs7Gxt2LBBF154oTIzM7Vv3z6f69esWaNhw4ZpzJgx2rhxowYPHqzBgwdr8+bNkqQjR45ow4YNmjFjhjZs2KA333xT27dv19VXXx3MwwIAwKcQx3Gchh6ittLS0tSrVy/NmTNHklRVVaXExERNmDBBU6ZMOWH90KFDdfjwYS1dutS7rXfv3kpKSlJubq7Px1i3bp1SU1O1Y8cOtW/fvlZzeTwexcTEqLS0VNHR0X4cGQAAJ2o0V9Ll5eUqKipSRkaGd1toaKgyMjJUWFjoc5/CwsJq6yUpMzPzpOslqbS0VCEhIYqNjQ3I3AAA+Cu8oQeorQMHDqiyslJxcXHVtsfFxam4uNjnPi6Xy+d6l8vlc/3Ro0d13333adiwYTVeEZeVlamsrMz7tcfjqe1hAABQa43mSrq+VVRU6Prrr5fjOHrxxRdrXJuTk6OYmBjvLTExMUhTAgCakkYT6VatWiksLExut7vadrfbrfj4eJ/7xMfH12r98UDv2LFDK1asOOXrylOnTlVpaan3tmvXLj+OCACAmjWaSEdERCg5OVkFBQXebVVVVSooKFB6errPfdLT06utl6QVK1ZUW3880F9++aVWrlypli1bnnKWyMhIRUdHV7sBABBojeY1aUnKysrSqFGjlJKSotTUVM2ePVuHDx/W6NGjJUkjR45Uu3btlJOTI0maOHGi+vbtq1mzZmnQoEFauHCh1q9fr3nz5kn6OdDXXXedNmzYoKVLl6qystL7enWLFi0UERHRMAcKAIAaWaSHDh2q/fv3a+bMmXK5XEpKStKyZcu8bw7buXOnQkP/++RAnz59lJeXp+nTp+v+++9Xly5dtGTJEvXo0UOStHv3br399tuSpKSkpGqP9d577+myyy4LynEBAOBLo/qctFV8ThoAUB8azWvSAAA0NUQaAACjiDQAAEYRaQAAjCLSAAAYRaQBADCKSAMAYBSRBgDAKCINAIBRRBoAAKOINAAARhFpAACMItIAABhFpAEAMIpIAwBgFJEGAMAoIg0AgFFEGgAAo4g0AABGEWkAAIwi0gAAGEWkAQAwikgDAGAUkQYAwCgiDQCAUUQaAACjiDQAAEYRaQAAjCLSAAAYRaQBADCKSAMAYBSRBgDAKCINAIBRRBoAAKOINAAARhFpAACMItIAABhFpAEAMIpIAwBgFJEGAMAoIg0AgFFEGgAAo4g0AABGEWkAAIwi0gAAGEWkAQAwikgDAGAUkQYAwCgiDQCAUUQaAACjiDQAAEYRaQAAjCLSAAAYRaQBADCKSAMAYBSRBgDAKCINAIBRfkW6U6dO+v7770/YXlJSok6dOp32UAAAwM9If/vtt6qsrDxhe1lZmXbv3n3aQwEAACm8Lovffvtt7/9evny5YmJivF9XVlaqoKBAHTp0CNhwAAA0ZSGO4zi1XRwa+vOFd0hIiH65W7NmzdShQwfNmjVLV155ZWCnNM7j8SgmJkalpaWKjo5u6HEAAL8SdbqSrqqqkiR17NhR69atU6tWreplKAAAUMdIH/fNN98Eeg4AAPALfkVakgoKClRQUKB9+/Z5r7CPe/nll097MAAAmjq/Iv3ggw/qoYceUkpKitq2bauQkJBAzwUAQJPnV6Rzc3M1f/58jRgxItDzAACA/+fX56TLy8vVp0+fQM8CAAD+h1+RvuWWW5SXlxfoWQAAwP/w6+nuo0ePat68eVq5cqUuuOACNWvWrNr9Tz/9dECGAwCgKfMr0p9//rmSkpIkSZs3b652H28iAwAgMOr0E8fgGz9xDABQHxrdr6qcO3euOnTooKioKKWlpWnt2rU1rl+8eLG6du2qqKgo9ezZU/n5+dXudxxHM2fOVNu2bdW8eXNlZGToyy+/rM9DAACgVvx6urtfv341Pq29atUqvweqyaJFi5SVlaXc3FylpaVp9uzZyszM1Pbt29WmTZsT1q9Zs0bDhg1TTk6OrrzySuXl5Wnw4MHasGGDevToIUl64okn9Nxzz+nVV19Vx44dNWPGDGVmZmrr1q2Kioqql+MAAKA2/Hq6e9KkSdW+rqio0KZNm7R582aNGjVKzz77bMAG/F9paWnq1auX5syZI+nnnyWemJioCRMmaMqUKSesHzp0qA4fPqylS5d6t/Xu3VtJSUnKzc2V4zhKSEjQ5MmT9ac//UmSVFpaqri4OM2fP1833HBDrebi6W4AQH3w60r6mWee8bn9gQce0I8//nhaA51MeXm5ioqKNHXqVO+20NBQZWRkqLCw0Oc+hYWFysrKqrYtMzNTS5YskfTzzyB3uVzKyMjw3h8TE6O0tDQVFhaeNNJlZWUqKyvzfu3xePw9rBPs3btXe/fuDdj3AwAEVtu2bdW2bdugPJbfP7vbl5tuukmpqal66qmnAvltJUkHDhxQZWWl4uLiqm2Pi4tTcXGxz31cLpfP9S6Xy3v/8W0nW+NLTk6OHnzwwTofQ2385S9/qbfvDQA4fdnZ2XrggQeC8lgBjXRhYWGTeB136tSp1a7QPR6PEhMTA/K9b7vtNl199dUB+V4AgMAL1lW05Gekr7322mpfO46jvXv3av369ZoxY0ZABvulVq1aKSwsTG63u9p2t9ut+Ph4n/vEx8fXuP74/3W73dVOutvt9n4O3JfIyEhFRkb6cxinFMynUQAAtvn1EayYmJhqtxYtWuiyyy5Tfn6+srOzAz2jJCkiIkLJyckqKCjwbquqqlJBQYHS09N97pOenl5tvSStWLHCu75jx46Kj4+vtsbj8ejTTz896fcEACBY/LqSfuWVVwI9R61kZWVp1KhRSklJUWpqqmbPnq3Dhw9r9OjRkqSRI0eqXbt2ysnJkSRNnDhRffv21axZszRo0CAtXLhQ69ev17x58yT9/NPR7r77bj3yyCPq0qWL9yNYCQkJGjx4cIMcIwAAx53Wa9JFRUXatm2bJKl79+76/e9/H5ChTmbo0KHav3+/Zs6cKZfLpaSkJC1btsz7xq+dO3cqNPS/Tw706dNHeXl5mj59uu6//3516dJFS5Ys8X5GWpLuvfdeHT58WGPHjlVJSYkuueQSLVu2rEm8tg4AsM2vz0nv27dPN9xwg1avXq3Y2FhJUklJifr166eFCxeqdevWgZ7TND4nDQCoD369Jj1hwgQdOnRIW7Zs0cGDB3Xw4EFt3rxZHo9Hd911V6BnBACgSfLrSjomJkYrV65Ur169qm1fu3atLr/8cpWUlARqvkaBK2kAQH3w60q6qqrqhN8hLUnNmjVTVVXVaQ8FAAD8jHT//v01ceJE7dmzx7tt9+7dmjRpkgYMGBCw4QAAaMr8ivScOXPk8XjUoUMHde7cWZ07d1bHjh3l8Xj0/PPPB3pGAACaJL9ek5Z+/iljK1eu9P7c7G7dulX7RRVNCa9JAwDqQ50ivWrVKo0fP16ffPLJCTEqLS1Vnz59lJubq0svvTTgg1pGpAEA9aFOT3fPnj1bt956q88QxcTE6LbbbtPTTz8dsOEAAGjK6hTpzz77TH/4wx9Oev/ll1+uoqKi0x4KAADUMdJut9vnR6+OCw8P1/79+097KAAAUMdIt2vXTps3bz7p/Z9//jm/ZhEAgACpU6QHDhyoGTNm6OjRoyfc99NPPyk7O1tXXnllwIYDAKApq9O7u91uty666CKFhYVp/PjxOu+88yRJxcXFmjt3riorK7Vhwwbvb6VqKnh3NwCgPtT5c9I7duzQHXfcoeXLl+v4riEhIcrMzNTcuXPVsWPHehnUMiINAKgPfv8wkx9++EFfffWVHMdRly5ddNZZZwV6tkaDSAMA6oPfkcZ/EWkAQH3w62d3AwCA+kekAQAwikgDAGAUkQYAwCgiDQCAUUQaAACjiDQAAEYRaQAAjCLSAAAYRaQBADCKSAMAYBSRBgDAKCINAIBRRBoAAKOINAAARhFpAACMItIAABhFpAEAMIpIAwBgFJEGAMAoIg0AgFFEGgAAo4g0AABGEWkAAIwi0gAAGEWkAQAwikgDAGAUkQYAwCgiDQCAUUQaAACjiDQAAEYRaQAAjCLSAAAYRaQBADCKSAMAYBSRBgDAKCINAIBRRBoAAKOINAAARhFpAACMItIAABhFpAEAMIpIAwBgFJEGAMAoIg0AgFFEGgAAo4g0AABGEWkAAIwi0gAAGEWkAQAwikgDAGAUkQYAwCgiDQCAUUQaAACjGk2kDx48qOHDhys6OlqxsbEaM2aMfvzxxxr3OXr0qMaNG6eWLVvqjDPO0JAhQ+R2u733f/bZZxo2bJgSExPVvHlzdevWTc8++2x9HwoAALXSaCI9fPhwbdmyRStWrNDSpUv1wQcfaOzYsTXuM2nSJL3zzjtavHix3n//fe3Zs0fXXnut9/6ioiK1adNGr7/+urZs2aJp06Zp6tSpmjNnTn0fDgAApxTiOI7T0EOcyrZt23T++edr3bp1SklJkSQtW7ZMAwcO1HfffaeEhIQT9iktLVXr1q2Vl5en6667TpJUXFysbt26qbCwUL179/b5WOPGjdO2bdu0atWqWs/n8XgUExOj0tJSRUdH+3GEAACcqFFcSRcWFio2NtYbaEnKyMhQaGioPv30U5/7FBUVqaKiQhkZGd5tXbt2Vfv27VVYWHjSxyotLVWLFi1qnKesrEwej6faDQCAQGsUkXa5XGrTpk21beHh4WrRooVcLtdJ94mIiFBsbGy17XFxcSfdZ82aNVq0aNEpn0bPyclRTEyM95aYmFj7gwEAoJYaNNJTpkxRSEhIjbfi4uKgzLJ582Zdc801ys7O1uWXX17j2qlTp6q0tNR727VrV1BmBAA0LeEN+eCTJ0/WzTffXOOaTp06KT4+Xvv27au2/dixYzp48KDi4+N97hcfH6/y8nKVlJRUu5p2u90n7LN161YNGDBAY8eO1fTp0085d2RkpCIjI0+5DgCA09GgkW7durVat259ynXp6ekqKSlRUVGRkpOTJUmrVq1SVVWV0tLSfO6TnJysZs2aqaCgQEOGDJEkbd++XTt37lR6erp33ZYtW9S/f3+NGjVKjz76aACOCgCAwGgU7+6WpCuuuEJut1u5ubmqqKjQ6NGjlZKSory8PEnS7t27NWDAAL322mtKTU2VJN1xxx3Kz8/X/PnzFR0drQkTJkj6+bVn6eenuPv376/MzEw9+eST3scKCwur1T8ejuPd3QCA+tCgV9J1sWDBAo0fP14DBgxQaGiohgwZoueee857f0VFhbZv364jR454tz3zzDPetWVlZcrMzNQLL7zgvf+NN97Q/v379frrr+v111/3bj/nnHP07bffBuW4AAA4mUZzJW0ZV9IAgPrQKD6CBQBAU0SkAQAwikgDAGAUkQYAwCgiDQCAUUQaAACjiDQAAEYRaQAAjCLSAAAYRaQBADCKSAMAYBSRBgDAKCINAIBRRBoAAKOINAAARhFpAACMItIAABhFpAEAMIpIAwBgFJEGAMAoIg0AgFFEGgAAo4g0AABGEWkAAIwi0gAAGEWkAQAwikgDAGAUkQYAwCgiDQCAUUQaAACjiDQAAEYRaQAAjCLSAAAYRaQBADCKSAMAYBSRBgDAKCINAIBRRBoAAKOINAAARhFpAACMItIAABhFpAEAMIpIAwBgFJEGAMAoIg0AgFFEGgAAo4g0AABGEWkAAIwi0gAAGEWkAQAwikgDAGAUkQYAwCgiDQCAUUQaAACjiDQAAEYRaQAAjCLSAAAYRaQBADCKSAMAYBSRBgDAKCINAIBRRBoAAKOINAAARhFpAACMItIAABhFpAEAMIpIAwBgFJEGAMAoIg0AgFFEGgAAoxpNpA8ePKjhw4crOjpasbGxGjNmjH788cca9zl69KjGjRunli1b6owzztCQIUPkdrt9rv3+++919tlnKyQkRCUlJfVwBAAA1E2jifTw4cO1ZcsWrVixQkuXLtUHH3ygsWPH1rjPpEmT9M4772jx4sV6//33tWfPHl177bU+144ZM0YXXHBBfYwOAIBfQhzHcRp6iFPZtm2bzj//fK1bt04pKSmSpGXLlmngwIH67rvvlJCQcMI+paWlat26tfLy8nTddddJkoqLi9WtWzcVFhaqd+/e3rUvvviiFi1apJkzZ2rAgAH64YcfFBsbW+v5PB6PYmJiVFpaqujo6NM7WAAA/l+juJIuLCxUbGysN9CSlJGRodDQUH366ac+9ykqKlJFRYUyMjK827p27ar27dursLDQu23r1q166KGH9Nprryk0tHano6ysTB6Pp9oNAIBAaxSRdrlcatOmTbVt4eHhatGihVwu10n3iYiIOOGKOC4uzrtPWVmZhg0bpieffFLt27ev9Tw5OTmKiYnx3hITE+t2QAAA1EKDRnrKlCkKCQmp8VZcXFxvjz916lR169ZNN910U533Ky0t9d527dpVTxMCAJqy8IZ88MmTJ+vmm2+ucU2nTp0UHx+vffv2Vdt+7NgxHTx4UPHx8T73i4+PV3l5uUpKSqpdTbvdbu8+q1at0hdffKE33nhDknT85flWrVpp2rRpevDBB31+78jISEVGRtbmEAEA8FuDRrp169Zq3br1Kdelp6erpKRERUVFSk5OlvRzYKuqqpSWluZzn+TkZDVr1kwFBQUaMmSIJGn79u3auXOn0tPTJUn//Oc/9dNPP3n3Wbdunf74xz/qww8/VOfOnU/38AAAOC2N4t3dknTFFVfI7XYrNzdXFRUVGj16tFJSUpSXlydJ2r17twYMGKDXXntNqampkqQ77rhD+fn5mj9/vqKjozVhwgRJ0po1a3w+xurVq9WvXz/e3Q0AMKFBr6TrYsGCBRo/frwGDBig0NBQDRkyRM8995z3/oqKCm3fvl1HjhzxbnvmmWe8a8vKypSZmakXXnihIcYHAKDOGs2VtGVcSQMA6kOj+AgWAABNEZEGAMAoIg0AgFFEGgAAo4g0AABGEWkAAIwi0gAAGEWkAQAwikgDAGAUkQYAwCgiDQCAUUQaAACjiDQAAEYRaQAAjCLSAAAYRaQBADCKSAMAYBSRBgDAKCINAIBRRBoAAKOINAAARhFpAACMItIAABhFpAEAMIpIAwBgFJEGAMAoIg0AgFFEGgAAo4g0AABGEWkAAIwi0gAAGEWkAQAwikgDAGAUkQYAwCgiDQCAUUQaAACjiDQAAEYRaQAAjCLSAAAYRaQBADCKSAMAYBSRBgDAKCINAIBRRBoAAKOINAAARhFpAACMItIAABhFpAEAMIpIAwBgFJEGAMAoIg0AgFFEGgAAo4g0AABGhTf0AL8GjuNIkjweTwNPAgBoTM4880yFhISc9H4iHQCHDh2SJCUmJjbwJACAxqS0tFTR0dEnvT/EOX4ZCL9VVVVpz549p/wX0al4PB4lJiZq165dNf4/rani/Jwa56hmnJ+acX5OLdDniCvpIAgNDdXZZ58dsO8XHR3NfyA14PycGueoZpyfmnF+Ti1Y54g3jgEAYBSRBgDAKCJtSGRkpLKzsxUZGdnQo5jE+Tk1zlHNOD814/ycWrDPEW8cAwDAKK6kAQAwikgDAGAUkQYAwCgiDQCAUUQ6yObOnasOHTooKipKaWlpWrt2bY3rFy9erK5duyoqKko9e/ZUfn5+kCZtGHU5Py+99JIuvfRSnXXWWTrrrLOUkZFxyvPZ2NX1z89xCxcuVEhIiAYPHly/AxpQ13NUUlKicePGqW3btoqMjNS55577q/7vrK7nZ/bs2TrvvPPUvHlzJSYmatKkSTp69GiQpg2uDz74QFdddZUSEhIUEhKiJUuWnHKf1atX66KLLlJkZKR+97vfaf78+YEdykHQLFy40ImIiHBefvllZ8uWLc6tt97qxMbGOm632+f6jz/+2AkLC3OeeOIJZ+vWrc706dOdZs2aOV988UWQJw+Oup6fG2+80Zk7d66zceNGZ9u2bc7NN9/sxMTEON99912QJw+Oup6f47755hunXbt2zqWXXupcc801wRm2gdT1HJWVlTkpKSnOwIEDnY8++sj55ptvnNWrVzubNm0K8uTBUdfzs2DBAicyMtJZsGCB88033zjLly932rZt60yaNCnIkwdHfn6+M23aNOfNN990JDlvvfVWjeu//vpr5ze/+Y2TlZXlbN261Xn++eedsLAwZ9myZQGbiUgHUWpqqjNu3Djv15WVlU5CQoKTk5Pjc/3111/vDBo0qNq2tLQ057bbbqvXORtKXc/PLx07dsw588wznVdffbW+RmxQ/pyfY8eOOX369HH++te/OqNGjfrVR7qu5+jFF190OnXq5JSXlwdrxAZV1/Mzbtw4p3///tW2ZWVlORdffHG9zmlBbSJ97733Ot27d6+2bejQoU5mZmbA5uDp7iApLy9XUVGRMjIyvNtCQ0OVkZGhwsJCn/sUFhZWWy9JmZmZJ13fmPlzfn7pyJEjqqioUIsWLeprzAbj7/l56KGH1KZNG40ZMyYYYzYof87R22+/rfT0dI0bN05xcXHq0aOHHnvsMVVWVgZr7KDx5/z06dNHRUVF3qfEv/76a+Xn52vgwIFBmdm6YPwdzS/YCJIDBw6osrJScXFx1bbHxcWpuLjY5z4ul8vnepfLVW9zNhR/zs8v3XfffUpISDjhP5pfA3/Oz0cffaS//e1v2rRpUxAmbHj+nKOvv/5aq1at0vDhw5Wfn6+vvvpKd955pyoqKpSdnR2MsYPGn/Nz44036sCBA7rkkkvkOI6OHTum22+/Xffff38wRjbvZH9Hezwe/fTTT2revPlpPwZX0vhVePzxx7Vw4UK99dZbioqKauhxGtyhQ4c0YsQIvfTSS2rVqlVDj2NWVVWV2rRpo3nz5ik5OVlDhw7VtGnTlJub29CjmbB69Wo99thjeuGFF7Rhwwa9+eabevfdd/Xwww839GhNBlfSQdKqVSuFhYXJ7XZX2+52uxUfH+9zn/j4+Dqtb8z8OT/HPfXUU3r88ce1cuVKXXDBBfU5ZoOp6/n5z3/+o2+//VZXXXWVd1tVVZUkKTw8XNu3b1fnzp3rd+gg8+fPUNu2bdWsWTOFhYV5t3Xr1k0ul0vl5eWKiIio15mDyZ/zM2PGDI0YMUK33HKLJKlnz546fPiwxo4dq2nTpik0tGlf553s7+jo6OiAXEVLXEkHTUREhJKTk1VQUODdVlVVpYKCAqWnp/vcJz09vdp6SVqxYsVJ1zdm/pwfSXriiSf08MMPa9myZUpJSQnGqA2iruena9eu+uKLL7Rp0ybv7eqrr1a/fv20adMmJSYmBnP8oPDnz9DFF1+sr776yvsPGEn697//rbZt2/6qAi35d36OHDlyQoiP/4PG4dc+BOfv6IC9BQ2ntHDhQicyMtKZP3++s3XrVmfs2LFObGys43K5HMdxnBEjRjhTpkzxrv/444+d8PBw56mnnnK2bdvmZGdn/+o/glWX8/P44487ERERzhtvvOHs3bvXezt06FBDHUK9quv5+aWm8O7uup6jnTt3OmeeeaYzfvx4Z/v27c7SpUudNm3aOI888khDHUK9quv5yc7Ods4880zn73//u/P11187//rXv5zOnTs7119/fUMdQr06dOiQs3HjRmfjxo2OJOfpp592Nm7c6OzYscNxHMeZMmWKM2LECO/64x/Buueee5xt27Y5c+fO5SNYjd3zzz/vtG/f3omIiHBSU1OdTz75xHtf3759nVGjRlVb/49//MM599xznYiICKd79+7Ou+++G+SJg6su5+ecc85xJJ1wy87ODv7gQVLXPz//qylE2nHqfo7WrFnjpKWlOZGRkU6nTp2cRx991Dl27FiQpw6eupyfiooK54EHHnA6d+7sREVFOYmJic6dd97p/PDDD8EfPAjee+89n3+nHD8no0aNcvr27XvCPklJSU5ERITTqVMn55VXXgnoTPyqSgAAjOI1aQAAjCLSAAAYRaQBADCKSAMAYBSRBgDAKCINAIBRRBoAAKOINAAARhFpAACMItIAABhFpAEAMIpIAwBg1P8BJMriO+tUlsEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.displot(human_perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
