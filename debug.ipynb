{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from typing import List\n",
    "from copy import deepcopy\n",
    "import itertools\n",
    "\n",
    "from chatbot.adviser.app.rl.dialogtree import DialogTree\n",
    "import chatbot.adviser.app.rl.dataset as Data\n",
    "from chatbot.adviser.app.rl.spaceAdapter import AnswerSimilarityEmbeddingConfig, IntentEmbeddingConfig, SpaceAdapter, ActionConfig, SpaceAdapterAttentionInput, SpaceAdapterAttentionQueryInput, SpaceAdapterConfiguration, SpaceAdapterSpaceInput, TextEmbeddingConfig\n",
    "from chatbot.adviser.app.rl.utils import EMBEDDINGS, AutoSkipMode, StateEntry, AverageMetric, EnvInfo, ExperimentLogging, _del_checkpoint, _get_file_hash, _munchausen_stable_logsoftmax, _munchausen_stable_softmax, _save_checkpoint, safe_division\n",
    "from chatbot.adviser.app.rl.layers.attention.attention_factory import AttentionActivationConfig, AttentionMechanismConfig, AttentionVectorAggregation\n",
    "from chatbot.adviser.app.encoding.text import TextEmbeddingPooling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/mount/arbeitsdaten/asr-2/vaethdk/adviser_reisekosten/newruns/V9_ALTERNATIVESEED_ROBERTA_NEWARCH_dqn_50dialog_1_cross-en-de-roberta-sentence-transformer_nouser_intent_prediction_dqn_50dialog_1_cross-en-de-roberta-sentence-transformer_nouser_intent_prediction__9546370__1672918719\"\n",
    "CKPT = \"760000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "device = \"cuda:0\"\n",
    "test = torch.zeros(10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree depth 32\n",
      "Tree max. degree 13\n"
     ]
    }
   ],
   "source": [
    "Data.objects[0] = Data.Dataset.fromJSON('train_graph.json', version=0)\n",
    "Data.objects[1] = Data.Dataset.fromJSON('test_graph.json', version=1)\n",
    "\n",
    "tree = DialogTree(version=0)\n",
    "print(\"Tree depth\", tree.get_max_tree_depth())\n",
    "print(\"Tree max. degree\", tree.get_max_node_degree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name .models/T-Systems-onsite_cross-en-de-roberta-sentence-transformer. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "seed = 12345678\n",
    "args = {\n",
    "    \"spaceadapter\": {\n",
    "        \"configuration\": SpaceAdapterConfiguration(\n",
    "            text_embedding=\"cross-en-de-roberta-sentence-transformer\", #'distiluse-base-multilingual-cased-v2', # 'gbert-large' # 'cross-en-de-roberta-sentence-transformer',\n",
    "            action_config=ActionConfig.ACTIONS_IN_STATE_SPACE,\n",
    "            action_masking=True,\n",
    "            stop_action=False,\n",
    "            auto_skip=AutoSkipMode.NONE,\n",
    "            use_answer_synonyms=True\n",
    "        ),\n",
    "        \"state\": SpaceAdapterSpaceInput(\n",
    "            last_system_action=True,\n",
    "            beliefstate=True,\n",
    "            current_node_position=True,\n",
    "            current_node_type=True,\n",
    "            user_intent_prediction=IntentEmbeddingConfig(\n",
    "                active=False,\n",
    "                ckpt_dir='./.models/intentpredictor'\n",
    "            ),\n",
    "            answer_similarity_embedding=AnswerSimilarityEmbeddingConfig(\n",
    "                active=False,\n",
    "                model_name='distiluse-base-multilingual-cased-v2',\n",
    "                caching=False,\n",
    "            ),\n",
    "            dialog_node_text=TextEmbeddingConfig(\n",
    "                active=True,\n",
    "                pooling=TextEmbeddingPooling.MEAN,\n",
    "                caching=False,\n",
    "            ),\n",
    "            original_user_utterance=TextEmbeddingConfig(\n",
    "                active=True,\n",
    "                pooling=TextEmbeddingPooling.MEAN,\n",
    "                caching=False,\n",
    "            ),\n",
    "            current_user_utterance=TextEmbeddingConfig(\n",
    "                active=True,\n",
    "                pooling=TextEmbeddingPooling.MEAN,\n",
    "                caching=False,\n",
    "            ),\n",
    "            dialog_history=TextEmbeddingConfig(\n",
    "                active=True,\n",
    "                pooling=TextEmbeddingPooling.MEAN,\n",
    "                caching=False,\n",
    "            ),\n",
    "            action_text=TextEmbeddingConfig(\n",
    "                active=True,\n",
    "                pooling=TextEmbeddingPooling.MEAN,\n",
    "                caching=False,\n",
    "            ),\n",
    "            action_position=True\n",
    "        ),\n",
    "        \"attention\": [\n",
    "            SpaceAdapterAttentionInput(\n",
    "                active=False,\n",
    "                name=\"utterance_nodetext_attn\",\n",
    "                queries=SpaceAdapterAttentionQueryInput(\n",
    "                    input=['current_user_utterance',\n",
    "                            'original_user_utterance'],\n",
    "                    pooling=TextEmbeddingPooling.CLS,\n",
    "                    aggregation=AttentionVectorAggregation.SUM,\n",
    "                    caching=False,\n",
    "                    allow_noise=True\n",
    "                ),\n",
    "                matrix=\"dialog_node_text\",\n",
    "                activation=AttentionActivationConfig.NONE,\n",
    "                attention_mechanism=AttentionMechanismConfig.ADDITIVE,\n",
    "                caching=False,\n",
    "                allow_noise=False\n",
    "            ),\n",
    "            SpaceAdapterAttentionInput(\n",
    "                active=False,\n",
    "                name=\"utterance_history_attn\",\n",
    "                queries=SpaceAdapterAttentionQueryInput(\n",
    "                    input=['current_user_utterance',\n",
    "                            'original_user_utterance'],\n",
    "                    pooling=TextEmbeddingPooling.CLS,\n",
    "                    aggregation=AttentionVectorAggregation.MAX,\n",
    "                    caching=False,\n",
    "                    allow_noise=True\n",
    "                ),\n",
    "                matrix=\"dialog_history\",\n",
    "                activation=AttentionActivationConfig.NONE,\n",
    "                attention_mechanism=AttentionMechanismConfig.ADDITIVE,\n",
    "                caching=False,\n",
    "                allow_noise=False\n",
    "            )\n",
    "        ]\n",
    "    },\n",
    "    \"simulation\": {\n",
    "        \"normalize_rewards\": True,\n",
    "        \"max_steps\": 50,\n",
    "        \"user_patience\": 3,\n",
    "        \"stop_when_reaching_goal\": True,\n",
    "        \"dialog_faq_ratio\": 0.5,\n",
    "        \"parallel_train_envs\": 128,\n",
    "        \"parallel_test_envs\": 128,\n",
    "        \"train_noise\": 0.1,\n",
    "        \"eval_noise\": 0.0,\n",
    "        \"test_noise\": 0.0\n",
    "    },\n",
    "    \"experiment\": {\n",
    "        \"seed\": seed,\n",
    "        \"cudnn_deterministic\": False,\n",
    "        \"keep\": 5\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"architecture\": \"new_dueling\", # 'dueling', 'vanilla', \"new_dueling\"\n",
    "        \"shared_layer_sizes\": [8096, 4096, 4096],\n",
    "        \"value_layer_sizes\": [2048, 1024],\n",
    "        \"advantage_layer_sizes\": [4096, 2048, 1024],\n",
    "        \"hidden_layer_sizes\": [4096, 2048, 1024],\n",
    "        \"dropout\": 0.25,\n",
    "        \"activation_fn\": \"SELU\",\n",
    "        \"normalization_layers\": False,\n",
    "        \"intentprediction\": True # True # False\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"name\": \"Adam\",\n",
    "        \"lr\": 0.0001\n",
    "    },\n",
    "    \"algorithm\": {\n",
    "        \"timesteps_per_reset\": 1000000,\n",
    "        \"reset_exploration_times\": 0,\n",
    "        \"max_grad_norm\": 1.0,\n",
    "        \"batch_size\": 3,\n",
    "        \"gamma\": 0.99,\n",
    "        \"algorithm\": \"dqn\", # \"ppo\", \"dqn\"\n",
    "    },\n",
    "    \"ppo\": {\n",
    "        \"T\": 4, # timesteps per actor (<< episode length) included in one minibatch => parallel actors = batch_size // T2,\n",
    "        'update_epochs': 10,\n",
    "        'minibatch_size': 64\n",
    "    },\n",
    "    \"dqn\": {\n",
    "        \"buffer_size\": 100000,\n",
    "        \"buffer_type\": \"HER\", # \"prioritized\", \"LAP\", # \"uniform\", # \"HER\"\n",
    "        \"priority_replay_alpha\": 0.6,\n",
    "        \"priority_replay_beta\": 0.4,\n",
    "        \"exploration_fraction\": 0.99,\n",
    "        \"eps_start\": 0.6,\n",
    "        \"eps_end\": 0.0,\n",
    "        \"train_frequency\": 3,\n",
    "        \"learning_starts\": 1280,\n",
    "        \"target_network_frequency\": 15,\n",
    "        \"q_value_clipping\": 10.0,\n",
    "        \"munchausen_targets\": True,\n",
    "        \"munchausen_tau\": 0.03,\n",
    "        \"munchausen_alpha\": 0.9,\n",
    "        \"munchausen_clipping\": -1\n",
    "    },\n",
    "    \"evaluation\": {\n",
    "        \"evaluation\": True,\n",
    "        \"every_train_timesteps\": 10000,\n",
    "        \"dialogs\": 500\n",
    "    }\n",
    "}\n",
    "\n",
    "torch.backends.cudnn.deterministic = args[\"experiment\"][\"cudnn_deterministic\"]\n",
    "text_embedding_name = args['spaceadapter']['configuration'].text_embedding\n",
    "EMBEDDINGS[text_embedding_name]['args'].pop('cache_db_index')\n",
    "text_enc = EMBEDDINGS[text_embedding_name]['class'](device=device, **EMBEDDINGS[text_embedding_name]['args'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree embedding for nodes...\n",
      "Done\n",
      "NO ANSWER SIMILARITY EMBEDDING\n"
     ]
    }
   ],
   "source": [
    "cache_conn = None # no caching for testing\n",
    "spaceadapter_config: SpaceAdapterConfiguration = args['spaceadapter']['configuration']\n",
    "spaceadapter_state: SpaceAdapterSpaceInput = args['spaceadapter']['state']\n",
    "spaceadapter_attention: List[SpaceAdapterAttentionInput] = args['spaceadapter']['attention']\n",
    "spaceadapter_config.post_init(tree=tree)\n",
    "spaceadapter_state.post_init(device=device, tree=tree, text_embedding=text_enc, action_config=spaceadapter_config.action_config, action_masking=spaceadapter_config.action_masking, stop_action=spaceadapter_config.stop_action, cache_connection=cache_conn)\n",
    "for attn in spaceadapter_attention:\n",
    "    attn.post_init(device=device, tree=tree, text_embedding=text_enc, action_config=spaceadapter_config.action_config, action_masking=spaceadapter_config.action_masking, cache_connection=cache_conn)\n",
    "adapter = SpaceAdapter(device=device, dialog_tree=tree, **args[\"spaceadapter\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARCHITECTURE: New Dueling with Intent Prediction Head\n"
     ]
    }
   ],
   "source": [
    "if args['model']['activation_fn'] == \"ReLU\":\n",
    "    acivation_fn = torch.nn.ReLU\n",
    "elif args['model']['activation_fn']  == \"tanh\":\n",
    "    acivation_fn =  torch.nn.Tanh\n",
    "elif args['model']['activation_fn'] == \"SELU\":\n",
    "    acivation_fn =  torch.nn.SELU\n",
    "else:\n",
    "    assert False, f\"unknown activation function name: {args['model']['activation_fn']}\"\n",
    "q_value_clipping = args['dqn']['q_value_clipping'] if 'q_value_clipping' in args['dqn'] else 0\n",
    "kwargs = {\n",
    "    \"adapter\": adapter,\n",
    "    \"dropout_rate\": args['model']['dropout'],\n",
    "    \"activation_fn\": acivation_fn,\n",
    "    \"normalization_layers\": args['model']['normalization_layers'],\n",
    "    \"q_value_clipping\": q_value_clipping,\n",
    "}\n",
    "if 'dueling' in args['model']['architecture']:\n",
    "    kwargs |= {\n",
    "        \"shared_layer_sizes\": args['model']['shared_layer_sizes'],\n",
    "        \"advantage_layer_sizes\": args[\"model\"][\"advantage_layer_sizes\"],\n",
    "        \"value_layer_sizes\": args['model']['value_layer_sizes'],\n",
    "    }\n",
    "    if args['model']['intentprediction'] == False:\n",
    "        from chatbot.adviser.app.rl.dqn.dqn import DuelingDQN\n",
    "        model = DuelingDQN(**kwargs)\n",
    "    else:\n",
    "        if args['model']['architecture'] == \"dueling\":\n",
    "            from chatbot.adviser.app.rl.dqn.dqn import DuelingDQNWithIntentPredictionHead\n",
    "            model = DuelingDQNWithIntentPredictionHead(**kwargs)\n",
    "        elif args['model']['architecture'] == \"new_dueling\":\n",
    "            from chatbot.adviser.app.rl.dqn.dqn import NewDuelingDQNWithIntentPredictionHead\n",
    "            model = NewDuelingDQNWithIntentPredictionHead(**kwargs)\n",
    "elif args['model']['architecture'] == 'vanilla':\n",
    "    from chatbot.adviser.app.rl.dqn.dqn import DQN\n",
    "    model = DQN(hidden_layer_sizes=args[\"model\"][\"hidden_layer_sizes\"], **kwargs)\n",
    "assert model, f\"unknown model architecture {args['model']['architecture']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n"
     ]
    }
   ],
   "source": [
    "weights = f\"{MODEL_PATH}/ckpt_{CKPT}.pt\"\n",
    "model.load_state_dict(torch.load(weights, map_location=\"cpu\")['model'])\n",
    "model.to(device)\n",
    "adapter.set_model(model)\n",
    "model.eval()\n",
    "print(\"Model Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_system(node_key):\n",
    "    node = Data.objects[0].node_by_key(node_key)\n",
    "    print(\"System: \", node.content.text)\n",
    "    for answer in node.answers:\n",
    "        print(\" - \", answer.content.text)\n",
    "\n",
    "def _transform_dialog_history(user_utterances_history, system_utterances_history):\n",
    "        # interleave system and user utterances\n",
    "        # Returns: List[Tuple(sys: str, usr: str)]\n",
    "        usr_history = user_utterances_history\n",
    "        if len(usr_history) < len(system_utterances_history):\n",
    "            usr_history = usr_history + [\"\"]\n",
    "        assert len(usr_history) == len(system_utterances_history)\n",
    "        return list(itertools.chain(zip([utterance for utterance in system_utterances_history], [utterance for utterance in user_utterances_history])))\n",
    "\n",
    "def get_obs(current_node, initial_user_utterance, current_user_utterance, system_utterances_history, user_utterances_history, bst, last_action_idx):\n",
    "    obs = {\n",
    "        StateEntry.DIALOG_NODE.value: current_node,\n",
    "        StateEntry.DIALOG_NODE_KEY.value: current_node.key,\n",
    "        StateEntry.ORIGINAL_USER_UTTERANCE.value: deepcopy(initial_user_utterance),\n",
    "        StateEntry.CURRENT_USER_UTTERANCE.value: deepcopy(current_user_utterance),\n",
    "        StateEntry.SYSTEM_UTTERANCE_HISTORY.value: deepcopy(system_utterances_history),\n",
    "        StateEntry.USER_UTTERANCE_HISTORY.value: deepcopy(user_utterances_history),\n",
    "        StateEntry.DIALOG_HISTORY.value: _transform_dialog_history(user_utterances_history, system_utterances_history),\n",
    "        StateEntry.BST.value: deepcopy(bst),\n",
    "        StateEntry.LAST_SYSACT.value: last_action_idx,\n",
    "        StateEntry.NOISE.value: 0.0\n",
    "    }\n",
    "    return adapter.encode(obs)\n",
    "\n",
    "def action_to_text(node, index: int):\n",
    "    # decode action offsets: ASK=0, SKIP_1=1, ..., SKIP_N=N\n",
    "    if index == 0:\n",
    "        return \"ASK\"\n",
    "    else:\n",
    "        return node.answer_by_index(index - 1).content.text\n",
    "    \n",
    "\n",
    "def next_action(current_node, obs):\n",
    "    print(\"Batch size\", args['algorithm'][\"batch_size\"])\n",
    "    state = adapter.batch_state_vector_from_obs([obs, obs, obs], args['algorithm'][\"batch_size\"])\n",
    "    state = pack_sequence([s.to(device) for s in state], enforce_sorted=False)\n",
    "    q_values, intent_logits = model(state) # batch x num_max_actions\n",
    "    ### TEST ###\n",
    "    # q_values = q_values.view(-1, 7)\n",
    "    print(intent_logits.size())\n",
    "    ###\n",
    "    print(\"Intent logits\", intent_logits)\n",
    "    print(\"Q values:\", q_values)\n",
    "    if adapter.configuration.action_masking:\n",
    "        print(\"MASKING\")\n",
    "        q_values = torch.masked_fill(q_values, adapter.get_action_masks(node_keys=[current_node.key])[:,:q_values.size(-1)], float('-inf'))\n",
    "        print(\" Masked Q values:\", q_values)\n",
    "    next_action_indices = q_values.argmax(-1).tolist()\n",
    "    print(\"Next action\", [(action, action_to_text(current_node, action)) for action in next_action_indices])\n",
    "    intent_classes = None if isinstance(intent_logits, type(None)) else (torch.sigmoid(intent_logits).view(-1) > 0.5).long().tolist()\n",
    "    print(\"Intent class\", intent_classes)\n",
    "    print([\"FAQ\" if intent == 1 else \"DIALOG\" for intent in intent_classes])\n",
    "    return next_action_indices\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System:  Zu welchem Thema haben Sie Fragen?Sie können entweder eine passende Option aus der Vorschlagsliste anklicken oder unten Text eingeben.\n",
      " -  Reise buchen\n",
      " -  Forschungssemester\n",
      " -  Travel Risk Management\n",
      " -  Problem/Notfall während der Dienstreise\n",
      " -  Formulare, Fristen und Abläufe\n",
      " -  Reise stornieren\n"
     ]
    }
   ],
   "source": [
    "current_node = Data.objects[0].node_by_key(tree.get_start_node().connected_node_key)\n",
    "initial_user_utterance = \"Darf ich Taxi fahren?\"\n",
    "# initial_user_utterance = \"Dienstreise buchen\"\n",
    "# initial_user_utterance = \"Darf ich mein Hotel selbst buchen?\"\n",
    "# initial_user_utterance = \"Wer bezahlt Mitgliedschaftsgebühren?\"\n",
    "# initial_user_utterance = \"ändert sich mein Dienstort während eines Forschungsaufenthalts?\"\n",
    "current_user_utterance = deepcopy(initial_user_utterance)\n",
    "\n",
    "bst = {}\n",
    "last_action_idx = 1\n",
    "user_utterances_history = [deepcopy(initial_user_utterance)]\n",
    "system_utterances_history = [deepcopy(current_node.content.text)]\n",
    "\n",
    "# print(\"History\", _transform_dialog_history(user_utterances_history, system_utterances_history))\n",
    "\n",
    "print_system(current_node.key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size 3\n",
      "torch.Size([3, 1])\n",
      "Intent logits tensor([[2674.9446],\n",
      "        [2674.9446],\n",
      "        [2674.9446]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Q values: tensor([[0.2778, 1.0185, 0.0620, 0.1815, 0.1606, 0.1576, 0.2685],\n",
      "        [0.2778, 1.0185, 0.0620, 0.1815, 0.1606, 0.1576, 0.2685],\n",
      "        [0.2778, 1.0185, 0.0620, 0.1815, 0.1606, 0.1576, 0.2685]],\n",
      "       device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "MASKING\n",
      " Masked Q values: tensor([[0.2778, 1.0185, 0.0620, 0.1815, 0.1606, 0.1576, 0.2685],\n",
      "        [0.2778, 1.0185, 0.0620, 0.1815, 0.1606, 0.1576, 0.2685],\n",
      "        [0.2778, 1.0185, 0.0620, 0.1815, 0.1606, 0.1576, 0.2685]],\n",
      "       device='cuda:0', grad_fn=<MaskedFillBackward0>)\n",
      "Next action [(1, 'Reise buchen'), (1, 'Reise buchen'), (1, 'Reise buchen')]\n",
      "Intent class [1, 1, 1]\n",
      "['FAQ', 'FAQ', 'FAQ']\n"
     ]
    }
   ],
   "source": [
    "action = next_action(current_node, get_obs(current_node, initial_user_utterance, current_user_utterance, system_utterances_history, user_utterances_history, bst, last_action_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO nachsehen: wird die History VOR oder NACH der next action erweitert?\n",
    "# TODO prüfen: ist die Evaluierung korrekt (nimmt sie auch pack_sequence und nicht einfach cat?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "System:  Handelt es sich um eine Dienstreise oder einen Dienstgang?\n",
      " -  Dienstreise\n",
      " -  Dienstgang\n",
      " -  Was ist der Unterschied zwischen Dienstgang und Dienstreise?\n",
      "Batch size 3\n",
      "torch.Size([3, 1])\n",
      "Intent logits tensor([[2544.9609],\n",
      "        [2544.9609],\n",
      "        [2544.9609]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Q values: tensor([[0.2761, 0.0782, 1.0607, 0.1667],\n",
      "        [0.2761, 0.0782, 1.0607, 0.1667],\n",
      "        [0.2761, 0.0782, 1.0607, 0.1667]], device='cuda:0',\n",
      "       grad_fn=<ClampBackward1>)\n",
      "MASKING\n",
      " Masked Q values: tensor([[0.2761, 0.0782, 1.0607, 0.1667],\n",
      "        [0.2761, 0.0782, 1.0607, 0.1667],\n",
      "        [0.2761, 0.0782, 1.0607, 0.1667]], device='cuda:0',\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "Next action [(2, 'Dienstgang'), (2, 'Dienstgang'), (2, 'Dienstgang')]\n",
      "Intent class [1, 1, 1]\n",
      "['FAQ', 'FAQ', 'FAQ']\n"
     ]
    }
   ],
   "source": [
    "last_action_idx = action[0]\n",
    "print(last_action_idx)\n",
    "current_node = Data.objects[0].node_by_key(current_node.answer_by_index(last_action_idx - 1).connected_node_key)\n",
    "current_user_utterance =  \"\"\n",
    "bst = {}\n",
    "user_utterances_history = user_utterances_history + [deepcopy(current_user_utterance)]\n",
    "system_utterances_history = system_utterances_history + [deepcopy(current_node.content.text)]\n",
    "\n",
    "print_system(current_node.key)\n",
    "\n",
    "action = next_action(current_node, get_obs(current_node, initial_user_utterance, current_user_utterance, system_utterances_history, user_utterances_history, bst, last_action_idx))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "System:  Haben Sie die mündliche Genehmigung ihres Vorgesetzten eingeholt?\n",
      " -  Ja\n",
      " -  Nein\n",
      "Batch size 3\n",
      "torch.Size([3, 1])\n",
      "Intent logits tensor([[2353.9873],\n",
      "        [2353.9873],\n",
      "        [2353.9873]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Q values: tensor([[0.2074, 1.0901, 0.1542],\n",
      "        [0.2074, 1.0901, 0.1542],\n",
      "        [0.2074, 1.0901, 0.1542]], device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "MASKING\n",
      " Masked Q values: tensor([[0.2074, 1.0901, 0.1542],\n",
      "        [0.2074, 1.0901, 0.1542],\n",
      "        [0.2074, 1.0901, 0.1542]], device='cuda:0',\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "Next action [(1, 'Ja'), (1, 'Ja'), (1, 'Ja')]\n",
      "Intent class [1, 1, 1]\n",
      "['FAQ', 'FAQ', 'FAQ']\n"
     ]
    }
   ],
   "source": [
    "last_action_idx = action[0]\n",
    "print(last_action_idx)\n",
    "current_node = Data.objects[0].node_by_key(current_node.answer_by_index(last_action_idx - 1).connected_node_key)\n",
    "current_user_utterance =  \"\"\n",
    "bst = {}\n",
    "user_utterances_history = user_utterances_history + [deepcopy(current_user_utterance)]\n",
    "system_utterances_history = system_utterances_history + [deepcopy(current_node.content.text)]\n",
    "\n",
    "print_system(current_node.key)\n",
    "\n",
    "action = next_action(current_node, get_obs(current_node, initial_user_utterance, current_user_utterance, system_utterances_history, user_utterances_history, bst, last_action_idx))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "System:  Wollen Sie sich über die Buchung von Transportmitteln oder über Nebenkosten informieren?\n",
      " -  Transportmittel\n",
      " -  Was sind Nebenkosten?\n",
      " -  Nebenkosten\n",
      "Batch size 3\n",
      "torch.Size([3, 1])\n",
      "Intent logits tensor([[2633.6929],\n",
      "        [2633.6929],\n",
      "        [2633.6929]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Q values: tensor([[0.1939, 1.0829, 0.1287, 0.0881],\n",
      "        [0.1939, 1.0829, 0.1287, 0.0881],\n",
      "        [0.1939, 1.0829, 0.1287, 0.0881]], device='cuda:0',\n",
      "       grad_fn=<ClampBackward1>)\n",
      "MASKING\n",
      " Masked Q values: tensor([[0.1939, 1.0829, 0.1287, 0.0881],\n",
      "        [0.1939, 1.0829, 0.1287, 0.0881],\n",
      "        [0.1939, 1.0829, 0.1287, 0.0881]], device='cuda:0',\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "Next action [(1, 'Transportmittel'), (1, 'Transportmittel'), (1, 'Transportmittel')]\n",
      "Intent class [1, 1, 1]\n",
      "['FAQ', 'FAQ', 'FAQ']\n"
     ]
    }
   ],
   "source": [
    "last_action_idx = action[0]\n",
    "print(last_action_idx)\n",
    "current_node = Data.objects[0].node_by_key(current_node.answer_by_index(last_action_idx - 1).connected_node_key)\n",
    "current_user_utterance =  \"\"\n",
    "bst = {}\n",
    "user_utterances_history = user_utterances_history + [deepcopy(current_user_utterance)]\n",
    "system_utterances_history = system_utterances_history + [deepcopy(current_node.content.text)]\n",
    "\n",
    "print_system(current_node.key)\n",
    "\n",
    "action = next_action(current_node, get_obs(current_node, initial_user_utterance, current_user_utterance, system_utterances_history, user_utterances_history, bst, last_action_idx))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "System:  Welches Transportmittel möchten Sie nutzen?\n",
      " -  Zug\n",
      " -  ÖPNV\n",
      " -  eigener PKW\n",
      " -  Mietwagen\n",
      " -  Taxi\n",
      " -  Flug\n",
      "Batch size 3\n",
      "torch.Size([3, 1])\n",
      "Intent logits tensor([[2386.0151],\n",
      "        [2386.0151],\n",
      "        [2386.0151]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Q values: tensor([[0.1843, 0.1283, 0.3395, 0.1901, 0.8693, 1.0707, 0.1010],\n",
      "        [0.1843, 0.1283, 0.3395, 0.1901, 0.8693, 1.0707, 0.1010],\n",
      "        [0.1843, 0.1283, 0.3395, 0.1901, 0.8693, 1.0707, 0.1010]],\n",
      "       device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "MASKING\n",
      " Masked Q values: tensor([[0.1843, 0.1283, 0.3395, 0.1901, 0.8693, 1.0707, 0.1010],\n",
      "        [0.1843, 0.1283, 0.3395, 0.1901, 0.8693, 1.0707, 0.1010],\n",
      "        [0.1843, 0.1283, 0.3395, 0.1901, 0.8693, 1.0707, 0.1010]],\n",
      "       device='cuda:0', grad_fn=<MaskedFillBackward0>)\n",
      "Next action [(5, 'Taxi'), (5, 'Taxi'), (5, 'Taxi')]\n",
      "Intent class [1, 1, 1]\n",
      "['FAQ', 'FAQ', 'FAQ']\n"
     ]
    }
   ],
   "source": [
    "last_action_idx = action[0]\n",
    "print(last_action_idx)\n",
    "current_node = Data.objects[0].node_by_key(current_node.answer_by_index(last_action_idx - 1).connected_node_key)\n",
    "current_user_utterance =  \"\"\n",
    "bst = {}\n",
    "user_utterances_history = user_utterances_history + [deepcopy(current_user_utterance)]\n",
    "system_utterances_history = system_utterances_history + [deepcopy(current_node.content.text)]\n",
    "\n",
    "print_system(current_node.key)\n",
    "\n",
    "action = next_action(current_node, get_obs(current_node, initial_user_utterance, current_user_utterance, system_utterances_history, user_utterances_history, bst, last_action_idx))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "System:  Ein Taxi ist erstattungsfähig, wenn ein dienstlicher oder zwingender persönlicher Grund vorliegt, z.B.:schlechte Erreichbarkeit mit ÖPNVGesundheitszustandSchwerbehinderung\n",
      "Batch size 3\n",
      "torch.Size([3, 1])\n",
      "Intent logits tensor([[1736.8931],\n",
      "        [1736.8931],\n",
      "        [1736.8931]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Q values: tensor([[1.0601, 0.1229],\n",
      "        [1.0601, 0.1229],\n",
      "        [1.0601, 0.1229]], device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "MASKING\n",
      " Masked Q values: tensor([[1.0601, 0.1229],\n",
      "        [1.0601, 0.1229],\n",
      "        [1.0601, 0.1229]], device='cuda:0', grad_fn=<MaskedFillBackward0>)\n",
      "Next action [(0, 'ASK'), (0, 'ASK'), (0, 'ASK')]\n",
      "Intent class [1, 1, 1]\n",
      "['FAQ', 'FAQ', 'FAQ']\n"
     ]
    }
   ],
   "source": [
    "last_action_idx = action[0]\n",
    "print(last_action_idx)\n",
    "current_node = Data.objects[0].node_by_key(current_node.answer_by_index(last_action_idx - 1).connected_node_key)\n",
    "current_user_utterance =  \"\"\n",
    "bst = {}\n",
    "user_utterances_history = user_utterances_history + [deepcopy(current_user_utterance)]\n",
    "system_utterances_history = system_utterances_history + [deepcopy(current_node.content.text)]\n",
    "\n",
    "print_system(current_node.key)\n",
    "\n",
    "action = next_action(current_node, get_obs(current_node, initial_user_utterance, current_user_utterance, system_utterances_history, user_utterances_history, bst, last_action_idx))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.zeros(40000, 40000, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "056f2124f905d9dd687b58d215ad5799610d7eb9433e0a5b89cc00dc4660a857"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
