{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from typing import List, Tuple\n",
    "import re\n",
    "DEVICE = 'cuda:0'\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !GITHUB_ACTIONS=true pip install auto-gptq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM, set_seed\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mauto_gptq\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoGPTQForCausalLM, BaseQuantizeConfig\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset\u001b[39;00m \u001b[39mimport\u001b[39;00m GraphDataset, DataAugmentationLevel, NodeType, DialogNode, Question\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/auto_gptq/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m0.3.1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodeling\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseQuantizeConfig\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodeling\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoGPTQForCausalLM\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpeft_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m get_gptq_peft_model\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/auto_gptq/modeling/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_base\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseGPTQForCausalLM, BaseQuantizeConfig\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mauto\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbloom\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mgpt2\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/auto_gptq/modeling/auto.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mgptj\u001b[39;00m \u001b[39mimport\u001b[39;00m GPTJGPTQForCausalLM\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mgpt2\u001b[39;00m \u001b[39mimport\u001b[39;00m GPT2GPTQForCausalLM\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mllama\u001b[39;00m \u001b[39mimport\u001b[39;00m LlamaGPTQForCausalLM\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmoss\u001b[39;00m \u001b[39mimport\u001b[39;00m MOSSGPTQForCausalLM\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mopt\u001b[39;00m \u001b[39mimport\u001b[39;00m OPTGPTQForCausalLM\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/auto_gptq/modeling/llama.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimport_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m compare_transformers_version\n\u001b[1;32m      6\u001b[0m \u001b[39mif\u001b[39;00m compare_transformers_version(\u001b[39m\"\u001b[39m\u001b[39mv4.28.0\u001b[39m\u001b[39m\"\u001b[39m, op\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mge\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m----> 7\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn_modules\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfused_llama_attn\u001b[39;00m \u001b[39mimport\u001b[39;00m FusedLlamaAttentionForQuantizedModel\n\u001b[1;32m      8\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn_modules\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfused_llama_mlp\u001b[39;00m \u001b[39mimport\u001b[39;00m FusedLlamaMLPForQuantizedModel\n\u001b[1;32m      9\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/auto_gptq/nn_modules/fused_llama_attn.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mimport\u001b[39;00m functional \u001b[39mas\u001b[39;00m F\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mllama\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_llama\u001b[39;00m \u001b[39mimport\u001b[39;00m LlamaAttention, apply_rotary_pos_emb\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_fused_base\u001b[39;00m \u001b[39mimport\u001b[39;00m FusedBaseAttentionModule\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimport_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m compare_pytorch_version, dynamically_import_QuantLinear\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:975\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1074\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from data.dataset import GraphDataset, DataAugmentationLevel, NodeType, DialogNode, Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"TheBloke/upstage-llama-30b-instruct-2048-GPTQ\"\n",
    "model_basename = \"gptq_model-4bit--1g\"\n",
    "\n",
    "use_triton = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\n",
    "                                          use_fast=True,\n",
    "                                          cache_dir=\"/mount/arbeitsdaten/asr-2/vaethdk/resources/weights/\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension not installed.\n",
      "The safetensors archive passed at /mount/arbeitsdaten/asr-2/vaethdk/resources/weights/TheBloke--upstage-llama-30b-instruct-2048-GPTQ/gptq_model-4bit-32g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n",
      "skip module injection for FusedLlamaMLPForQuantizedModel not support integrate without triton yet.\n"
     ]
    }
   ],
   "source": [
    "model = AutoGPTQForCausalLM.from_quantized(\"/mount/arbeitsdaten/asr-2/vaethdk/resources/weights/TheBloke--upstage-llama-30b-instruct-2048-GPTQ\",\n",
    "        # model_basename=model_basename,\n",
    "        # revision=\"gptq-4bit-32g-actorder_True\",\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=False,\n",
    "        device=\"cuda:0\",\n",
    "        use_triton=use_triton,\n",
    "        quantize_config=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System:\n",
    "{System}\n",
    "\n",
    "### User:\n",
    "{User}\n",
    "\n",
    "### Assistant:\n",
    "{Assistant}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> \n",
      "### System:\n",
      "You are a helpful assistant creating a list of FAQ-style questions from given facts.\n",
      "Only generate questions that can be answered by the given facts, without any external knowledge.\n",
      "Remove some information, especially nouns and named entities, between generated questions.\n",
      "Use casual language.\n",
      "Order the generated paraphrases in a numbered list.\n",
      "\n",
      "### User:\n",
      "Generate 10 FAQ-style questions from the fact: \"In the US, you are entitled to 30$ per day, minus any free meals which you choose to decline.\"\n",
      "\n",
      "### Assistant:\n",
      "1. What is the daily allowance for meals in the US?\n",
      "2. Can you receive more than 30$ per day for meals in the US?\n",
      "3. Are you allowed to decline free meals in the US?\n",
      "4. How does declining free meals affect your daily meal allowance?\n",
      "5. Is there a specific amount you can receive for meals in the US?\n",
      "6. Can you receive less than 30$ per day for meals in the US?\n",
      "7. How does the daily meal allowance work in the US?\n",
      "8. Is there a limit to the number of meals you can decline in the US?\n",
      "9. Can you receive more than one meal per day in the US?\n",
      "10. How does declining free meals affect your daily meal allowance in the US?</s>\n"
     ]
    }
   ],
   "source": [
    "system = \"\"\"You are a helpful assistant creating a list of FAQ-style questions from given facts.\n",
    "Only generate questions that can be answered by the given facts, without any external knowledge.\n",
    "Remove some information, especially nouns and named entities, between generated questions.\n",
    "Use casual language.\n",
    "Order the generated paraphrases in a numbered list.\"\"\"\n",
    "user = 'Generate 10 FAQ-style questions from the fact: \"In the US, you are entitled to 30$ per day, minus any free meals which you choose to decline.\"'\n",
    "\n",
    "\n",
    "prompt = f\"\"\"\n",
    "### System:\n",
    "{system}\n",
    "\n",
    "### User:\n",
    "{user}\n",
    "\n",
    "### Assistant:\"\"\"\n",
    "\n",
    "set_seed(42)\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Question Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(system: str, user: str) -> str:\n",
    "    return f\"\"\"\n",
    "    ### System:\n",
    "    {system}\n",
    "\n",
    "    ### User:\n",
    "    {user}\n",
    "\n",
    "    ### Assistant:\"\"\"\n",
    "\n",
    "def generate_output(prompt: str, temperature: float = 0.7, max_new_tokens: int = 512) -> torch.FloatTensor:\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\n",
    "    output = model.generate(inputs=input_ids, temperature=temperature, max_new_tokens=max_new_tokens)\n",
    "    return tokenizer.decode(output[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- not using synonyms\n",
      "===== Dataset Statistics =====\n",
      "- files:  resources/en/train_graph.json resources/en/train_answers.json\n",
      "- synonyms: False\n",
      "- depth: 20  - degree: 13\n",
      "- answers: 73\n",
      "- questions: 279\n",
      "- loaded original data: True\n",
      "- loaded generated data: False\n"
     ]
    }
   ],
   "source": [
    "human_data_train = GraphDataset('en/train_graph.json', 'en/train_answers.json', False, augmentation=DataAugmentationLevel.NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that we don't have any answer synonyms\n",
    "for node in human_data_train.node_list:\n",
    "    for question in node.questions:\n",
    "        human_data_train.question_list.remove(question)\n",
    "        del human_data_train.questions_by_key[question.key]\n",
    "    node.questions.clear()\n",
    "assert len(human_data_train.question_list) == 0\n",
    "assert len(human_data_train.questions_by_key) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_output(original_question: str, prompt: str, output: str, num_paraphrases: int) -> List[str]:\n",
    "    # remove prompt from output first (ends at ### ASSISTANT: )\n",
    "    questions = []\n",
    "    cleaned = output[len(prompt):]\n",
    "    \n",
    "    if not \"1.\" in cleaned: \n",
    "        print(\"NO LIST FOR QUESTION\", original_question)\n",
    "        return questions\n",
    "    \n",
    "    for i in range(1, num_paraphrases+1):\n",
    "        if not f\"{i}.\" in cleaned: \n",
    "            print(f\" - NO {i}. CANDIDATE FOR QUESTION\", original_question)\n",
    "            continue\n",
    "\n",
    "        start_idx = cleaned.find(f\"{i}.\") # find i. line\n",
    "        end_idx = cleaned.find(\"\\n\", start_idx) # read until line end \n",
    "        if i == num_paraphrases and end_idx == -1:\n",
    "            # last line might not have line break\n",
    "            end_idx = len(cleaned)\n",
    "        if start_idx == -1 or end_idx == -1:\n",
    "            print(f\" - INDEX PROBLEM FOR {i}. CANDIDATE: ({start_idx}, {end_idx})\")\n",
    "            continue\n",
    "        # parse answer\n",
    "        questions.append(cleaned[start_idx:end_idx].replace(\"</s>\", \"\").strip())\n",
    "\n",
    "        cleaned = cleaned[end_idx:] # remove i. line\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [9:40:49<00:00, 435.62s/it]  \n"
     ]
    }
   ],
   "source": [
    "from data.dataset import NodeType, Question\n",
    "import time\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "system = \"\"\"You are a helpful assistant creating a list of FAQ-style questions from given facts.\n",
    "Only generate questions that can be answered by the given facts, without any external knowledge.\n",
    "Remove some information, especially nouns and named entities, between generated questions.\n",
    "Use casual language.\n",
    "Order the generated paraphrases in a numbered list.\"\"\"\n",
    "\n",
    "def user(answer_text: str, num_paraphrases: int) -> str:\n",
    "    return f'Generate {num_paraphrases} FAQ-style questions from the fact: \"{answer_text}\"'\n",
    "\n",
    "NUM_QUESTIONS = 10\n",
    "TEMPERATURE = 0.7\n",
    "MAX_NEW_TOKENS = 1024\n",
    "generated_data = {}\n",
    "\n",
    "for node in tqdm(human_data_train.nodes_by_type[NodeType.INFO]):\n",
    "    prompt = generate_prompt(system=system, user=user(node.text, NUM_QUESTIONS))\n",
    "    gen = generate_output(prompt=prompt, temperature=TEMPERATURE, max_new_tokens=MAX_NEW_TOKENS)\n",
    "    candidates = parse_output(original_question=node.text, prompt=prompt, output=gen, num_paraphrases=NUM_QUESTIONS)\n",
    "    for candidate in candidates:\n",
    "        key = str(time.time()).replace(\".\", \"\")\n",
    "        generated_data[key] = {\n",
    "            \"dialog_node_key\": node.key,\n",
    "            \"key\": key,\n",
    "            \"text\": candidate,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "cleaned_data = {}\n",
    "for key in generated_data:\n",
    "    node = human_data_train.nodes_by_key[generated_data[key]['dialog_node_key']]\n",
    "    cleaned_data[key] = generated_data[key]\n",
    "    for i in range (1, NUM_QUESTIONS+1):\n",
    "        cleaned_data[key]['text'] = cleaned_data[key]['text'].replace(f\"{i}.\", \"\").strip()\n",
    "    cleaned_data[key][\"node_text\"] = node.text\n",
    "    cleaned_data[key][\"node_type\"] = node.node_type.value\n",
    "\n",
    "with open(\"resources/en/generated/train_questions_v1.json\", \"w\") as f:\n",
    "    json.dump(cleaned_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA GENERATION V2: SHORTER QUESTIONS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [11:34:35<00:00, 520.95s/it]  \n"
     ]
    }
   ],
   "source": [
    "from data.dataset import NodeType, Question\n",
    "import time\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "system = \"\"\"You are a helpful assistant creating a list of FAQ-style questions from given facts.\n",
    "Only generate questions that can be answered by the given facts, without any external knowledge.\n",
    "Remove some information, especially nouns and named entities, between generated questions.\n",
    "Use casual language.\n",
    "Prefer short questions.\n",
    "Order the generated questions in a numbered list.\"\"\"\n",
    "\n",
    "def user(answer_text: str, num_paraphrases: int) -> str:\n",
    "    return f'Generate {num_paraphrases} short FAQ-style questions from the fact: \"{answer_text}\"'\n",
    "\n",
    "NUM_QUESTIONS = 10\n",
    "TEMPERATURE = 0.7\n",
    "MAX_NEW_TOKENS = 1024\n",
    "generated_data = {}\n",
    "\n",
    "for node in tqdm(human_data_train.nodes_by_type[NodeType.INFO]):\n",
    "    prompt = generate_prompt(system=system, user=user(node.text, NUM_QUESTIONS))\n",
    "    gen = generate_output(prompt=prompt, temperature=TEMPERATURE, max_new_tokens=MAX_NEW_TOKENS)\n",
    "    candidates = parse_output(original_question=node.text, prompt=prompt, output=gen, num_paraphrases=NUM_QUESTIONS)\n",
    "    for candidate in candidates:\n",
    "        key = str(time.time()).replace(\".\", \"\")\n",
    "        generated_data[key] = {\n",
    "            \"dialog_node_key\": node.key,\n",
    "            \"key\": key,\n",
    "            \"text\": candidate,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "cleaned_data = {}\n",
    "for key in generated_data:\n",
    "    node = human_data_train.nodes_by_key[generated_data[key]['dialog_node_key']]\n",
    "    cleaned_data[key] = generated_data[key]\n",
    "    for i in range (1, NUM_QUESTIONS+1):\n",
    "        cleaned_data[key]['text'] = cleaned_data[key]['text'].replace(f\"{i}.\", \"\").strip()\n",
    "    cleaned_data[key][\"node_text\"] = node.text\n",
    "    cleaned_data[key][\"node_type\"] = node.node_type.value\n",
    "\n",
    "with open(\"resources/en/generated/train_questions_v2.json\", \"w\") as f:\n",
    "    json.dump(cleaned_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA GENERATION V3: SHORTER QUESTIONS + SPLIT NODE CONTEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Try to generate shorter questions (-> change prompt)\n",
    "2. Try to generate more diverse questions\n",
    "    1. Detect relevant sentences in node text via NER tool (also detects time, quantities, ...)\n",
    "    2. Generate questions for whole node context, then for only relevant sentences / sub-sentences of node\n",
    "    3. Choose amount of questions to be generated depending on amount of extracted NERs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanza\n",
      "  Downloading stanza-1.5.0-py3-none-any.whl (802 kB)\n",
      "\u001b[K     |████████████████████████████████| 802 kB 14.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.3.0 in ./.env/lib/python3.10/site-packages (from stanza) (2.0.0+cu118)\n",
      "Requirement already satisfied: requests in ./.env/lib/python3.10/site-packages (from stanza) (2.28.2)\n",
      "Requirement already satisfied: numpy in ./.env/lib/python3.10/site-packages (from stanza) (1.24.2)\n",
      "Requirement already satisfied: six in ./.env/lib/python3.10/site-packages (from stanza) (1.16.0)\n",
      "Requirement already satisfied: protobuf in ./.env/lib/python3.10/site-packages (from stanza) (3.20.0)\n",
      "Requirement already satisfied: tqdm in ./.env/lib/python3.10/site-packages (from stanza) (4.65.0)\n",
      "Collecting emoji\n",
      "  Downloading emoji-2.7.0.tar.gz (361 kB)\n",
      "\u001b[K     |████████████████████████████████| 361 kB 119.2 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in ./.env/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (4.4.0)\n",
      "Requirement already satisfied: jinja2 in ./.env/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in ./.env/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (2.0.0)\n",
      "Requirement already satisfied: sympy in ./.env/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (1.11.1)\n",
      "Requirement already satisfied: filelock in ./.env/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.9.0)\n",
      "Requirement already satisfied: networkx in ./.env/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.0)\n",
      "Requirement already satisfied: cmake in ./.env/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.3.0->stanza) (3.25.0)\n",
      "Requirement already satisfied: lit in ./.env/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.3.0->stanza) (15.0.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.env/lib/python3.10/site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.env/lib/python3.10/site-packages (from requests->stanza) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.env/lib/python3.10/site-packages (from requests->stanza) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.env/lib/python3.10/site-packages (from requests->stanza) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.env/lib/python3.10/site-packages (from requests->stanza) (1.26.15)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.env/lib/python3.10/site-packages (from sympy->torch>=1.3.0->stanza) (1.2.1)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for emoji: filename=emoji-2.7.0-py2.py3-none-any.whl size=356563 sha256=d6fa52fd4eea49ae84298a10e2c77bf642c90e75abcdd4db626c8b0309a7e8f2\n",
      "  Stored in directory: /home/users2/vaethdk/.cache/pip/wheels/41/11/48/5df0b9727d5669c9174a141134f10304d1d78a3b89a4676f3d\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji, stanza\n",
      "Successfully installed emoji-2.7.0 stanza-1.5.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/fs/scratch/users/vaethdk/adviser_reisekosten/.env/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json: 216kB [00:00, 59.2MB/s]                    \n",
      "2023-08-04 16:21:45 INFO: Downloading default packages for language: en (English) ...\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.5.0/models/default.zip: 100%|██████████| 594M/594M [00:05<00:00, 115MB/s]  \n",
      "2023-08-04 16:21:56 INFO: Finished downloading models and saved to .models/.\n"
     ]
    }
   ],
   "source": [
    "# stanza.download(lang=\"en\", model_dir=\".models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 11:18:46 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json: 216kB [00:00, 49.5MB/s]                    \n",
      "2023-08-11 11:18:49 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2023-08-11 11:18:49 INFO: Using device: cuda:0\n",
      "2023-08-11 11:18:49 INFO: Loading: tokenize\n",
      "2023-08-11 11:18:52 INFO: Loading: ner\n",
      "2023-08-11 11:18:52 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline('en', processors='tokenize,ner', device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\n",
      "  \"text\": \"Barack Obama\",\n",
      "  \"type\": \"PERSON\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 12\n",
      "}, {\n",
      "  \"text\": \"Hawaii\",\n",
      "  \"type\": \"GPE\",\n",
      "  \"start_char\": 25,\n",
      "  \"end_char\": 31\n",
      "}]\n"
     ]
    }
   ],
   "source": [
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:04<00:00, 16.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL INFO NODES 80\n",
      "NODES WITH NER 33\n",
      "NODES WITHOUT NER 47\n",
      "AVG NER PER NODE WITH NER 2.090909090909091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "nodes_with_ner = 0\n",
    "nodes_without_ner = 0\n",
    "avg_node_ner = []\n",
    "\n",
    "for node in tqdm(human_data_train.nodes_by_type[NodeType.INFO]):\n",
    "    context = nlp(node.text)\n",
    "    if len(context.ents) > 0:\n",
    "        nodes_with_ner += 1\n",
    "        avg_node_ner.append(len(context.ents))\n",
    "    else:\n",
    "        nodes_without_ner += 1\n",
    "\n",
    "print(\"TOTAL INFO NODES\", len(human_data_train.nodes_by_type[NodeType.INFO]))\n",
    "print(\"NODES WITH NER\", nodes_with_ner)\n",
    "print(\"NODES WITHOUT NER\", nodes_without_ner)\n",
    "print(\"AVG NER PER NODE WITH NER\", mean(avg_node_ner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX #SENTENCES PER NODE 6\n",
      "AVG #SENTENCES PER NODE 1.8\n"
     ]
    }
   ],
   "source": [
    "avg_node_sentence_length = []\n",
    "\n",
    "for node in human_data_train.nodes_by_type[NodeType.INFO]:\n",
    "    avg_node_sentence_length.append(node.text.count(\".\"))\n",
    "\n",
    "print(\"MAX #SENTENCES PER NODE\", max(avg_node_sentence_length))\n",
    "print(\"AVG #SENTENCES PER NODE\", mean(avg_node_sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ner_sentences(node: DialogNode) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Extract all sentences from node text that mention NER's.\n",
    "    Returns them as a list of tuples, where each tuple contains\n",
    "        1. the name of the entity\n",
    "        2. the sentence containing that entity\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    context = nlp(node.text)\n",
    "    entities = context.ents\n",
    "    for entity in entities:\n",
    "        start_idx = entity.start_char\n",
    "        end_idx = entity.end_char\n",
    "        # expand start index to beginning of sentence\n",
    "        while start_idx > 0 and node.text[start_idx-1] != \".\":\n",
    "            start_idx -= 1\n",
    "        # expand end index to end of sentence\n",
    "        while end_idx < len(node.text) and node.text[end_idx-1] != \".\":\n",
    "            end_idx += 1\n",
    "        results.append((entity.text, node.text[start_idx:end_idx]))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('COVID-19', 'Please check the current COVID-19 travel warnings travel restrictions from the foreign ministry and the RKI.'), ('RKI', 'Please check the current COVID-19 travel warnings travel restrictions from the foreign ministry and the RKI.'), ('Department 4 (Administrative Department', ' In In extreme cases, authorization can be given by the leadership of Department 4 (Administrative Department).')]\n"
     ]
    }
   ],
   "source": [
    "# find a testing candidate\n",
    "for node in human_data_train.nodes_by_type[NodeType.INFO]:\n",
    "    results = extract_ner_sentences(node)\n",
    "    if len(results) > 1:\n",
    "        print(results)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please check the current COVID-19 travel warnings travel restrictions from the foreign ministry and the RKI. Business trips to high risk areas or virus variation areas are not generally not allowed. In In extreme cases, authorization can be given by the leadership of Department 4 (Administrative Department).\n"
     ]
    }
   ],
   "source": [
    "print(node.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [03:52<00:00, 77.40s/it]\n"
     ]
    }
   ],
   "source": [
    "from data.dataset import NodeType, Question\n",
    "import time\n",
    "\n",
    "system = \"\"\"You are a helpful assistant creating a list of diverse FAQ-style questions from given facts.\n",
    "Only generate questions that can be answered by the given facts, without any external knowledge.\n",
    "Use casual language.\n",
    "Prefer short questions.\n",
    "Order the generated paraphrases in a numbered list.\"\"\"\n",
    "\n",
    "def user(answer_text: str, num_paraphrases: int) -> str:\n",
    "    return f'Generate {num_paraphrases} short and diverse FAQ-style questions from the fact: \"{answer_text}\"'\n",
    "\n",
    "\n",
    "NUM_QUESTIONS = 3\n",
    "TEMPERATURE = 0.7\n",
    "MAX_NEW_TOKENS = 1024\n",
    "generated_data = {}\n",
    "\n",
    "\n",
    "for entity, sentence in tqdm(extract_ner_sentences(node)):\n",
    "    prompt = generate_prompt(system=system, user=user(sentence, NUM_QUESTIONS))\n",
    "    gen = generate_output(prompt=prompt, temperature=TEMPERATURE, max_new_tokens=MAX_NEW_TOKENS)\n",
    "    candidates = parse_output(original_question=node.text, prompt=prompt, output=gen, num_paraphrases=NUM_QUESTIONS)\n",
    "    generated_data[entity] = candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'COVID-19': ['1. What should I do to stay updated on COVID-19 travel warnings '\n",
      "              'and restrictions?',\n",
      "              '2. Where can I find the latest information on travel advisories '\n",
      "              'related to COVID-19?',\n",
      "              '3. Which organizations should I refer to for understanding the '\n",
      "              'current travel restrictions due to COVID-19?'],\n",
      " 'Department 4 (Administrative Department': ['1. Who can grant authorization '\n",
      "                                             'in extreme cases?',\n",
      "                                             \"2. What department's leadership \"\n",
      "                                             'can authorize in extreme '\n",
      "                                             'situations?',\n",
      "                                             \"3. Which department's leadership \"\n",
      "                                             'has the power to authorize in '\n",
      "                                             'extreme cases?'],\n",
      " 'RKI': ['1. What should I do to stay updated on COVID-19 travel warnings and '\n",
      "         'restrictions?',\n",
      "         '2. Where can I find the latest information on travel advisories '\n",
      "         'related to COVID-19?',\n",
      "         '3. Which organizations should I refer to for understanding the '\n",
      "         'current travel restrictions due to COVID-19?']}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(generated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m MAX_NEW_TOKENS \u001b[39m=\u001b[39m \u001b[39m1024\u001b[39m\n\u001b[1;32m     18\u001b[0m prompt \u001b[39m=\u001b[39m generate_prompt(system\u001b[39m=\u001b[39msystem, user\u001b[39m=\u001b[39muser(node\u001b[39m.\u001b[39mtext, NUM_QUESTIONS))\n\u001b[0;32m---> 19\u001b[0m gen \u001b[39m=\u001b[39m generate_output(prompt\u001b[39m=\u001b[39;49mprompt, temperature\u001b[39m=\u001b[39;49mTEMPERATURE, max_new_tokens\u001b[39m=\u001b[39;49mMAX_NEW_TOKENS)\n\u001b[1;32m     20\u001b[0m candidates \u001b[39m=\u001b[39m parse_output(original_question\u001b[39m=\u001b[39mnode\u001b[39m.\u001b[39mtext, prompt\u001b[39m=\u001b[39mprompt, output\u001b[39m=\u001b[39mgen, num_paraphrases\u001b[39m=\u001b[39mNUM_QUESTIONS)\n\u001b[1;32m     21\u001b[0m pprint\u001b[39m.\u001b[39mpprint(candidates)\n",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m, in \u001b[0;36mgenerate_output\u001b[0;34m(prompt, temperature, max_new_tokens)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_output\u001b[39m(prompt: \u001b[39mstr\u001b[39m, temperature: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0.7\u001b[39m, max_new_tokens: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m512\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor:\n\u001b[1;32m     12\u001b[0m     input_ids \u001b[39m=\u001b[39m tokenizer(prompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39minput_ids\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m---> 13\u001b[0m     output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(inputs\u001b[39m=\u001b[39;49minput_ids, temperature\u001b[39m=\u001b[39;49mtemperature, max_new_tokens\u001b[39m=\u001b[39;49mmax_new_tokens)\n\u001b[1;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/auto_gptq/modeling/_base.py:438\u001b[0m, in \u001b[0;36mBaseGPTQForCausalLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"shortcut for model.generate\"\"\"\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode(), torch\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast(device_type\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype):\n\u001b[0;32m--> 438\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/transformers/generation/utils.py:1538\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1532\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1533\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnum_return_sequences has to be 1 when doing greedy search, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1534\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut is \u001b[39m\u001b[39m{\u001b[39;00mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1535\u001b[0m         )\n\u001b[1;32m   1537\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1538\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgreedy_search(\n\u001b[1;32m   1539\u001b[0m         input_ids,\n\u001b[1;32m   1540\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1541\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1542\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1543\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1544\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1545\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1546\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1547\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1548\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1549\u001b[0m     )\n\u001b[1;32m   1551\u001b[0m \u001b[39melif\u001b[39;00m is_contrastive_search_gen_mode:\n\u001b[1;32m   1552\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/transformers/generation/utils.py:2419\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2414\u001b[0m     unfinished_sequences \u001b[39m=\u001b[39m unfinished_sequences\u001b[39m.\u001b[39mmul(\n\u001b[1;32m   2415\u001b[0m         next_tokens\u001b[39m.\u001b[39mtile(eos_token_id_tensor\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mne(eos_token_id_tensor\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mprod(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m   2416\u001b[0m     )\n\u001b[1;32m   2418\u001b[0m     \u001b[39m# stop when each sentence is finished\u001b[39;00m\n\u001b[0;32m-> 2419\u001b[0m     \u001b[39mif\u001b[39;00m unfinished_sequences\u001b[39m.\u001b[39mmax() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   2420\u001b[0m         this_peer_finished \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   2422\u001b[0m \u001b[39m# stop if we exceed the maximum length\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from data.dataset import NodeType, Question\n",
    "import time\n",
    "\n",
    "system = \"\"\"You are a helpful assistant creating a list of diverse FAQ-style questions from given facts.\n",
    "Only generate questions that can be answered by the given facts, without any external knowledge.\n",
    "Use casual language.\n",
    "Prefer short questions.\n",
    "Order the generated paraphrases in a numbered list.\"\"\"\n",
    "\n",
    "def user(answer_text: str, num_paraphrases: int) -> str:\n",
    "    return f'Generate {num_paraphrases} short and diverse FAQ-style questions from the fact: \"{answer_text}\"'\n",
    "\n",
    "\n",
    "NUM_QUESTIONS = 3\n",
    "TEMPERATURE = 0.7\n",
    "MAX_NEW_TOKENS = 1024\n",
    "\n",
    "prompt = generate_prompt(system=system, user=user(node.text, NUM_QUESTIONS))\n",
    "gen = generate_output(prompt=prompt, temperature=TEMPERATURE, max_new_tokens=MAX_NEW_TOKENS)\n",
    "candidates = parse_output(original_question=node.text, prompt=prompt, output=gen, num_paraphrases=NUM_QUESTIONS)\n",
    "pprint.pprint(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [12:37:21<00:00, 568.01s/it]  \n"
     ]
    }
   ],
   "source": [
    "system = \"\"\"You are a helpful assistant creating a list of diverse FAQ-style questions from given facts.\n",
    "Only generate questions that can be answered by the given facts, without any external knowledge.\n",
    "Use casual language.\n",
    "Prefer short questions.\n",
    "Order the generated paraphrases in a numbered list.\"\"\"\n",
    "\n",
    "def user(answer_text: str, num_paraphrases: int) -> str:\n",
    "    return f'Generate {num_paraphrases} short and diverse FAQ-style questions from the fact: \"{answer_text}\"'\n",
    "\n",
    "def user_ner(answer_text: str, ner: str, num_paraphrases: int) -> str:\n",
    "    return f'Generate {num_paraphrases} short and diverse FAQ-style questions about the entity \"{ner}\" from the fact: \"{answer_text}\"'\n",
    "\n",
    "\n",
    "NUM_QUESTIONS = 10\n",
    "NUM_QUESTIONS_PER_SENTENCE = 3\n",
    "TEMPERATURE = 0.7\n",
    "MAX_NEW_TOKENS = 1024\n",
    "generated_data = {}\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "for node in tqdm(human_data_train.nodes_by_type[NodeType.INFO]):\n",
    "    # use dict indexed by generated text to filter out duplicates\n",
    "    all_generations = {}\n",
    "    \n",
    "    # extract NERs\n",
    "    named_entities = extract_ner_sentences(node)\n",
    "\n",
    "    # Generate questions with NER sentences only, make asking about NER a requirement\n",
    "    for entity, sentence in named_entities:\n",
    "        prompt = generate_prompt(system=system, user=user_ner(node.text, entity, NUM_QUESTIONS_PER_SENTENCE))\n",
    "        gen = generate_output(prompt=prompt, temperature=TEMPERATURE, max_new_tokens=MAX_NEW_TOKENS)\n",
    "        candidates = parse_output(original_question=node.text, prompt=prompt, output=gen, num_paraphrases=NUM_QUESTIONS_PER_SENTENCE)\n",
    "        for candidate_idx, candidate in enumerate(candidates):\n",
    "            key = str(time.time()).replace(\".\", \"\")\n",
    "            cleaned_candidate = candidate.replace(f\"{candidate_idx+1}.\", \"\").strip()\n",
    "            all_generations[cleaned_candidate] = {\n",
    "                \"context\": \"ner\",\n",
    "                \"entity\": entity,\n",
    "                \"dialog_node_key\": node.key,\n",
    "                \"key\": key,\n",
    "                \"text\": cleaned_candidate\n",
    "            }\n",
    "\n",
    "    # Generate questions with whole context\n",
    "    num_node_level_questions = max(NUM_QUESTIONS_PER_SENTENCE, NUM_QUESTIONS - len(named_entities) * NUM_QUESTIONS_PER_SENTENCE)\n",
    "    prompt = generate_prompt(system=system, user=user(node.text, num_node_level_questions))\n",
    "    gen = generate_output(prompt=prompt, temperature=TEMPERATURE, max_new_tokens=MAX_NEW_TOKENS)\n",
    "    candidates = parse_output(original_question=node.text, prompt=prompt, output=gen, num_paraphrases=NUM_QUESTIONS_PER_SENTENCE)\n",
    "    for candidate_idx, candidate in enumerate(candidates):\n",
    "        key = str(time.time()).replace(\".\", \"\")\n",
    "        cleaned_candidate = candidate.replace(f\"{candidate_idx+1}.\", \"\").strip()\n",
    "        all_generations[cleaned_candidate] = {\n",
    "            \"context\": \"node\",\n",
    "            \"dialog_node_key\": node.key,\n",
    "            \"key\": key,\n",
    "            \"text\": cleaned_candidate\n",
    "        }\n",
    "    \n",
    "    # add filtered questions to generated dataset\n",
    "    for text in all_generations:\n",
    "        entry = all_generations[text]\n",
    "        generated_data[entry[\"key\"]] = entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "cleaned_data = {}\n",
    "for key in generated_data:\n",
    "    node = human_data_train.nodes_by_key[generated_data[key]['dialog_node_key']]\n",
    "    cleaned_data[key] = generated_data[key]\n",
    "    for i in range (1, NUM_QUESTIONS+1):\n",
    "        cleaned_data[key]['text'] = cleaned_data[key]['text'].replace(f\"{i}.\", \"\").strip()\n",
    "    cleaned_data[key][\"node_text\"] = node.text\n",
    "    cleaned_data[key][\"node_type\"] = node.node_type.value\n",
    "\n",
    "with open(\"resources/en/generated/train_questions_v3.json\", \"w\") as f:\n",
    "    json.dump(cleaned_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
