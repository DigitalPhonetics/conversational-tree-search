{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from typing import List, Tuple\n",
    "import re\n",
    "DEVICE = 'cuda:0'\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mount/arbeitsdaten41/projekte/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/generation/onboarding/llama\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../..')\n",
    "print(os.path.realpath(\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros(10,10,device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !GITHUB_ACTIONS=true pip install auto-gptq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- not using synonyms\n",
      "===== Dataset Statistics =====\n",
      "- files:  en/onboarding/train_graph.json en/onboading/train_graph.json\n",
      "- synonyms: False\n",
      "- depth: 13  - degree: 9\n",
      "- answers: 43\n",
      "- questions: 0\n",
      "- loaded original data: True\n",
      "- loaded generated data: False\n"
     ]
    }
   ],
   "source": [
    "from data.dataset import OnboardingGraphDataset, DataAugmentationLevel, NodeType, DialogNode, Question\n",
    "human_data_train = OnboardingGraphDataset('en/onboarding/train_graph.json', 'en/onboading/train_graph.json', False, augmentation=DataAugmentationLevel.NONE, resource_dir=\"../../../resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"TheBloke/upstage-llama-30b-instruct-2048-GPTQ\"\n",
    "model_basename = \"gptq_model-4bit--1g\"\n",
    "\n",
    "use_triton = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\n",
    "                                          use_fast=True,\n",
    "                                          cache_dir=\"/mount/arbeitsdaten/asr-2/vaethdk/resources/weights/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mount/arbeitsdaten/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/generation/onboarding/llama/generation_questions.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bmadagaskarweihe.ims.uni-stuttgart.de/mount/arbeitsdaten/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/generation/onboarding/llama/generation_questions.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m AutoGPTQForCausalLM\u001b[39m.\u001b[39;49mfrom_quantized(\u001b[39m\"\u001b[39;49m\u001b[39m/mount/arbeitsdaten/asr-2/vaethdk/resources/weights/TheBloke--upstage-llama-30b-instruct-2048-GPTQ\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmadagaskarweihe.ims.uni-stuttgart.de/mount/arbeitsdaten/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/generation/onboarding/llama/generation_questions.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m         \u001b[39m# model_basename=model_basename,\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmadagaskarweihe.ims.uni-stuttgart.de/mount/arbeitsdaten/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/generation/onboarding/llama/generation_questions.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39m# revision=\"gptq-4bit-32g-actorder_True\",\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmadagaskarweihe.ims.uni-stuttgart.de/mount/arbeitsdaten/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/generation/onboarding/llama/generation_questions.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m         use_safetensors\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmadagaskarweihe.ims.uni-stuttgart.de/mount/arbeitsdaten/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/generation/onboarding/llama/generation_questions.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmadagaskarweihe.ims.uni-stuttgart.de/mount/arbeitsdaten/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/generation/onboarding/llama/generation_questions.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcuda:0\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmadagaskarweihe.ims.uni-stuttgart.de/mount/arbeitsdaten/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/generation/onboarding/llama/generation_questions.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         use_triton\u001b[39m=\u001b[39;49muse_triton,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmadagaskarweihe.ims.uni-stuttgart.de/mount/arbeitsdaten/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/generation/onboarding/llama/generation_questions.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m         quantize_config\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/auto_gptq/modeling/auto.py:105\u001b[0m, in \u001b[0;36mAutoGPTQForCausalLM.from_quantized\u001b[0;34m(cls, model_name_or_path, device_map, max_memory, device, low_cpu_mem_usage, use_triton, inject_fused_attention, inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors, trust_remote_code, warmup_triton, trainable, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39m# TODO: do we need this filtering of kwargs? @PanQiWei is there a reason we can't just pass all kwargs?\u001b[39;00m\n\u001b[1;32m    100\u001b[0m keywords \u001b[39m=\u001b[39m {\n\u001b[1;32m    101\u001b[0m     key: kwargs[key]\n\u001b[1;32m    102\u001b[0m     \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(signature(quant_func)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mkeys()) \u001b[39m+\u001b[39m huggingface_kwargs\n\u001b[1;32m    103\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m kwargs\n\u001b[1;32m    104\u001b[0m }\n\u001b[0;32m--> 105\u001b[0m \u001b[39mreturn\u001b[39;00m quant_func(\n\u001b[1;32m    106\u001b[0m     model_name_or_path\u001b[39m=\u001b[39;49mmodel_name_or_path,\n\u001b[1;32m    107\u001b[0m     device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m    108\u001b[0m     max_memory\u001b[39m=\u001b[39;49mmax_memory,\n\u001b[1;32m    109\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    110\u001b[0m     low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m    111\u001b[0m     use_triton\u001b[39m=\u001b[39;49muse_triton,\n\u001b[1;32m    112\u001b[0m     inject_fused_attention\u001b[39m=\u001b[39;49minject_fused_attention,\n\u001b[1;32m    113\u001b[0m     inject_fused_mlp\u001b[39m=\u001b[39;49minject_fused_mlp,\n\u001b[1;32m    114\u001b[0m     use_cuda_fp16\u001b[39m=\u001b[39;49muse_cuda_fp16,\n\u001b[1;32m    115\u001b[0m     quantize_config\u001b[39m=\u001b[39;49mquantize_config,\n\u001b[1;32m    116\u001b[0m     model_basename\u001b[39m=\u001b[39;49mmodel_basename,\n\u001b[1;32m    117\u001b[0m     use_safetensors\u001b[39m=\u001b[39;49muse_safetensors,\n\u001b[1;32m    118\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49mtrust_remote_code,\n\u001b[1;32m    119\u001b[0m     warmup_triton\u001b[39m=\u001b[39;49mwarmup_triton,\n\u001b[1;32m    120\u001b[0m     trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[1;32m    121\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkeywords\n\u001b[1;32m    122\u001b[0m )\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/auto_gptq/modeling/_base.py:803\u001b[0m, in \u001b[0;36mBaseGPTQForCausalLM.from_quantized\u001b[0;34m(cls, model_name_or_path, device_map, max_memory, device, low_cpu_mem_usage, use_triton, torch_dtype, inject_fused_attention, inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors, trust_remote_code, warmup_triton, trainable, **kwargs)\u001b[0m\n\u001b[1;32m    800\u001b[0m             logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m not been quantized, will be ignored when make_quant.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    801\u001b[0m             \u001b[39mdel\u001b[39;00m layers[name]\n\u001b[0;32m--> 803\u001b[0m     make_quant(\n\u001b[1;32m    804\u001b[0m         model,\n\u001b[1;32m    805\u001b[0m         layers,\n\u001b[1;32m    806\u001b[0m         quantize_config\u001b[39m.\u001b[39;49mbits,\n\u001b[1;32m    807\u001b[0m         quantize_config\u001b[39m.\u001b[39;49mgroup_size,\n\u001b[1;32m    808\u001b[0m         use_triton\u001b[39m=\u001b[39;49muse_triton,\n\u001b[1;32m    809\u001b[0m         use_cuda_fp16\u001b[39m=\u001b[39;49muse_cuda_fp16,\n\u001b[1;32m    810\u001b[0m         desc_act\u001b[39m=\u001b[39;49mquantize_config\u001b[39m.\u001b[39;49mdesc_act,\n\u001b[1;32m    811\u001b[0m         trainable\u001b[39m=\u001b[39;49mtrainable\n\u001b[1;32m    812\u001b[0m     )\n\u001b[1;32m    813\u001b[0m     model\u001b[39m.\u001b[39mtie_weights()\n\u001b[1;32m    815\u001b[0m \u001b[39m# == step3: load checkpoint and dispatch == #\u001b[39;00m\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/auto_gptq/modeling/_utils.py:92\u001b[0m, in \u001b[0;36mmake_quant\u001b[0;34m(module, names, bits, group_size, name, use_triton, use_cuda_fp16, desc_act, trainable)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[39msetattr\u001b[39m(module, attr, new_layer\u001b[39m.\u001b[39mto(ori_layer_device))\n\u001b[1;32m     91\u001b[0m \u001b[39mfor\u001b[39;00m name1, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39mnamed_children():\n\u001b[0;32m---> 92\u001b[0m     make_quant(\n\u001b[1;32m     93\u001b[0m         child,\n\u001b[1;32m     94\u001b[0m         names,\n\u001b[1;32m     95\u001b[0m         bits,\n\u001b[1;32m     96\u001b[0m         group_size,\n\u001b[1;32m     97\u001b[0m         name \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m name1 \u001b[39mif\u001b[39;49;00m name \u001b[39m!=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39melse\u001b[39;49;00m name1,\n\u001b[1;32m     98\u001b[0m         use_triton\u001b[39m=\u001b[39;49muse_triton,\n\u001b[1;32m     99\u001b[0m         use_cuda_fp16\u001b[39m=\u001b[39;49muse_cuda_fp16,\n\u001b[1;32m    100\u001b[0m         desc_act\u001b[39m=\u001b[39;49mdesc_act,\n\u001b[1;32m    101\u001b[0m         trainable\u001b[39m=\u001b[39;49mtrainable\n\u001b[1;32m    102\u001b[0m     )\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/auto_gptq/modeling/_utils.py:92\u001b[0m, in \u001b[0;36mmake_quant\u001b[0;34m(module, names, bits, group_size, name, use_triton, use_cuda_fp16, desc_act, trainable)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[39msetattr\u001b[39m(module, attr, new_layer\u001b[39m.\u001b[39mto(ori_layer_device))\n\u001b[1;32m     91\u001b[0m \u001b[39mfor\u001b[39;00m name1, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39mnamed_children():\n\u001b[0;32m---> 92\u001b[0m     make_quant(\n\u001b[1;32m     93\u001b[0m         child,\n\u001b[1;32m     94\u001b[0m         names,\n\u001b[1;32m     95\u001b[0m         bits,\n\u001b[1;32m     96\u001b[0m         group_size,\n\u001b[1;32m     97\u001b[0m         name \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m name1 \u001b[39mif\u001b[39;49;00m name \u001b[39m!=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39melse\u001b[39;49;00m name1,\n\u001b[1;32m     98\u001b[0m         use_triton\u001b[39m=\u001b[39;49muse_triton,\n\u001b[1;32m     99\u001b[0m         use_cuda_fp16\u001b[39m=\u001b[39;49muse_cuda_fp16,\n\u001b[1;32m    100\u001b[0m         desc_act\u001b[39m=\u001b[39;49mdesc_act,\n\u001b[1;32m    101\u001b[0m         trainable\u001b[39m=\u001b[39;49mtrainable\n\u001b[1;32m    102\u001b[0m     )\n",
      "    \u001b[0;31m[... skipping similar frames: make_quant at line 92 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/auto_gptq/modeling/_utils.py:92\u001b[0m, in \u001b[0;36mmake_quant\u001b[0;34m(module, names, bits, group_size, name, use_triton, use_cuda_fp16, desc_act, trainable)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[39msetattr\u001b[39m(module, attr, new_layer\u001b[39m.\u001b[39mto(ori_layer_device))\n\u001b[1;32m     91\u001b[0m \u001b[39mfor\u001b[39;00m name1, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39mnamed_children():\n\u001b[0;32m---> 92\u001b[0m     make_quant(\n\u001b[1;32m     93\u001b[0m         child,\n\u001b[1;32m     94\u001b[0m         names,\n\u001b[1;32m     95\u001b[0m         bits,\n\u001b[1;32m     96\u001b[0m         group_size,\n\u001b[1;32m     97\u001b[0m         name \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m name1 \u001b[39mif\u001b[39;49;00m name \u001b[39m!=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39melse\u001b[39;49;00m name1,\n\u001b[1;32m     98\u001b[0m         use_triton\u001b[39m=\u001b[39;49muse_triton,\n\u001b[1;32m     99\u001b[0m         use_cuda_fp16\u001b[39m=\u001b[39;49muse_cuda_fp16,\n\u001b[1;32m    100\u001b[0m         desc_act\u001b[39m=\u001b[39;49mdesc_act,\n\u001b[1;32m    101\u001b[0m         trainable\u001b[39m=\u001b[39;49mtrainable\n\u001b[1;32m    102\u001b[0m     )\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/auto_gptq/modeling/_utils.py:88\u001b[0m, in \u001b[0;36mmake_quant\u001b[0;34m(module, names, bits, group_size, name, use_triton, use_cuda_fp16, desc_act, trainable)\u001b[0m\n\u001b[1;32m     84\u001b[0m     new_layer \u001b[39m=\u001b[39m QuantLinear(\n\u001b[1;32m     85\u001b[0m         bits, group_size, in_features, out_features, \u001b[39mTrue\u001b[39;00m, use_cuda_fp16\u001b[39m=\u001b[39muse_cuda_fp16, trainable\u001b[39m=\u001b[39mtrainable\n\u001b[1;32m     86\u001b[0m     )\n\u001b[1;32m     87\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 88\u001b[0m     new_layer \u001b[39m=\u001b[39m QuantLinear(bits, group_size, in_features, out_features, \u001b[39mTrue\u001b[39;49;00m, trainable\u001b[39m=\u001b[39;49mtrainable)\n\u001b[1;32m     89\u001b[0m new_layer\u001b[39m.\u001b[39mdevice \u001b[39m=\u001b[39m ori_layer_device\n\u001b[1;32m     90\u001b[0m \u001b[39msetattr\u001b[39m(module, attr, new_layer\u001b[39m.\u001b[39mto(ori_layer_device))\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/auto_gptq/nn_modules/qlinear/qlinear_cuda.py:48\u001b[0m, in \u001b[0;36mQuantLinear.__init__\u001b[0;34m(self, bits, group_size, infeatures, outfeatures, bias, kernel_switch_threshold, trainable)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroup_size \u001b[39m=\u001b[39m group_size \u001b[39mif\u001b[39;00m group_size \u001b[39m!=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m infeatures\n\u001b[1;32m     44\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxq \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbits \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     46\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_buffer(\n\u001b[1;32m     47\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mqweight\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m---> 48\u001b[0m     torch\u001b[39m.\u001b[39;49mzeros((infeatures \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m \u001b[39m32\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbits, outfeatures), dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mint32)\n\u001b[1;32m     49\u001b[0m )\n\u001b[1;32m     50\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_buffer(\n\u001b[1;32m     51\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mqzeros\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     52\u001b[0m     torch\u001b[39m.\u001b[39mzeros((math\u001b[39m.\u001b[39mceil(infeatures \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroup_size), outfeatures \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m32\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbits), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint32)\n\u001b[1;32m     53\u001b[0m )\n\u001b[1;32m     54\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_buffer(\n\u001b[1;32m     55\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mscales\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     56\u001b[0m     torch\u001b[39m.\u001b[39mzeros((math\u001b[39m.\u001b[39mceil(infeatures \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroup_size), outfeatures), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat16)\n\u001b[1;32m     57\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = AutoGPTQForCausalLM.from_quantized(\"/mount/arbeitsdaten/asr-2/vaethdk/resources/weights/TheBloke--upstage-llama-30b-instruct-2048-GPTQ\",\n",
    "        # model_basename=model_basename,\n",
    "        # revision=\"gptq-4bit-32g-actorder_True\",\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=False,\n",
    "        device=\"cuda:0\",\n",
    "        use_triton=use_triton,\n",
    "        quantize_config=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System:\n",
    "{System}\n",
    "\n",
    "### User:\n",
    "{User}\n",
    "\n",
    "### Assistant:\n",
    "{Assistant}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"You are a helpful assistant creating a list of FAQ-style questions from given facts.\n",
    "Only generate questions that can be answered by the given facts, without any external knowledge.\n",
    "Remove some information, especially nouns and named entities, between generated questions.\n",
    "Use casual language.\n",
    "Order the generated paraphrases in a numbered list.\"\"\"\n",
    "user = 'Generate 10 FAQ-style questions from the fact: \"In the US, you are entitled to 30$ per day, minus any free meals which you choose to decline.\"'\n",
    "\n",
    "\n",
    "prompt = f\"\"\"\n",
    "### System:\n",
    "{system}\n",
    "\n",
    "### User:\n",
    "{user}\n",
    "\n",
    "### Assistant:\"\"\"\n",
    "\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mount/arbeitsdaten/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/generation/onboarding/llama/generation_questions.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmadagaskarweihe.ims.uni-stuttgart.de/mount/arbeitsdaten/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/generation/onboarding/llama/generation_questions.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer(prompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39minput_ids\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bmadagaskarweihe.ims.uni-stuttgart.de/mount/arbeitsdaten/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/generation/onboarding/llama/generation_questions.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(inputs\u001b[39m=\u001b[39;49minput_ids, temperature\u001b[39m=\u001b[39;49m\u001b[39m0.7\u001b[39;49m, max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmadagaskarweihe.ims.uni-stuttgart.de/mount/arbeitsdaten/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/generation/onboarding/llama/generation_questions.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(tokenizer\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m]))\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/auto_gptq/modeling/_base.py:438\u001b[0m, in \u001b[0;36mBaseGPTQForCausalLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"shortcut for model.generate\"\"\"\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode(), torch\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast(device_type\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype):\n\u001b[0;32m--> 438\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/transformers/generation/utils.py:1522\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1517\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnum_return_sequences has to be 1 when doing greedy search, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1518\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut is \u001b[39m\u001b[39m{\u001b[39;00mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1519\u001b[0m         )\n\u001b[1;32m   1521\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1522\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgreedy_search(\n\u001b[1;32m   1523\u001b[0m         input_ids,\n\u001b[1;32m   1524\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1525\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1526\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1527\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1528\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1529\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1530\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1531\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1532\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1533\u001b[0m     )\n\u001b[1;32m   1535\u001b[0m \u001b[39melif\u001b[39;00m is_contrastive_search_gen_mode:\n\u001b[1;32m   1536\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/transformers/generation/utils.py:2339\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2336\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2338\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2339\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2340\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2341\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2342\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2343\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2344\u001b[0m )\n\u001b[1;32m   2346\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2347\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/transformers/models/llama/modeling_llama.py:688\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    685\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m    687\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 688\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m    689\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    690\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    691\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    692\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    693\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    694\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    695\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    696\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    697\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    698\u001b[0m )\n\u001b[1;32m    700\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    701\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/transformers/models/llama/modeling_llama.py:578\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    570\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    571\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    572\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    576\u001b[0m     )\n\u001b[1;32m    577\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 578\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    579\u001b[0m         hidden_states,\n\u001b[1;32m    580\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    581\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    582\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    583\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    584\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    585\u001b[0m     )\n\u001b[1;32m    587\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    589\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/transformers/models/llama/modeling_llama.py:292\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    289\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    291\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    293\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    294\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    295\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    296\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    297\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    298\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    299\u001b[0m )\n\u001b[1;32m    300\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    302\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/auto_gptq/nn_modules/fused_llama_attn.py:63\u001b[0m, in \u001b[0;36mFusedLlamaAttentionForQuantizedModel.forward\u001b[0;34m(self, hidden_states, past_key_value, attention_mask, position_ids, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     kv_seq_len \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m past_key_value[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\n\u001b[0;32m---> 63\u001b[0m cos, sin \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrotary_emb(value_states, seq_len\u001b[39m=\u001b[39;49mkv_seq_len)\n\u001b[1;32m     64\u001b[0m query_states, key_states \u001b[39m=\u001b[39m apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n\u001b[1;32m     65\u001b[0m \u001b[39m# [bsz, nh, t, hd]\u001b[39;00m\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/transformers/models/llama/modeling_llama.py:119\u001b[0m, in \u001b[0;36mLlamaRotaryEmbedding.forward\u001b[0;34m(self, x, seq_len)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_buffer(\u001b[39m\"\u001b[39m\u001b[39mcos_cached\u001b[39m\u001b[39m\"\u001b[39m, emb\u001b[39m.\u001b[39mcos()[\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, :, :], persistent\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    116\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_buffer(\u001b[39m\"\u001b[39m\u001b[39msin_cached\u001b[39m\u001b[39m\"\u001b[39m, emb\u001b[39m.\u001b[39msin()[\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, :, :], persistent\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    117\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    118\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcos_cached[:, :, :seq_len, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mdtype),\n\u001b[0;32m--> 119\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msin_cached[:, :, :seq_len, \u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m]\u001b[39m.\u001b[39;49mto(dtype\u001b[39m=\u001b[39;49mx\u001b[39m.\u001b[39;49mdtype),\n\u001b[1;32m    120\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# input_ids = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\n",
    "# output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n",
    "# print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # try batch processing\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# input_ids = tokenizer([prompt, prompt + prompt], return_tensors='pt', padding=True, ).input_ids.cuda()\n",
    "# output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Question Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(system: str, user: str) -> str:\n",
    "    return f\"\"\"\n",
    "    ### System:\n",
    "    {system}\n",
    "\n",
    "    ### User:\n",
    "    {user}\n",
    "\n",
    "    ### Assistant:\"\"\"\n",
    "\n",
    "def generate_output(prompt: str, temperature: float = 0.7, max_new_tokens: int = 512) -> torch.FloatTensor:\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\n",
    "    output = model.generate(inputs=input_ids, temperature=temperature, max_new_tokens=max_new_tokens)\n",
    "    return tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- not using synonyms\n",
      "===== Dataset Statistics =====\n",
      "- files:  en/onboarding/train_graph.json en/onboading/train_graph.json\n",
      "- synonyms: False\n",
      "- depth: 13  - degree: 9\n",
      "- answers: 43\n",
      "- questions: 0\n",
      "- loaded original data: True\n",
      "- loaded generated data: False\n"
     ]
    }
   ],
   "source": [
    "human_data_train = OnboardingGraphDataset('en/onboarding/train_graph.json', 'en/onboading/train_graph.json', False, augmentation=DataAugmentationLevel.NONE, resource_dir=\"../../../resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that we don't have any answer synonyms\n",
    "for node in human_data_train.node_list:\n",
    "    for question in node.questions:\n",
    "        human_data_train.question_list.remove(question)\n",
    "        del human_data_train.questions_by_key[question.key]\n",
    "    node.questions.clear()\n",
    "assert len(human_data_train.question_list) == 0\n",
    "assert len(human_data_train.questions_by_key) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_output(original_question: str, prompt: str, output: str, num_paraphrases: int) -> List[str]:\n",
    "    # remove prompt from output first (ends at ### ASSISTANT: )\n",
    "    questions = []\n",
    "    cleaned = output[len(prompt):]\n",
    "    \n",
    "    if not \"1.\" in cleaned: \n",
    "        print(\"NO LIST FOR QUESTION\", original_question)\n",
    "        return questions\n",
    "    \n",
    "    for i in range(1, num_paraphrases+1):\n",
    "        if not f\"{i}.\" in cleaned: \n",
    "            print(f\" - NO {i}. CANDIDATE FOR QUESTION\", original_question)\n",
    "            continue\n",
    "\n",
    "        start_idx = cleaned.find(f\"{i}.\") # find i. line\n",
    "        end_idx = cleaned.find(\"\\n\", start_idx) # read until line end \n",
    "        if i == num_paraphrases and end_idx == -1:\n",
    "            # last line might not have line break\n",
    "            end_idx = len(cleaned)\n",
    "        if start_idx == -1 or end_idx == -1:\n",
    "            print(f\" - INDEX PROBLEM FOR {i}. CANDIDATE: ({start_idx}, {end_idx})\")\n",
    "            continue\n",
    "        # parse answer\n",
    "        questions.append(cleaned[start_idx:end_idx].replace(\"</s>\", \"\").strip())\n",
    "\n",
    "        cleaned = cleaned[end_idx:] # remove i. line\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 7/61 [51:33<6:37:45, 441.95s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mount/arbeitsdaten/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/generation/onboarding/llama/generation_questions.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmadagaskarweihe.ims.uni-stuttgart.de/mount/arbeitsdaten/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/generation/onboarding/llama/generation_questions.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m tqdm(human_data_train\u001b[39m.\u001b[39mnodes_by_type[NodeType\u001b[39m.\u001b[39mINFO]):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmadagaskarweihe.ims.uni-stuttgart.de/mount/arbeitsdaten/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/generation/onboarding/llama/generation_questions.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     prompt \u001b[39m=\u001b[39m generate_prompt(system\u001b[39m=\u001b[39msystem, user\u001b[39m=\u001b[39muser(node\u001b[39m.\u001b[39mtext, NUM_QUESTIONS))\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bmadagaskarweihe.ims.uni-stuttgart.de/mount/arbeitsdaten/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/generation/onboarding/llama/generation_questions.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     gen \u001b[39m=\u001b[39m generate_output(prompt\u001b[39m=\u001b[39;49mprompt, temperature\u001b[39m=\u001b[39;49mTEMPERATURE, max_new_tokens\u001b[39m=\u001b[39;49mMAX_NEW_TOKENS)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmadagaskarweihe.ims.uni-stuttgart.de/mount/arbeitsdaten/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/generation/onboarding/llama/generation_questions.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     candidates \u001b[39m=\u001b[39m parse_output(original_question\u001b[39m=\u001b[39mnode\u001b[39m.\u001b[39mtext, prompt\u001b[39m=\u001b[39mprompt, output\u001b[39m=\u001b[39mgen, num_paraphrases\u001b[39m=\u001b[39mNUM_QUESTIONS)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmadagaskarweihe.ims.uni-stuttgart.de/mount/arbeitsdaten/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/generation/onboarding/llama/generation_questions.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mfor\u001b[39;00m candidate \u001b[39min\u001b[39;00m candidates:\n",
      "\u001b[1;32m/mount/arbeitsdaten/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/generation/onboarding/llama/generation_questions.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmadagaskarweihe.ims.uni-stuttgart.de/mount/arbeitsdaten/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/generation/onboarding/llama/generation_questions.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_output\u001b[39m(prompt: \u001b[39mstr\u001b[39m, temperature: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0.7\u001b[39m, max_new_tokens: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m512\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmadagaskarweihe.ims.uni-stuttgart.de/mount/arbeitsdaten/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/generation/onboarding/llama/generation_questions.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     input_ids \u001b[39m=\u001b[39m tokenizer(prompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39minput_ids\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bmadagaskarweihe.ims.uni-stuttgart.de/mount/arbeitsdaten/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/generation/onboarding/llama/generation_questions.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(inputs\u001b[39m=\u001b[39;49minput_ids, temperature\u001b[39m=\u001b[39;49mtemperature, max_new_tokens\u001b[39m=\u001b[39;49mmax_new_tokens)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmadagaskarweihe.ims.uni-stuttgart.de/mount/arbeitsdaten/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/generation/onboarding/llama/generation_questions.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/auto_gptq/modeling/_base.py:438\u001b[0m, in \u001b[0;36mBaseGPTQForCausalLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"shortcut for model.generate\"\"\"\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode(), torch\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast(device_type\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype):\n\u001b[0;32m--> 438\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/transformers/generation/utils.py:1522\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1517\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnum_return_sequences has to be 1 when doing greedy search, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1518\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut is \u001b[39m\u001b[39m{\u001b[39;00mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1519\u001b[0m         )\n\u001b[1;32m   1521\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1522\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgreedy_search(\n\u001b[1;32m   1523\u001b[0m         input_ids,\n\u001b[1;32m   1524\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1525\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1526\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1527\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1528\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1529\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1530\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1531\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1532\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1533\u001b[0m     )\n\u001b[1;32m   1535\u001b[0m \u001b[39melif\u001b[39;00m is_contrastive_search_gen_mode:\n\u001b[1;32m   1536\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/transformers/generation/utils.py:2339\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2336\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2338\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2339\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2340\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2341\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2342\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2343\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2344\u001b[0m )\n\u001b[1;32m   2346\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2347\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/transformers/models/llama/modeling_llama.py:688\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    685\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m    687\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 688\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m    689\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    690\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    691\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    692\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    693\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    694\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    695\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    696\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    697\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    698\u001b[0m )\n\u001b[1;32m    700\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    701\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/transformers/models/llama/modeling_llama.py:578\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    570\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    571\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    572\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    576\u001b[0m     )\n\u001b[1;32m    577\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 578\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    579\u001b[0m         hidden_states,\n\u001b[1;32m    580\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    581\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    582\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    583\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    584\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    585\u001b[0m     )\n\u001b[1;32m    587\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    589\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/transformers/models/llama/modeling_llama.py:292\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    289\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    291\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    293\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    294\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    295\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    296\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    297\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    298\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    299\u001b[0m )\n\u001b[1;32m    300\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    302\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/auto_gptq/nn_modules/fused_llama_attn.py:121\u001b[0m, in \u001b[0;36mFusedLlamaAttentionForQuantizedModel.forward\u001b[0;34m(self, hidden_states, past_key_value, attention_mask, position_ids, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    119\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mreshape(bsz, q_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size)\n\u001b[0;32m--> 121\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mo_proj(attn_output)\n\u001b[1;32m    123\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m output_attentions:\n\u001b[1;32m    124\u001b[0m     attn_weights \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/auto_gptq/nn_modules/qlinear/qlinear_cuda.py:254\u001b[0m, in \u001b[0;36mQuantLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    252\u001b[0m num_itr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg_idx\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m/\u001b[39m\u001b[39m/\u001b[39mx\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    253\u001b[0m \u001b[39mif\u001b[39;00m num_itr \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 254\u001b[0m     weights \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscales[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg_idx\u001b[39m.\u001b[39mlong()] \u001b[39m*\u001b[39m (weight \u001b[39m-\u001b[39m zeros[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mg_idx\u001b[39m.\u001b[39mlong()]))\n\u001b[1;32m    255\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    256\u001b[0m     num_dim \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg_idx\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m/\u001b[39m\u001b[39m/\u001b[39mnum_itr\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/torch/nn/modules/module.py:1601\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1598\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_backward_pre_hooks\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m   1599\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39m=\u001b[39m OrderedDict()\n\u001b[0;32m-> 1601\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tensor, \u001b[39m'\u001b[39m\u001b[39mModule\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m   1602\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m   1603\u001b[0m         _parameters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from data.dataset import NodeType, Question\n",
    "import time\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "system = \"\"\"You are a helpful assistant creating a list of FAQ-style questions from given facts.\n",
    "Only generate questions that can be answered by the given facts, without any external knowledge.\n",
    "Remove some information, especially nouns and named entities, between generated questions.\n",
    "Use casual language.\n",
    "Order the generated paraphrases in a numbered list.\"\"\"\n",
    "\n",
    "def user(answer_text: str, num_paraphrases: int) -> str:\n",
    "    return f'Generate {num_paraphrases} FAQ-style questions from the fact: \"{answer_text}\"'\n",
    "\n",
    "NUM_QUESTIONS = 10\n",
    "TEMPERATURE = 0.7\n",
    "MAX_NEW_TOKENS = 1024\n",
    "generated_data = {}\n",
    "\n",
    "for node in tqdm(human_data_train.nodes_by_type[NodeType.INFO]):\n",
    "    prompt = generate_prompt(system=system, user=user(node.text, NUM_QUESTIONS))\n",
    "    gen = generate_output(prompt=prompt, temperature=TEMPERATURE, max_new_tokens=MAX_NEW_TOKENS)\n",
    "    candidates = parse_output(original_question=node.text, prompt=prompt, output=gen, num_paraphrases=NUM_QUESTIONS)\n",
    "    for candidate in candidates:\n",
    "        key = str(time.time()).replace(\".\", \"\")\n",
    "        generated_data[key] = {\n",
    "            \"dialog_node_key\": node.key,\n",
    "            \"key\": key,\n",
    "            \"text\": candidate,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "cleaned_data = {}\n",
    "for key in generated_data:\n",
    "    node = human_data_train.nodes_by_key[generated_data[key]['dialog_node_key']]\n",
    "    cleaned_data[key] = generated_data[key]\n",
    "    for i in range (1, NUM_QUESTIONS+1):\n",
    "        cleaned_data[key]['text'] = cleaned_data[key]['text'].replace(f\"{i}.\", \"\").strip()\n",
    "    cleaned_data[key][\"node_text\"] = node.text\n",
    "    cleaned_data[key][\"node_type\"] = node.node_type.value\n",
    "\n",
    "with open(\"resources/en/onboarding/generated/train_questions_v1.json\", \"w\") as f:\n",
    "    json.dump(cleaned_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA GENERATION V2: SHORTER QUESTIONS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [11:34:35<00:00, 520.95s/it]  \n"
     ]
    }
   ],
   "source": [
    "from data.dataset import NodeType, Question\n",
    "import time\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "system = \"\"\"You are a helpful assistant creating a list of FAQ-style questions from given facts.\n",
    "Only generate questions that can be answered by the given facts, without any external knowledge.\n",
    "Remove some information, especially nouns and named entities, between generated questions.\n",
    "Use casual language.\n",
    "Prefer short questions.\n",
    "Order the generated questions in a numbered list.\"\"\"\n",
    "\n",
    "def user(answer_text: str, num_paraphrases: int) -> str:\n",
    "    return f'Generate {num_paraphrases} short FAQ-style questions from the fact: \"{answer_text}\"'\n",
    "\n",
    "NUM_QUESTIONS = 10\n",
    "TEMPERATURE = 0.7\n",
    "MAX_NEW_TOKENS = 1024\n",
    "generated_data = {}\n",
    "\n",
    "for node in tqdm(human_data_train.nodes_by_type[NodeType.INFO]):\n",
    "    prompt = generate_prompt(system=system, user=user(node.text, NUM_QUESTIONS))\n",
    "    gen = generate_output(prompt=prompt, temperature=TEMPERATURE, max_new_tokens=MAX_NEW_TOKENS)\n",
    "    candidates = parse_output(original_question=node.text, prompt=prompt, output=gen, num_paraphrases=NUM_QUESTIONS)\n",
    "    for candidate in candidates:\n",
    "        key = str(time.time()).replace(\".\", \"\")\n",
    "        generated_data[key] = {\n",
    "            \"dialog_node_key\": node.key,\n",
    "            \"key\": key,\n",
    "            \"text\": candidate,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "cleaned_data = {}\n",
    "for key in generated_data:\n",
    "    node = human_data_train.nodes_by_key[generated_data[key]['dialog_node_key']]\n",
    "    cleaned_data[key] = generated_data[key]\n",
    "    for i in range (1, NUM_QUESTIONS+1):\n",
    "        cleaned_data[key]['text'] = cleaned_data[key]['text'].replace(f\"{i}.\", \"\").strip()\n",
    "    cleaned_data[key][\"node_text\"] = node.text\n",
    "    cleaned_data[key][\"node_type\"] = node.node_type.value\n",
    "\n",
    "with open(\"resources/en/onboarding/generated/train_questions_v2.json\", \"w\") as f:\n",
    "    json.dump(cleaned_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA GENERATION V3: SHORTER QUESTIONS + SPLIT NODE CONTEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Try to generate shorter questions (-> change prompt)\n",
    "2. Try to generate more diverse questions\n",
    "    1. Detect relevant sentences in node text via NER tool (also detects time, quantities, ...)\n",
    "    2. Generate questions for whole node context, then for only relevant sentences / sub-sentences of node\n",
    "    3. Choose amount of questions to be generated depending on amount of extracted NERs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanza\n",
      "  Downloading stanza-1.5.0-py3-none-any.whl (802 kB)\n",
      "\u001b[K     |████████████████████████████████| 802 kB 14.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.3.0 in ./.env/lib/python3.10/site-packages (from stanza) (2.0.0+cu118)\n",
      "Requirement already satisfied: requests in ./.env/lib/python3.10/site-packages (from stanza) (2.28.2)\n",
      "Requirement already satisfied: numpy in ./.env/lib/python3.10/site-packages (from stanza) (1.24.2)\n",
      "Requirement already satisfied: six in ./.env/lib/python3.10/site-packages (from stanza) (1.16.0)\n",
      "Requirement already satisfied: protobuf in ./.env/lib/python3.10/site-packages (from stanza) (3.20.0)\n",
      "Requirement already satisfied: tqdm in ./.env/lib/python3.10/site-packages (from stanza) (4.65.0)\n",
      "Collecting emoji\n",
      "  Downloading emoji-2.7.0.tar.gz (361 kB)\n",
      "\u001b[K     |████████████████████████████████| 361 kB 119.2 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in ./.env/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (4.4.0)\n",
      "Requirement already satisfied: jinja2 in ./.env/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in ./.env/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (2.0.0)\n",
      "Requirement already satisfied: sympy in ./.env/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (1.11.1)\n",
      "Requirement already satisfied: filelock in ./.env/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.9.0)\n",
      "Requirement already satisfied: networkx in ./.env/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.0)\n",
      "Requirement already satisfied: cmake in ./.env/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.3.0->stanza) (3.25.0)\n",
      "Requirement already satisfied: lit in ./.env/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.3.0->stanza) (15.0.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.env/lib/python3.10/site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.env/lib/python3.10/site-packages (from requests->stanza) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.env/lib/python3.10/site-packages (from requests->stanza) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.env/lib/python3.10/site-packages (from requests->stanza) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.env/lib/python3.10/site-packages (from requests->stanza) (1.26.15)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.env/lib/python3.10/site-packages (from sympy->torch>=1.3.0->stanza) (1.2.1)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for emoji: filename=emoji-2.7.0-py2.py3-none-any.whl size=356563 sha256=d6fa52fd4eea49ae84298a10e2c77bf642c90e75abcdd4db626c8b0309a7e8f2\n",
      "  Stored in directory: /home/users2/vaethdk/.cache/pip/wheels/41/11/48/5df0b9727d5669c9174a141134f10304d1d78a3b89a4676f3d\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji, stanza\n",
      "Successfully installed emoji-2.7.0 stanza-1.5.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/fs/scratch/users/vaethdk/adviser_reisekosten/.env/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json: 216kB [00:00, 59.2MB/s]                    \n",
      "2023-08-04 16:21:45 INFO: Downloading default packages for language: en (English) ...\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.5.0/models/default.zip: 100%|██████████| 594M/594M [00:05<00:00, 115MB/s]  \n",
      "2023-08-04 16:21:56 INFO: Finished downloading models and saved to .models/.\n"
     ]
    }
   ],
   "source": [
    "# stanza.download(lang=\"en\", model_dir=\".models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 11:18:46 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json: 216kB [00:00, 49.5MB/s]                    \n",
      "2023-08-11 11:18:49 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2023-08-11 11:18:49 INFO: Using device: cuda:0\n",
      "2023-08-11 11:18:49 INFO: Loading: tokenize\n",
      "2023-08-11 11:18:52 INFO: Loading: ner\n",
      "2023-08-11 11:18:52 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline('en', processors='tokenize,ner', device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\n",
      "  \"text\": \"Barack Obama\",\n",
      "  \"type\": \"PERSON\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 12\n",
      "}, {\n",
      "  \"text\": \"Hawaii\",\n",
      "  \"type\": \"GPE\",\n",
      "  \"start_char\": 25,\n",
      "  \"end_char\": 31\n",
      "}]\n"
     ]
    }
   ],
   "source": [
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:04<00:00, 16.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL INFO NODES 80\n",
      "NODES WITH NER 33\n",
      "NODES WITHOUT NER 47\n",
      "AVG NER PER NODE WITH NER 2.090909090909091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "nodes_with_ner = 0\n",
    "nodes_without_ner = 0\n",
    "avg_node_ner = []\n",
    "\n",
    "for node in tqdm(human_data_train.nodes_by_type[NodeType.INFO]):\n",
    "    context = nlp(node.text)\n",
    "    if len(context.ents) > 0:\n",
    "        nodes_with_ner += 1\n",
    "        avg_node_ner.append(len(context.ents))\n",
    "    else:\n",
    "        nodes_without_ner += 1\n",
    "\n",
    "print(\"TOTAL INFO NODES\", len(human_data_train.nodes_by_type[NodeType.INFO]))\n",
    "print(\"NODES WITH NER\", nodes_with_ner)\n",
    "print(\"NODES WITHOUT NER\", nodes_without_ner)\n",
    "print(\"AVG NER PER NODE WITH NER\", mean(avg_node_ner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX #SENTENCES PER NODE 6\n",
      "AVG #SENTENCES PER NODE 1.8\n"
     ]
    }
   ],
   "source": [
    "avg_node_sentence_length = []\n",
    "\n",
    "for node in human_data_train.nodes_by_type[NodeType.INFO]:\n",
    "    avg_node_sentence_length.append(node.text.count(\".\"))\n",
    "\n",
    "print(\"MAX #SENTENCES PER NODE\", max(avg_node_sentence_length))\n",
    "print(\"AVG #SENTENCES PER NODE\", mean(avg_node_sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ner_sentences(node: DialogNode) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Extract all sentences from node text that mention NER's.\n",
    "    Returns them as a list of tuples, where each tuple contains\n",
    "        1. the name of the entity\n",
    "        2. the sentence containing that entity\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    context = nlp(node.text)\n",
    "    entities = context.ents\n",
    "    for entity in entities:\n",
    "        start_idx = entity.start_char\n",
    "        end_idx = entity.end_char\n",
    "        # expand start index to beginning of sentence\n",
    "        while start_idx > 0 and node.text[start_idx-1] != \".\":\n",
    "            start_idx -= 1\n",
    "        # expand end index to end of sentence\n",
    "        while end_idx < len(node.text) and node.text[end_idx-1] != \".\":\n",
    "            end_idx += 1\n",
    "        results.append((entity.text, node.text[start_idx:end_idx]))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('COVID-19', 'Please check the current COVID-19 travel warnings travel restrictions from the foreign ministry and the RKI.'), ('RKI', 'Please check the current COVID-19 travel warnings travel restrictions from the foreign ministry and the RKI.'), ('Department 4 (Administrative Department', ' In In extreme cases, authorization can be given by the leadership of Department 4 (Administrative Department).')]\n"
     ]
    }
   ],
   "source": [
    "# find a testing candidate\n",
    "for node in human_data_train.nodes_by_type[NodeType.INFO]:\n",
    "    results = extract_ner_sentences(node)\n",
    "    if len(results) > 1:\n",
    "        print(results)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please check the current COVID-19 travel warnings travel restrictions from the foreign ministry and the RKI. Business trips to high risk areas or virus variation areas are not generally not allowed. In In extreme cases, authorization can be given by the leadership of Department 4 (Administrative Department).\n"
     ]
    }
   ],
   "source": [
    "print(node.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [03:52<00:00, 77.40s/it]\n"
     ]
    }
   ],
   "source": [
    "from data.dataset import NodeType, Question\n",
    "import time\n",
    "\n",
    "system = \"\"\"You are a helpful assistant creating a list of diverse FAQ-style questions from given facts.\n",
    "Only generate questions that can be answered by the given facts, without any external knowledge.\n",
    "Use casual language.\n",
    "Prefer short questions.\n",
    "Order the generated paraphrases in a numbered list.\"\"\"\n",
    "\n",
    "def user(answer_text: str, num_paraphrases: int) -> str:\n",
    "    return f'Generate {num_paraphrases} short and diverse FAQ-style questions from the fact: \"{answer_text}\"'\n",
    "\n",
    "\n",
    "NUM_QUESTIONS = 3\n",
    "TEMPERATURE = 0.7\n",
    "MAX_NEW_TOKENS = 1024\n",
    "generated_data = {}\n",
    "\n",
    "\n",
    "for entity, sentence in tqdm(extract_ner_sentences(node)):\n",
    "    prompt = generate_prompt(system=system, user=user(sentence, NUM_QUESTIONS))\n",
    "    gen = generate_output(prompt=prompt, temperature=TEMPERATURE, max_new_tokens=MAX_NEW_TOKENS)\n",
    "    candidates = parse_output(original_question=node.text, prompt=prompt, output=gen, num_paraphrases=NUM_QUESTIONS)\n",
    "    generated_data[entity] = candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'COVID-19': ['1. What should I do to stay updated on COVID-19 travel warnings '\n",
      "              'and restrictions?',\n",
      "              '2. Where can I find the latest information on travel advisories '\n",
      "              'related to COVID-19?',\n",
      "              '3. Which organizations should I refer to for understanding the '\n",
      "              'current travel restrictions due to COVID-19?'],\n",
      " 'Department 4 (Administrative Department': ['1. Who can grant authorization '\n",
      "                                             'in extreme cases?',\n",
      "                                             \"2. What department's leadership \"\n",
      "                                             'can authorize in extreme '\n",
      "                                             'situations?',\n",
      "                                             \"3. Which department's leadership \"\n",
      "                                             'has the power to authorize in '\n",
      "                                             'extreme cases?'],\n",
      " 'RKI': ['1. What should I do to stay updated on COVID-19 travel warnings and '\n",
      "         'restrictions?',\n",
      "         '2. Where can I find the latest information on travel advisories '\n",
      "         'related to COVID-19?',\n",
      "         '3. Which organizations should I refer to for understanding the '\n",
      "         'current travel restrictions due to COVID-19?']}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(generated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m MAX_NEW_TOKENS \u001b[39m=\u001b[39m \u001b[39m1024\u001b[39m\n\u001b[1;32m     18\u001b[0m prompt \u001b[39m=\u001b[39m generate_prompt(system\u001b[39m=\u001b[39msystem, user\u001b[39m=\u001b[39muser(node\u001b[39m.\u001b[39mtext, NUM_QUESTIONS))\n\u001b[0;32m---> 19\u001b[0m gen \u001b[39m=\u001b[39m generate_output(prompt\u001b[39m=\u001b[39;49mprompt, temperature\u001b[39m=\u001b[39;49mTEMPERATURE, max_new_tokens\u001b[39m=\u001b[39;49mMAX_NEW_TOKENS)\n\u001b[1;32m     20\u001b[0m candidates \u001b[39m=\u001b[39m parse_output(original_question\u001b[39m=\u001b[39mnode\u001b[39m.\u001b[39mtext, prompt\u001b[39m=\u001b[39mprompt, output\u001b[39m=\u001b[39mgen, num_paraphrases\u001b[39m=\u001b[39mNUM_QUESTIONS)\n\u001b[1;32m     21\u001b[0m pprint\u001b[39m.\u001b[39mpprint(candidates)\n",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m, in \u001b[0;36mgenerate_output\u001b[0;34m(prompt, temperature, max_new_tokens)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_output\u001b[39m(prompt: \u001b[39mstr\u001b[39m, temperature: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0.7\u001b[39m, max_new_tokens: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m512\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor:\n\u001b[1;32m     12\u001b[0m     input_ids \u001b[39m=\u001b[39m tokenizer(prompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39minput_ids\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m---> 13\u001b[0m     output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(inputs\u001b[39m=\u001b[39;49minput_ids, temperature\u001b[39m=\u001b[39;49mtemperature, max_new_tokens\u001b[39m=\u001b[39;49mmax_new_tokens)\n\u001b[1;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/auto_gptq/modeling/_base.py:438\u001b[0m, in \u001b[0;36mBaseGPTQForCausalLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"shortcut for model.generate\"\"\"\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode(), torch\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast(device_type\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype):\n\u001b[0;32m--> 438\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/transformers/generation/utils.py:1538\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1532\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1533\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnum_return_sequences has to be 1 when doing greedy search, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1534\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut is \u001b[39m\u001b[39m{\u001b[39;00mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1535\u001b[0m         )\n\u001b[1;32m   1537\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1538\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgreedy_search(\n\u001b[1;32m   1539\u001b[0m         input_ids,\n\u001b[1;32m   1540\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1541\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1542\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1543\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1544\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1545\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1546\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1547\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1548\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1549\u001b[0m     )\n\u001b[1;32m   1551\u001b[0m \u001b[39melif\u001b[39;00m is_contrastive_search_gen_mode:\n\u001b[1;32m   1552\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/transformers/generation/utils.py:2419\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2414\u001b[0m     unfinished_sequences \u001b[39m=\u001b[39m unfinished_sequences\u001b[39m.\u001b[39mmul(\n\u001b[1;32m   2415\u001b[0m         next_tokens\u001b[39m.\u001b[39mtile(eos_token_id_tensor\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mne(eos_token_id_tensor\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mprod(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m   2416\u001b[0m     )\n\u001b[1;32m   2418\u001b[0m     \u001b[39m# stop when each sentence is finished\u001b[39;00m\n\u001b[0;32m-> 2419\u001b[0m     \u001b[39mif\u001b[39;00m unfinished_sequences\u001b[39m.\u001b[39mmax() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   2420\u001b[0m         this_peer_finished \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   2422\u001b[0m \u001b[39m# stop if we exceed the maximum length\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from data.dataset import NodeType, Question\n",
    "import time\n",
    "\n",
    "system = \"\"\"You are a helpful assistant creating a list of diverse FAQ-style questions from given facts.\n",
    "Only generate questions that can be answered by the given facts, without any external knowledge.\n",
    "Use casual language.\n",
    "Prefer short questions.\n",
    "Order the generated paraphrases in a numbered list.\"\"\"\n",
    "\n",
    "def user(answer_text: str, num_paraphrases: int) -> str:\n",
    "    return f'Generate {num_paraphrases} short and diverse FAQ-style questions from the fact: \"{answer_text}\"'\n",
    "\n",
    "\n",
    "NUM_QUESTIONS = 3\n",
    "TEMPERATURE = 0.7\n",
    "MAX_NEW_TOKENS = 1024\n",
    "\n",
    "prompt = generate_prompt(system=system, user=user(node.text, NUM_QUESTIONS))\n",
    "gen = generate_output(prompt=prompt, temperature=TEMPERATURE, max_new_tokens=MAX_NEW_TOKENS)\n",
    "candidates = parse_output(original_question=node.text, prompt=prompt, output=gen, num_paraphrases=NUM_QUESTIONS)\n",
    "pprint.pprint(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [12:37:21<00:00, 568.01s/it]  \n"
     ]
    }
   ],
   "source": [
    "system = \"\"\"You are a helpful assistant creating a list of diverse FAQ-style questions from given facts.\n",
    "Only generate questions that can be answered by the given facts, without any external knowledge.\n",
    "Use casual language.\n",
    "Prefer short questions.\n",
    "Order the generated paraphrases in a numbered list.\"\"\"\n",
    "\n",
    "def user(answer_text: str, num_paraphrases: int) -> str:\n",
    "    return f'Generate {num_paraphrases} short and diverse FAQ-style questions from the fact: \"{answer_text}\"'\n",
    "\n",
    "def user_ner(answer_text: str, ner: str, num_paraphrases: int) -> str:\n",
    "    return f'Generate {num_paraphrases} short and diverse FAQ-style questions about the entity \"{ner}\" from the fact: \"{answer_text}\"'\n",
    "\n",
    "\n",
    "NUM_QUESTIONS = 10\n",
    "NUM_QUESTIONS_PER_SENTENCE = 3\n",
    "TEMPERATURE = 0.7\n",
    "MAX_NEW_TOKENS = 1024\n",
    "generated_data = {}\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "for node in tqdm(human_data_train.nodes_by_type[NodeType.INFO]):\n",
    "    # use dict indexed by generated text to filter out duplicates\n",
    "    all_generations = {}\n",
    "    \n",
    "    # extract NERs\n",
    "    named_entities = extract_ner_sentences(node)\n",
    "\n",
    "    # Generate questions with NER sentences only, make asking about NER a requirement\n",
    "    for entity, sentence in named_entities:\n",
    "        prompt = generate_prompt(system=system, user=user_ner(node.text, entity, NUM_QUESTIONS_PER_SENTENCE))\n",
    "        gen = generate_output(prompt=prompt, temperature=TEMPERATURE, max_new_tokens=MAX_NEW_TOKENS)\n",
    "        candidates = parse_output(original_question=node.text, prompt=prompt, output=gen, num_paraphrases=NUM_QUESTIONS_PER_SENTENCE)\n",
    "        for candidate_idx, candidate in enumerate(candidates):\n",
    "            key = str(time.time()).replace(\".\", \"\")\n",
    "            cleaned_candidate = candidate.replace(f\"{candidate_idx+1}.\", \"\").strip()\n",
    "            all_generations[cleaned_candidate] = {\n",
    "                \"context\": \"ner\",\n",
    "                \"entity\": entity,\n",
    "                \"dialog_node_key\": node.key,\n",
    "                \"key\": key,\n",
    "                \"text\": cleaned_candidate\n",
    "            }\n",
    "\n",
    "    # Generate questions with whole context\n",
    "    num_node_level_questions = max(NUM_QUESTIONS_PER_SENTENCE, NUM_QUESTIONS - len(named_entities) * NUM_QUESTIONS_PER_SENTENCE)\n",
    "    prompt = generate_prompt(system=system, user=user(node.text, num_node_level_questions))\n",
    "    gen = generate_output(prompt=prompt, temperature=TEMPERATURE, max_new_tokens=MAX_NEW_TOKENS)\n",
    "    candidates = parse_output(original_question=node.text, prompt=prompt, output=gen, num_paraphrases=NUM_QUESTIONS_PER_SENTENCE)\n",
    "    for candidate_idx, candidate in enumerate(candidates):\n",
    "        key = str(time.time()).replace(\".\", \"\")\n",
    "        cleaned_candidate = candidate.replace(f\"{candidate_idx+1}.\", \"\").strip()\n",
    "        all_generations[cleaned_candidate] = {\n",
    "            \"context\": \"node\",\n",
    "            \"dialog_node_key\": node.key,\n",
    "            \"key\": key,\n",
    "            \"text\": cleaned_candidate\n",
    "        }\n",
    "    \n",
    "    # add filtered questions to generated dataset\n",
    "    for text in all_generations:\n",
    "        entry = all_generations[text]\n",
    "        generated_data[entry[\"key\"]] = entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "cleaned_data = {}\n",
    "for key in generated_data:\n",
    "    node = human_data_train.nodes_by_key[generated_data[key]['dialog_node_key']]\n",
    "    cleaned_data[key] = generated_data[key]\n",
    "    for i in range (1, NUM_QUESTIONS+1):\n",
    "        cleaned_data[key]['text'] = cleaned_data[key]['text'].replace(f\"{i}.\", \"\").strip()\n",
    "    cleaned_data[key][\"node_text\"] = node.text\n",
    "    cleaned_data[key][\"node_type\"] = node.node_type.value\n",
    "\n",
    "with open(\"resources/en/onboarding/generated/train_questions_v3.json\", \"w\") as f:\n",
    "    json.dump(cleaned_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
