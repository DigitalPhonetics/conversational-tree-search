{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"8\"\n",
    "DEVICE = 'cuda:0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mount/arbeitsdaten/asr-2/vaethdk/virtualenvs/cts_en/lib64/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from typing import List\n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mount/arbeitsdaten41/projekte/asr-2/vaethdk/cts_newcodebase_rollback/conversational-tree-search/generation/reimburse/llama\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../..')\n",
    "print(os.path.realpath(\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension not installed.\n",
      "The safetensors archive passed at /mount/arbeitsdaten/asr-2/vaethdk/resources/weights/TheBloke--upstage-llama-30b-instruct-2048-GPTQ/gptq_model-4bit-32g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n",
      "skip module injection for FusedLlamaMLPForQuantizedModel not support integrate without triton yet.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "\n",
    "model_name_or_path = \"TheBloke/upstage-llama-30b-instruct-2048-GPTQ\"\n",
    "model_basename = \"gptq_model-4bit--1g\"\n",
    "\n",
    "use_triton = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\n",
    "                                          use_fast=True,\n",
    "                                          cache_dir=\"/mount/arbeitsdaten/asr-2/vaethdk/resources/weights/\",)\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(\"/mount/arbeitsdaten/asr-2/vaethdk/resources/weights/TheBloke--upstage-llama-30b-instruct-2048-GPTQ\",\n",
    "        # model_basename=model_basename,\n",
    "        # revision=\"gptq-4bit-32g-actorder_True\",\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=False,\n",
    "        device=\"cuda:0\",\n",
    "        use_triton=use_triton,\n",
    "        quantize_config=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(system: str, user: str) -> str:\n",
    "    return f\"\"\"\n",
    "    ### System:\n",
    "    {system}\n",
    "\n",
    "    ### User:\n",
    "    {user}\n",
    "\n",
    "    ### Assistant:\"\"\"\n",
    "\n",
    "def generate_output(prompt: str, temperature: float = 0.7, max_new_tokens: int = 512) -> torch.FloatTensor:\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\n",
    "    output = model.generate(inputs=input_ids, temperature=temperature, max_new_tokens=max_new_tokens)\n",
    "    return tokenizer.decode(output[0])\n",
    "\n",
    "def parse_output(prompt: str, output: str) -> List[str]:\n",
    "    # remove prompt from output first (ends at ### ASSISTANT: )\n",
    "    return output.replace(\"<s>\", \"\").strip()[len(prompt):].replace(\"</s>\", \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loading questions from  ../../../resources/en/reimburse/generated/train_questions_v1.json\n",
      "- only artificial answers\n",
      "Loading augmentation answers from ../../../resources/en/reimburse/generated/train_answers.json\n",
      "===== Dataset Statistics =====\n",
      "- files:  en/reimburse/train_graph.json en/reimburse/train_answers.json\n",
      "- synonyms: True\n",
      "- depth: 20  - degree: 13\n",
      "- answers: 803\n",
      "- questions: 800\n",
      "- loaded original data: False\n",
      "- loaded generated data: True\n",
      "- question limit: 0  - maximum loaded:  10\n",
      "- answer limit: 0  - maximum loaded:  11\n",
      "- Loading questions from  ../../../resources/en/reimburse/generated/train_questions_v1_ling.json\n",
      "- only artificial answers\n",
      "Loading augmentation answers from ../../../resources/en/reimburse/generated/train_answers.json\n",
      "===== Dataset Statistics =====\n",
      "- files:  en/reimburse/train_graph.json en/reimburse/train_answers.json\n",
      "- synonyms: True\n",
      "- depth: 20  - degree: 13\n",
      "- answers: 803\n",
      "- questions: 3681\n",
      "- loaded original data: False\n",
      "- loaded generated data: True\n",
      "- question limit: 0  - maximum loaded:  98\n",
      "- answer limit: 0  - maximum loaded:  11\n",
      "- Loading questions from  ../../../resources/en/reimburse/generated/train_questions_v2.json\n",
      "- only artificial answers\n",
      "Loading augmentation answers from ../../../resources/en/reimburse/generated/train_answers.json\n",
      "===== Dataset Statistics =====\n",
      "- files:  en/reimburse/train_graph.json en/reimburse/train_answers.json\n",
      "- synonyms: True\n",
      "- depth: 20  - degree: 13\n",
      "- answers: 803\n",
      "- questions: 800\n",
      "- loaded original data: False\n",
      "- loaded generated data: True\n",
      "- question limit: 0  - maximum loaded:  10\n",
      "- answer limit: 0  - maximum loaded:  11\n",
      "- Loading questions from  ../../../resources/en/reimburse/generated/train_questions_v2_ling.json\n",
      "- only artificial answers\n",
      "Loading augmentation answers from ../../../resources/en/reimburse/generated/train_answers.json\n",
      "===== Dataset Statistics =====\n",
      "- files:  en/reimburse/train_graph.json en/reimburse/train_answers.json\n",
      "- synonyms: True\n",
      "- depth: 20  - degree: 13\n",
      "- answers: 803\n",
      "- questions: 3062\n",
      "- loaded original data: False\n",
      "- loaded generated data: True\n",
      "- question limit: 0  - maximum loaded:  77\n",
      "- answer limit: 0  - maximum loaded:  11\n",
      "- Loading questions from  ../../../resources/en/reimburse/generated/train_questions_v3.json\n",
      "- only artificial answers\n",
      "Loading augmentation answers from ../../../resources/en/reimburse/generated/train_answers.json\n",
      "===== Dataset Statistics =====\n",
      "- files:  en/reimburse/train_graph.json en/reimburse/train_answers.json\n",
      "- synonyms: True\n",
      "- depth: 20  - degree: 13\n",
      "- answers: 803\n",
      "- questions: 416\n",
      "- loaded original data: False\n",
      "- loaded generated data: True\n",
      "- question limit: 0  - maximum loaded:  20\n",
      "- answer limit: 0  - maximum loaded:  11\n",
      "- Loading questions from  ../../../resources/en/reimburse/generated/train_questions_v3_ling.json\n",
      "- only artificial answers\n",
      "Loading augmentation answers from ../../../resources/en/reimburse/generated/train_answers.json\n",
      "===== Dataset Statistics =====\n",
      "- files:  en/reimburse/train_graph.json en/reimburse/train_answers.json\n",
      "- synonyms: True\n",
      "- depth: 20  - degree: 13\n",
      "- answers: 803\n",
      "- questions: 2134\n",
      "- loaded original data: False\n",
      "- loaded generated data: True\n",
      "- question limit: 0  - maximum loaded:  115\n",
      "- answer limit: 0  - maximum loaded:  11\n"
     ]
    }
   ],
   "source": [
    "from data.dataset import ReimburseGraphDataset, DataAugmentationLevel\n",
    "\n",
    "data_generated_v1 = ReimburseGraphDataset('en/reimburse/train_graph.json', 'en/reimburse/train_answers.json', True, augmentation=DataAugmentationLevel.ARTIFICIAL_ONLY, augmentation_path=\"en/reimburse/generated/train_questions_v1.json\", resource_dir=\"../../../resources\")\n",
    "data_generated_v1_ling = ReimburseGraphDataset('en/reimburse/train_graph.json', 'en/reimburse/train_answers.json', True, augmentation=DataAugmentationLevel.ARTIFICIAL_ONLY, augmentation_path=\"en/reimburse/generated/train_questions_v1_ling.json\", resource_dir=\"../../../resources\")\n",
    "\n",
    "data_generated_v2 = ReimburseGraphDataset('en/reimburse/train_graph.json', 'en/reimburse/train_answers.json', True, augmentation=DataAugmentationLevel.ARTIFICIAL_ONLY, augmentation_path=\"en/reimburse/generated/train_questions_v2.json\", resource_dir=\"../../../resources\")\n",
    "data_generated_v2_ling = ReimburseGraphDataset('en/reimburse/train_graph.json', 'en/reimburse/train_answers.json', True, augmentation=DataAugmentationLevel.ARTIFICIAL_ONLY, augmentation_path=\"en/reimburse/generated/train_questions_v2_ling.json\", resource_dir=\"../../../resources\")\n",
    "\n",
    "data_generated_v3 = ReimburseGraphDataset('en/reimburse/train_graph.json', 'en/reimburse/train_answers.json', True, augmentation=DataAugmentationLevel.ARTIFICIAL_ONLY, augmentation_path=\"en/reimburse/generated/train_questions_v3.json\", resource_dir=\"../../../resources\")\n",
    "data_generated_v3_ling = ReimburseGraphDataset('en/reimburse/train_graph.json', 'en/reimburse/train_answers.json', True, augmentation=DataAugmentationLevel.ARTIFICIAL_ONLY, augmentation_path=\"en/reimburse/generated/train_questions_v3_ling.json\", resource_dir=\"../../../resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes\n"
     ]
    }
   ],
   "source": [
    "from data.dataset import NodeType, Question\n",
    "import time\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "system = \"\"\"You are a truthful assistant deciding if a given question can be answered only using the presented fact, without any additional external knowledge.\n",
    "Only reply with \"yes\" or \"no\".\"\"\"\n",
    "\n",
    "def user(question_text: str, node_text: str) -> str:\n",
    "    return f'Can the question \"{question_text}\" be answered without any additional external knowledge and only using the fact: \"{node_text}\"'\n",
    "\n",
    "TEMPERATURE = 0.7\n",
    "MAX_NEW_TOKENS = 1024\n",
    "generated_data = {}\n",
    "\n",
    "for node in data_generated_v1.nodes_by_type[NodeType.INFO]:\n",
    "    for question in node.questions:\n",
    "        prompt = generate_prompt(system=system, user=user(question.text, node.text)).strip()\n",
    "        gen = generate_output(prompt=prompt, temperature=TEMPERATURE, max_new_tokens=MAX_NEW_TOKENS)\n",
    "        cleaned = parse_output(prompt, gen)\n",
    "        print(cleaned)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 3/80 [10:35<4:31:36, 211.65s/it]"
     ]
    }
   ],
   "source": [
    "from data.dataset import NodeType, Question\n",
    "import time\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "system = \"\"\"You are a truthful assistant deciding if a given question can be answered only using the presented fact, without any additional external knowledge.\n",
    "Only reply with \"yes\" or \"no\".\"\"\"\n",
    "\n",
    "def user(question_text: str, node_text: str) -> str:\n",
    "    return f'Can the question \"{question_text}\" be answered without any additional external knowledge and only using the fact: \"{node_text}\"'\n",
    "\n",
    "TEMPERATURE = 0.7\n",
    "MAX_NEW_TOKENS = 1024\n",
    "generated_data = {}\n",
    "responses = set()\n",
    "\n",
    "for node in tqdm(data_generated_v1.nodes_by_type[NodeType.INFO]):\n",
    "    for question in node.questions:\n",
    "        prompt = generate_prompt(system=system, user=user(question.text, node.text)).strip()\n",
    "        gen = generate_output(prompt=prompt, temperature=TEMPERATURE, max_new_tokens=MAX_NEW_TOKENS)\n",
    "        cleaned = parse_output(prompt, gen)\n",
    "        generated_data[question.key] = cleaned\n",
    "        responses.add(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO save data!\n",
    "\n",
    "# import json\n",
    "\n",
    "# cleaned_data = {}\n",
    "# for key in generated_data:\n",
    "#     node = human_data_train.nodes_by_key[generated_data[key]['dialog_node_key']]\n",
    "#     cleaned_data[key] = generated_data[key]\n",
    "#     for i in range (1, NUM_QUESTIONS+1):\n",
    "#         cleaned_data[key]['text'] = cleaned_data[key]['text'].replace(f\"{i}.\", \"\").strip()\n",
    "#     cleaned_data[key][\"node_text\"] = node.text\n",
    "#     cleaned_data[key][\"node_type\"] = node.node_type.value\n",
    "\n",
    "# with open(\"TODO/train_questions_v1.json\", \"w\") as f:\n",
    "    # json.dump(cleaned_data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cts_en",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
