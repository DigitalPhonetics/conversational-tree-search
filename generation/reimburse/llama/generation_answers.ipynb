{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs/scratch/users/vaethdk/adviser_reisekosten/.env/lib64/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from typing import List\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:0'\n",
    "import torch\n",
    "a= torch.zeros(1,1,device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !GITHUB_ACTIONS=true pip install auto-gptq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"TheBloke/upstage-llama-30b-instruct-2048-GPTQ\"\n",
    "model_basename = \"gptq_model-4bit--1g\"\n",
    "\n",
    "use_triton = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\n",
    "                                          use_fast=True,\n",
    "                                          cache_dir=\"/mount/arbeitsdaten/asr-2/vaethdk/resources/weights/\",)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension not installed.\n",
      "The safetensors archive passed at /mount/arbeitsdaten/asr-2/vaethdk/resources/weights/TheBloke--upstage-llama-30b-instruct-2048-GPTQ/gptq_model-4bit-32g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n",
      "skip module injection for FusedLlamaMLPForQuantizedModel not support integrate without triton yet.\n"
     ]
    }
   ],
   "source": [
    "model = AutoGPTQForCausalLM.from_quantized(\"/mount/arbeitsdaten/asr-2/vaethdk/resources/weights/TheBloke--upstage-llama-30b-instruct-2048-GPTQ\",\n",
    "        # model_basename=model_basename,\n",
    "        # revision=\"gptq-4bit-32g-actorder_True\",\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=False,\n",
    "        device=\"cuda:0\",\n",
    "        use_triton=use_triton,\n",
    "        quantize_config=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System:\n",
    "{System}\n",
    "\n",
    "### User:\n",
    "{User}\n",
    "\n",
    "### Assistant:\n",
    "{Assistant}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Answer Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset import ReimburseGraphDataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(system: str, user: str) -> str:\n",
    "    return f\"\"\"\n",
    "    ### System:\n",
    "    {system}\n",
    "\n",
    "    ### User:\n",
    "    {user}\n",
    "\n",
    "    ### Assistant:\"\"\"\n",
    "\n",
    "def generate_output(prompt: str, temperature: float = 0.7, max_new_tokens: int = 512) -> torch.FloatTensor:\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\n",
    "    output = model.generate(inputs=input_ids, temperature=temperature, max_new_tokens=max_new_tokens)\n",
    "    return tokenizer.decode(output[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- not using synonyms\n",
      "===== Dataset Statistics =====\n",
      "- files:  resources/en/train_graph.json resources/en/train_answers.json\n",
      "- synonyms: False\n",
      "- depth: 20  - degree: 13\n",
      "- answers: 73\n",
      "- questions: 279\n"
     ]
    }
   ],
   "source": [
    "train = ReimburseGraphDataset('en/reimburse/train_graph.json', 'en/reimburse/train_answers.json', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that we don't have any answer synonyms\n",
    "for answer_candidate in train.answer_synonyms:\n",
    "    assert len(train.answer_synonyms[answer_candidate]) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_output(original_answer: str, prompt: str, output: str, num_paraphrases: int) -> List[str]:\n",
    "    # remove prompt from output first (ends at ### ASSISTANT: )\n",
    "    answers = []\n",
    "    cleaned = output[len(prompt):]\n",
    "    \n",
    "    if not \"1.\" in cleaned: \n",
    "        print(\"NO LIST FOR ANSWER\", original_answer)\n",
    "        return answers\n",
    "    \n",
    "    for i in range(1, num_paraphrases+1):\n",
    "        if not f\"{i}.\" in cleaned: \n",
    "            print(f\" - NO {i}. CANDIDATE FOR ANSWER\", original_answer)\n",
    "            continue\n",
    "\n",
    "        start_idx = cleaned.find(f\"{i}.\") # find i. line\n",
    "        end_idx = cleaned.find(\"\\n\", start_idx) # read until line end \n",
    "        if i == num_paraphrases and end_idx == -1:\n",
    "            # last line might not have line break\n",
    "            end_idx = len(cleaned)\n",
    "        if start_idx == -1 or end_idx == -1:\n",
    "            print(f\" - INDEX PROBLEM FOR {i}. CANDIDATE: ({start_idx}, {end_idx})\")\n",
    "            continue\n",
    "        # parse answer\n",
    "        answers.append(cleaned[start_idx:end_idx].replace(f\"{i}.\", \"\").replace(\"</s>\", \"\").strip())\n",
    "\n",
    "        cleaned = cleaned[end_idx:] # remove i. line\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [3:32:00<00:00, 174.26s/it]  \n"
     ]
    }
   ],
   "source": [
    "system = \"\"\"You are a helpful assistant creating a list of paraphrases for a given answer to a question.\n",
    "Order the generated paraphrases in a numbered list.\"\"\"\n",
    "def user(answer_text: str, num_paraphrases: int) -> str:\n",
    "    return f'Generate {num_paraphrases} paraphrases for the answer \"{answer_text}\"'\n",
    "\n",
    "NUM_PARAPHRASES = 10\n",
    "TEMPERATURE = 0.7\n",
    "MAX_NEW_TOKENS = 512\n",
    "generated_data = {}\n",
    "for answer_key in tqdm(train.answer_synonyms):\n",
    "    prompt = generate_prompt(system=system, user=user(answer_key, NUM_PARAPHRASES))\n",
    "    gen = generate_output(prompt=prompt, temperature=TEMPERATURE, max_new_tokens=MAX_NEW_TOKENS)\n",
    "    candidates = parse_output(original_answer=answer_key, prompt=prompt, output=gen, num_paraphrases=NUM_PARAPHRASES)\n",
    "    generated_data[answer_key] = candidates\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"en/reimburse/generated/train_answers.json\", \"w\") as f:\n",
    "    json.dump(generated_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
