{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6,8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs/scratch/users/vaethdk/adviser_reisekosten/.env/lib64/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from typing import List, Tuple\n",
    "import re\n",
    "DEVICE = 'cuda:0'\n",
    "import torch\n",
    "a= torch.zeros(1,1,device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !GITHUB_ACTIONS=true pip install auto-gptq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from data.dataset import GraphDataset, DataAugmentationLevel, NodeType, DialogNode, Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"TheBloke/upstage-llama-30b-instruct-2048-GPTQ\"\n",
    "model_basename = \"gptq_model-4bit--1g\"\n",
    "\n",
    "use_triton = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\n",
    "                                          use_fast=True,\n",
    "                                          cache_dir=\"/mount/arbeitsdaten/asr-2/vaethdk/resources/weights/\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension not installed.\n",
      "The safetensors archive passed at /mount/arbeitsdaten/asr-2/vaethdk/resources/weights/TheBloke--upstage-llama-30b-instruct-2048-GPTQ/gptq_model-4bit-32g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n",
      "skip module injection for FusedLlamaMLPForQuantizedModel not support integrate without triton yet.\n"
     ]
    }
   ],
   "source": [
    "model = AutoGPTQForCausalLM.from_quantized(\"/mount/arbeitsdaten/asr-2/vaethdk/resources/weights/TheBloke--upstage-llama-30b-instruct-2048-GPTQ\",\n",
    "        # model_basename=model_basename,\n",
    "        # revision=\"gptq-4bit-32g-actorder_True\",\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=False,\n",
    "        device=\"cuda:0\",\n",
    "        use_triton=use_triton,\n",
    "        quantize_config=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System:\n",
    "{System}\n",
    "\n",
    "### User:\n",
    "{User}\n",
    "\n",
    "### Assistant:\n",
    "{Assistant}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m set_seed(\u001b[39m42\u001b[39m)\n\u001b[1;32m     19\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer(prompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39minput_ids\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m---> 20\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(inputs\u001b[39m=\u001b[39;49minput_ids, temperature\u001b[39m=\u001b[39;49m\u001b[39m0.7\u001b[39;49m, max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m)\n\u001b[1;32m     21\u001b[0m \u001b[39mprint\u001b[39m(tokenizer\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m]))\n",
      "File \u001b[0;32m/fs/scratch/users/vaethdk/adviser_reisekosten/.env/lib64/python3.10/site-packages/auto_gptq/modeling/_base.py:438\u001b[0m, in \u001b[0;36mBaseGPTQForCausalLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"shortcut for model.generate\"\"\"\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode(), torch\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast(device_type\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype):\n\u001b[0;32m--> 438\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/fs/scratch/users/vaethdk/adviser_reisekosten/.env/lib64/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/fs/scratch/users/vaethdk/adviser_reisekosten/.env/lib64/python3.10/site-packages/transformers/generation/utils.py:1538\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1532\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1533\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnum_return_sequences has to be 1 when doing greedy search, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1534\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut is \u001b[39m\u001b[39m{\u001b[39;00mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1535\u001b[0m         )\n\u001b[1;32m   1537\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1538\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgreedy_search(\n\u001b[1;32m   1539\u001b[0m         input_ids,\n\u001b[1;32m   1540\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1541\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1542\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1543\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1544\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1545\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1546\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1547\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1548\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1549\u001b[0m     )\n\u001b[1;32m   1551\u001b[0m \u001b[39melif\u001b[39;00m is_contrastive_search_gen_mode:\n\u001b[1;32m   1552\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/fs/scratch/users/vaethdk/adviser_reisekosten/.env/lib64/python3.10/site-packages/transformers/generation/utils.py:2362\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2359\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2361\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2362\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2363\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2364\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2365\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2366\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2367\u001b[0m )\n\u001b[1;32m   2369\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2370\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/fs/scratch/users/vaethdk/adviser_reisekosten/.env/lib64/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/fs/scratch/users/vaethdk/adviser_reisekosten/.env/lib64/python3.10/site-packages/transformers/models/llama/modeling_llama.py:806\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    803\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m    805\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 806\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m    807\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    808\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    809\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    810\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    811\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    812\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    813\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    814\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    815\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    816\u001b[0m )\n\u001b[1;32m    818\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    819\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpretraining_tp \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/fs/scratch/users/vaethdk/adviser_reisekosten/.env/lib64/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/fs/scratch/users/vaethdk/adviser_reisekosten/.env/lib64/python3.10/site-packages/transformers/models/llama/modeling_llama.py:693\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    685\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    686\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    687\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    691\u001b[0m     )\n\u001b[1;32m    692\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 693\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    694\u001b[0m         hidden_states,\n\u001b[1;32m    695\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    696\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    697\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    698\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    699\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    700\u001b[0m     )\n\u001b[1;32m    702\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    704\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/fs/scratch/users/vaethdk/adviser_reisekosten/.env/lib64/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/fs/scratch/users/vaethdk/adviser_reisekosten/.env/lib64/python3.10/site-packages/transformers/models/llama/modeling_llama.py:408\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    405\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    407\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    409\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    410\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    411\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    412\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    413\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    414\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    415\u001b[0m )\n\u001b[1;32m    416\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    418\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/fs/scratch/users/vaethdk/adviser_reisekosten/.env/lib64/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/fs/scratch/users/vaethdk/adviser_reisekosten/.env/lib64/python3.10/site-packages/auto_gptq/nn_modules/fused_llama_attn.py:64\u001b[0m, in \u001b[0;36mFusedLlamaAttentionForQuantizedModel.forward\u001b[0;34m(self, hidden_states, past_key_value, attention_mask, position_ids, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     kv_seq_len \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m past_key_value[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\n\u001b[1;32m     63\u001b[0m cos, sin \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrotary_emb(value_states, seq_len\u001b[39m=\u001b[39mkv_seq_len)\n\u001b[0;32m---> 64\u001b[0m query_states, key_states \u001b[39m=\u001b[39m apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n\u001b[1;32m     65\u001b[0m \u001b[39m# [bsz, nh, t, hd]\u001b[39;00m\n\u001b[1;32m     67\u001b[0m is_causal \u001b[39m=\u001b[39m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/fs/scratch/users/vaethdk/adviser_reisekosten/.env/lib64/python3.10/site-packages/transformers/models/llama/modeling_llama.py:187\u001b[0m, in \u001b[0;36mapply_rotary_pos_emb\u001b[0;34m(q, k, cos, sin, position_ids)\u001b[0m\n\u001b[1;32m    185\u001b[0m sin \u001b[39m=\u001b[39m sin[position_ids]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)  \u001b[39m# [bs, 1, seq_len, dim]\u001b[39;00m\n\u001b[1;32m    186\u001b[0m q_embed \u001b[39m=\u001b[39m (q \u001b[39m*\u001b[39m cos) \u001b[39m+\u001b[39m (rotate_half(q) \u001b[39m*\u001b[39m sin)\n\u001b[0;32m--> 187\u001b[0m k_embed \u001b[39m=\u001b[39m (k \u001b[39m*\u001b[39m cos) \u001b[39m+\u001b[39m (rotate_half(k) \u001b[39m*\u001b[39m sin)\n\u001b[1;32m    188\u001b[0m \u001b[39mreturn\u001b[39;00m q_embed, k_embed\n",
      "File \u001b[0;32m/fs/scratch/users/vaethdk/adviser_reisekosten/.env/lib64/python3.10/site-packages/transformers/models/llama/modeling_llama.py:173\u001b[0m, in \u001b[0;36mrotate_half\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_buffer(\u001b[39m\"\u001b[39m\u001b[39mcos_cached\u001b[39m\u001b[39m\"\u001b[39m, emb\u001b[39m.\u001b[39mcos()[\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, :, :]\u001b[39m.\u001b[39mto(dtype), persistent\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    170\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_buffer(\u001b[39m\"\u001b[39m\u001b[39msin_cached\u001b[39m\u001b[39m\"\u001b[39m, emb\u001b[39m.\u001b[39msin()[\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, :, :]\u001b[39m.\u001b[39mto(dtype), persistent\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrotate_half\u001b[39m(x):\n\u001b[1;32m    174\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Rotates half the hidden dims of the input.\"\"\"\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     x1 \u001b[39m=\u001b[39m x[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, : x\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "system = \"\"\"You are a helpful assistant creating a list of FAQ-style questions from given facts.\n",
    "Only generate questions that can be answered by the given facts, without any external knowledge.\n",
    "Remove some information, especially nouns and named entities, between generated questions.\n",
    "Use casual language.\n",
    "Order the generated paraphrases in a numbered list.\"\"\"\n",
    "user = 'Generate 10 FAQ-style questions from the fact: \"In the US, you are entitled to 30$ per day, minus any free meals which you choose to decline.\"'\n",
    "\n",
    "\n",
    "prompt = f\"\"\"\n",
    "### System:\n",
    "{system}\n",
    "\n",
    "### User:\n",
    "{user}\n",
    "\n",
    "### Assistant:\"\"\"\n",
    "\n",
    "set_seed(42)\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Question Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(system: str, user: str) -> str:\n",
    "    return f\"\"\"\n",
    "    ### System:\n",
    "    {system}\n",
    "\n",
    "    ### User:\n",
    "    {user}\n",
    "\n",
    "    ### Assistant:\"\"\"\n",
    "\n",
    "def generate_output(prompt: str, temperature: float = 0.7, max_new_tokens: int = 512) -> torch.FloatTensor:\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\n",
    "    output = model.generate(inputs=input_ids, temperature=temperature, max_new_tokens=max_new_tokens)\n",
    "    return tokenizer.decode(output[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- not using synonyms\n",
      "===== Dataset Statistics =====\n",
      "- files:  resources/en/train_graph.json resources/en/train_answers.json\n",
      "- synonyms: False\n",
      "- depth: 20  - degree: 13\n",
      "- answers: 73\n",
      "- questions: 279\n",
      "- loaded original data: True\n",
      "- loaded generated data: False\n"
     ]
    }
   ],
   "source": [
    "human_data_train = GraphDataset('resources/en/train_graph.json', 'resources/en/train_answers.json', False, augmentation=DataAugmentationLevel.NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that we don't have any answer synonyms\n",
    "for node in human_data_train.node_list:\n",
    "    for question in node.questions:\n",
    "        human_data_train.question_list.remove(question)\n",
    "        del human_data_train.questions_by_key[question.key]\n",
    "    node.questions.clear()\n",
    "assert len(human_data_train.question_list) == 0\n",
    "assert len(human_data_train.questions_by_key) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_output(original_question: str, prompt: str, output: str, num_paraphrases: int) -> List[str]:\n",
    "    # remove prompt from output first (ends at ### ASSISTANT: )\n",
    "    questions = []\n",
    "    cleaned = output[len(prompt):]\n",
    "    \n",
    "    if not \"1.\" in cleaned: \n",
    "        print(\"NO LIST FOR QUESTION\", original_question)\n",
    "        return questions\n",
    "    \n",
    "    for i in range(1, num_paraphrases+1):\n",
    "        if not f\"{i}.\" in cleaned: \n",
    "            print(f\" - NO {i}. CANDIDATE FOR QUESTION\", original_question)\n",
    "            continue\n",
    "\n",
    "        start_idx = cleaned.find(f\"{i}.\") # find i. line\n",
    "        end_idx = cleaned.find(\"\\n\", start_idx) # read until line end \n",
    "        if i == num_paraphrases and end_idx == -1:\n",
    "            # last line might not have line break\n",
    "            end_idx = len(cleaned)\n",
    "        if start_idx == -1 or end_idx == -1:\n",
    "            print(f\" - INDEX PROBLEM FOR {i}. CANDIDATE: ({start_idx}, {end_idx})\")\n",
    "            continue\n",
    "        # parse answer\n",
    "        questions.append(cleaned[start_idx:end_idx].replace(\"</s>\", \"\").strip())\n",
    "\n",
    "        cleaned = cleaned[end_idx:] # remove i. line\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 18/123 [56:29<4:41:16, 160.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - INDEX PROBLEM FOR 3. CANDIDATE: (1, -1)\n",
      " - NO 4. CANDIDATE FOR QUESTION What would you like to know about? Information Before the start of a Business trip An Emergency during your Business trip Information about the Country risk categories\n",
      " - NO 5. CANDIDATE FOR QUESTION What would you like to know about? Information Before the start of a Business trip An Emergency during your Business trip Information about the Country risk categories\n",
      " - NO 6. CANDIDATE FOR QUESTION What would you like to know about? Information Before the start of a Business trip An Emergency during your Business trip Information about the Country risk categories\n",
      " - NO 7. CANDIDATE FOR QUESTION What would you like to know about? Information Before the start of a Business trip An Emergency during your Business trip Information about the Country risk categories\n",
      " - NO 8. CANDIDATE FOR QUESTION What would you like to know about? Information Before the start of a Business trip An Emergency during your Business trip Information about the Country risk categories\n",
      " - NO 9. CANDIDATE FOR QUESTION What would you like to know about? Information Before the start of a Business trip An Emergency during your Business trip Information about the Country risk categories\n",
      " - NO 10. CANDIDATE FOR QUESTION What would you like to know about? Information Before the start of a Business trip An Emergency during your Business trip Information about the Country risk categories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 123/123 [7:02:31<00:00, 206.11s/it]  \n"
     ]
    }
   ],
   "source": [
    "from data.dataset import NodeType, Question\n",
    "import time\n",
    "\n",
    "system = \"\"\"You are a helpful assistant creating a list of FAQ-style questions from given facts.\n",
    "Only generate questions that can be answered by the given facts, without any external knowledge.\n",
    "Remove some information, especially nouns and named entities, between generated questions.\n",
    "Use casual language.\n",
    "Order the generated paraphrases in a numbered list.\"\"\"\n",
    "\n",
    "def user(answer_text: str, num_paraphrases: int) -> str:\n",
    "    return f'Generate {num_paraphrases} FAQ-style questions from the fact: \"{answer_text}\"'\n",
    "\n",
    "NUM_QUESTIONS = 10\n",
    "TEMPERATURE = 0.7\n",
    "MAX_NEW_TOKENS = 1024\n",
    "generated_data = {}\n",
    "\n",
    "for node in tqdm(human_data_train.node_list):\n",
    "    if node.node_type in [NodeType.INFO, NodeType.QUESTION]:\n",
    "        prompt = generate_prompt(system=system, user=user(node.text, NUM_QUESTIONS))\n",
    "        gen = generate_output(prompt=prompt, temperature=TEMPERATURE, max_new_tokens=MAX_NEW_TOKENS)\n",
    "        candidates = parse_output(original_question=node.text, prompt=prompt, output=gen, num_paraphrases=NUM_QUESTIONS)\n",
    "        for candidate in candidates:\n",
    "            key = str(time.time()).replace(\".\", \"\")\n",
    "            generated_data[key] = {\n",
    "                \"dialog_node_key\": node.key,\n",
    "                \"key\": key,\n",
    "                \"text\": candidate,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "cleaned_data = {}\n",
    "for key in generated_data:\n",
    "    node = human_data_train.nodes_by_key[generated_data[key]['dialog_node_key']]\n",
    "    cleaned_data[key] = generated_data[key]\n",
    "    for i in range (1, NUM_QUESTIONS+1):\n",
    "        cleaned_data[key]['text'] = cleaned_data[key]['text'].replace(f\"{i}.\", \"\").strip()\n",
    "    cleaned_data[key][\"node_text\"] = node.text\n",
    "    cleaned_data[key][\"node_type\"] = node.node_type.value\n",
    "\n",
    "with open(\"resources/en/generated/train_questions_v1.json\", \"w\") as f:\n",
    "    json.dump(cleaned_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA GENERATION V2: SHORTER QUESTIONS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/123 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 123/123 [5:04:30<00:00, 148.54s/it] \n"
     ]
    }
   ],
   "source": [
    "from data.dataset import NodeType, Question\n",
    "import time\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "system = \"\"\"You are a helpful assistant creating a list of FAQ-style questions from given facts.\n",
    "Only generate questions that can be answered by the given facts, without any external knowledge.\n",
    "Remove some information, especially nouns and named entities, between generated questions.\n",
    "Use casual language.\n",
    "Prefer short questions.\n",
    "Order the generated questions in a numbered list.\"\"\"\n",
    "\n",
    "def user(answer_text: str, num_paraphrases: int) -> str:\n",
    "    return f'Generate {num_paraphrases} short FAQ-style questions from the fact: \"{answer_text}\"'\n",
    "\n",
    "NUM_QUESTIONS = 10\n",
    "TEMPERATURE = 0.7\n",
    "MAX_NEW_TOKENS = 1024\n",
    "generated_data = {}\n",
    "\n",
    "for node in tqdm(human_data_train.node_list):\n",
    "    if node.node_type in [NodeType.INFO]:\n",
    "        prompt = generate_prompt(system=system, user=user(node.text, NUM_QUESTIONS))\n",
    "        gen = generate_output(prompt=prompt, temperature=TEMPERATURE, max_new_tokens=MAX_NEW_TOKENS)\n",
    "        candidates = parse_output(original_question=node.text, prompt=prompt, output=gen, num_paraphrases=NUM_QUESTIONS)\n",
    "        for candidate in candidates:\n",
    "            key = str(time.time()).replace(\".\", \"\")\n",
    "            generated_data[key] = {\n",
    "                \"dialog_node_key\": node.key,\n",
    "                \"key\": key,\n",
    "                \"text\": candidate,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "cleaned_data = {}\n",
    "for key in generated_data:\n",
    "    node = human_data_train.nodes_by_key[generated_data[key]['dialog_node_key']]\n",
    "    cleaned_data[key] = generated_data[key]\n",
    "    for i in range (1, NUM_QUESTIONS+1):\n",
    "        cleaned_data[key]['text'] = cleaned_data[key]['text'].replace(f\"{i}.\", \"\").strip()\n",
    "    cleaned_data[key][\"node_text\"] = node.text\n",
    "    cleaned_data[key][\"node_type\"] = node.node_type.value\n",
    "\n",
    "with open(\"resources/en/generated/train_questions_v2.json\", \"w\") as f:\n",
    "    json.dump(cleaned_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA GENERATION V3: SHORTER QUESTIONS + SPLIT NODE CONTEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Try to generate shorter questions (-> change prompt)\n",
    "2. Try to generate more diverse questions\n",
    "    1. Detect relevant sentences in node text via NER tool (also detects time, quantities, ...)\n",
    "    2. Generate questions for whole node context, then for only relevant sentences / sub-sentences of node\n",
    "    3. Choose amount of questions to be generated depending on amount of extracted NERs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanza\n",
      "  Downloading stanza-1.5.0-py3-none-any.whl (802 kB)\n",
      "\u001b[K     |████████████████████████████████| 802 kB 14.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.3.0 in ./.env/lib/python3.10/site-packages (from stanza) (2.0.0+cu118)\n",
      "Requirement already satisfied: requests in ./.env/lib/python3.10/site-packages (from stanza) (2.28.2)\n",
      "Requirement already satisfied: numpy in ./.env/lib/python3.10/site-packages (from stanza) (1.24.2)\n",
      "Requirement already satisfied: six in ./.env/lib/python3.10/site-packages (from stanza) (1.16.0)\n",
      "Requirement already satisfied: protobuf in ./.env/lib/python3.10/site-packages (from stanza) (3.20.0)\n",
      "Requirement already satisfied: tqdm in ./.env/lib/python3.10/site-packages (from stanza) (4.65.0)\n",
      "Collecting emoji\n",
      "  Downloading emoji-2.7.0.tar.gz (361 kB)\n",
      "\u001b[K     |████████████████████████████████| 361 kB 119.2 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in ./.env/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (4.4.0)\n",
      "Requirement already satisfied: jinja2 in ./.env/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in ./.env/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (2.0.0)\n",
      "Requirement already satisfied: sympy in ./.env/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (1.11.1)\n",
      "Requirement already satisfied: filelock in ./.env/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.9.0)\n",
      "Requirement already satisfied: networkx in ./.env/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.0)\n",
      "Requirement already satisfied: cmake in ./.env/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.3.0->stanza) (3.25.0)\n",
      "Requirement already satisfied: lit in ./.env/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.3.0->stanza) (15.0.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.env/lib/python3.10/site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.env/lib/python3.10/site-packages (from requests->stanza) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.env/lib/python3.10/site-packages (from requests->stanza) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.env/lib/python3.10/site-packages (from requests->stanza) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.env/lib/python3.10/site-packages (from requests->stanza) (1.26.15)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.env/lib/python3.10/site-packages (from sympy->torch>=1.3.0->stanza) (1.2.1)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for emoji: filename=emoji-2.7.0-py2.py3-none-any.whl size=356563 sha256=d6fa52fd4eea49ae84298a10e2c77bf642c90e75abcdd4db626c8b0309a7e8f2\n",
      "  Stored in directory: /home/users2/vaethdk/.cache/pip/wheels/41/11/48/5df0b9727d5669c9174a141134f10304d1d78a3b89a4676f3d\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji, stanza\n",
      "Successfully installed emoji-2.7.0 stanza-1.5.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/fs/scratch/users/vaethdk/adviser_reisekosten/.env/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json: 216kB [00:00, 59.2MB/s]                    \n",
      "2023-08-04 16:21:45 INFO: Downloading default packages for language: en (English) ...\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.5.0/models/default.zip: 100%|██████████| 594M/594M [00:05<00:00, 115MB/s]  \n",
      "2023-08-04 16:21:56 INFO: Finished downloading models and saved to .models/.\n"
     ]
    }
   ],
   "source": [
    "# stanza.download(lang=\"en\", model_dir=\".models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 11:04:27 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json: 216kB [00:00, 34.1MB/s]                    \n",
      "2023-08-08 11:04:28 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2023-08-08 11:04:28 INFO: Using device: cuda:1\n",
      "2023-08-08 11:04:28 INFO: Loading: tokenize\n",
      "2023-08-08 11:04:28 INFO: Loading: ner\n",
      "2023-08-08 11:04:28 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline('en', processors='tokenize,ner', device=\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\n",
      "  \"text\": \"Barack Obama\",\n",
      "  \"type\": \"PERSON\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 12\n",
      "}, {\n",
      "  \"text\": \"Hawaii\",\n",
      "  \"type\": \"GPE\",\n",
      "  \"start_char\": 25,\n",
      "  \"end_char\": 31\n",
      "}]\n"
     ]
    }
   ],
   "source": [
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL INFO NODES 80\n",
      "NODES WITH NER 33\n",
      "NODES WITHOUT NER 47\n",
      "AVG NER PER NODE WITH NER 2.1515151515151514\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "nodes_with_ner = 0\n",
    "nodes_without_ner = 0\n",
    "avg_node_ner = []\n",
    "\n",
    "for node in tqdm(human_data_train.nodes_by_type[NodeType.INFO]):\n",
    "    context = nlp(node.text)\n",
    "    if len(context.ents) > 0:\n",
    "        nodes_with_ner += 1\n",
    "        avg_node_ner.append(len(context.ents))\n",
    "    else:\n",
    "        nodes_without_ner += 1\n",
    "\n",
    "print(\"TOTAL INFO NODES\", len(human_data_train.nodes_by_type[NodeType.INFO]))\n",
    "print(\"NODES WITH NER\", nodes_with_ner)\n",
    "print(\"NODES WITHOUT NER\", nodes_without_ner)\n",
    "print(\"AVG NER PER NODE WITH NER\", mean(avg_node_ner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX #SENTENCES PER NODE 6\n",
      "AVG #SENTENCES PER NODE 1.8\n"
     ]
    }
   ],
   "source": [
    "avg_node_sentence_length = []\n",
    "\n",
    "for node in human_data_train.nodes_by_type[NodeType.INFO]:\n",
    "    avg_node_sentence_length.append(node.text.count(\".\"))\n",
    "\n",
    "print(\"MAX #SENTENCES PER NODE\", max(avg_node_sentence_length))\n",
    "print(\"AVG #SENTENCES PER NODE\", mean(avg_node_sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ner_sentences(node: DialogNode) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Extract all sentences from node text that mention NER's.\n",
    "    Returns them as a list of tuples, where each tuple contains\n",
    "        1. the name of the entity\n",
    "        2. the sentence containing that entity\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    context = nlp(node.text)\n",
    "    entities = context.ents\n",
    "    for entity in entities:\n",
    "        start_idx = entity.start_char\n",
    "        end_idx = entity.end_char\n",
    "        # expand start index to beginning of sentence\n",
    "        while start_idx > 0 and node.text[start_idx-1] != \".\":\n",
    "            start_idx -= 1\n",
    "        # expand end index to end of sentence\n",
    "        while end_idx < len(node.text) and node.text[end_idx-1] != \".\":\n",
    "            end_idx += 1\n",
    "        results.append((entity.text, node.text[start_idx:end_idx]))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('COVID-19', 'Please check the current COVID-19 travel warnings travel restrictions from the foreign ministry and the RKI.'), ('RKI', 'Please check the current COVID-19 travel warnings travel restrictions from the foreign ministry and the RKI.'), ('Department 4 (Administrative Department', ' In In extreme cases, authorization can be given by the leadership of Department 4 (Administrative Department).')]\n"
     ]
    }
   ],
   "source": [
    "# find a testing candidate\n",
    "for node in human_data_train.nodes_by_type[NodeType.INFO]:\n",
    "    results = extract_ner_sentences(node)\n",
    "    if len(results) > 1:\n",
    "        print(results)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please check the current COVID-19 travel warnings travel restrictions from the foreign ministry and the RKI. Business trips to high risk areas or virus variation areas are not generally not allowed. In In extreme cases, authorization can be given by the leadership of Department 4 (Administrative Department).\n"
     ]
    }
   ],
   "source": [
    "print(node.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [03:52<00:00, 77.40s/it]\n"
     ]
    }
   ],
   "source": [
    "from data.dataset import NodeType, Question\n",
    "import time\n",
    "\n",
    "system = \"\"\"You are a helpful assistant creating a list of diverse FAQ-style questions from given facts.\n",
    "Only generate questions that can be answered by the given facts, without any external knowledge.\n",
    "Use casual language.\n",
    "Prefer short questions.\n",
    "Order the generated paraphrases in a numbered list.\"\"\"\n",
    "\n",
    "def user(answer_text: str, num_paraphrases: int) -> str:\n",
    "    return f'Generate {num_paraphrases} short and diverse FAQ-style questions from the fact: \"{answer_text}\"'\n",
    "\n",
    "\n",
    "NUM_QUESTIONS = 3\n",
    "TEMPERATURE = 0.7\n",
    "MAX_NEW_TOKENS = 1024\n",
    "generated_data = {}\n",
    "\n",
    "\n",
    "for entity, sentence in tqdm(extract_ner_sentences(node)):\n",
    "    prompt = generate_prompt(system=system, user=user(sentence, NUM_QUESTIONS))\n",
    "    gen = generate_output(prompt=prompt, temperature=TEMPERATURE, max_new_tokens=MAX_NEW_TOKENS)\n",
    "    candidates = parse_output(original_question=node.text, prompt=prompt, output=gen, num_paraphrases=NUM_QUESTIONS)\n",
    "    generated_data[entity] = candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'COVID-19': ['1. What should I do to stay updated on COVID-19 travel warnings '\n",
      "              'and restrictions?',\n",
      "              '2. Where can I find the latest information on travel advisories '\n",
      "              'related to COVID-19?',\n",
      "              '3. Which organizations should I refer to for understanding the '\n",
      "              'current travel restrictions due to COVID-19?'],\n",
      " 'Department 4 (Administrative Department': ['1. Who can grant authorization '\n",
      "                                             'in extreme cases?',\n",
      "                                             \"2. What department's leadership \"\n",
      "                                             'can authorize in extreme '\n",
      "                                             'situations?',\n",
      "                                             \"3. Which department's leadership \"\n",
      "                                             'has the power to authorize in '\n",
      "                                             'extreme cases?'],\n",
      " 'RKI': ['1. What should I do to stay updated on COVID-19 travel warnings and '\n",
      "         'restrictions?',\n",
      "         '2. Where can I find the latest information on travel advisories '\n",
      "         'related to COVID-19?',\n",
      "         '3. Which organizations should I refer to for understanding the '\n",
      "         'current travel restrictions due to COVID-19?']}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(generated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. What should I do before traveling for business purposes during the '\n",
      " 'COVID-19 pandemic?',\n",
      " '2. Are business trips to high risk or virus variation areas allowed?',\n",
      " '3. Can I get authorization for traveling to high risk areas from Department '\n",
      " '4?']\n"
     ]
    }
   ],
   "source": [
    "from data.dataset import NodeType, Question\n",
    "import time\n",
    "\n",
    "system = \"\"\"You are a helpful assistant creating a list of diverse FAQ-style questions from given facts.\n",
    "Only generate questions that can be answered by the given facts, without any external knowledge.\n",
    "Use casual language.\n",
    "Prefer short questions.\n",
    "Order the generated paraphrases in a numbered list.\"\"\"\n",
    "\n",
    "def user(answer_text: str, num_paraphrases: int) -> str:\n",
    "    return f'Generate {num_paraphrases} short and diverse FAQ-style questions from the fact: \"{answer_text}\"'\n",
    "\n",
    "\n",
    "NUM_QUESTIONS = 3\n",
    "TEMPERATURE = 0.7\n",
    "MAX_NEW_TOKENS = 1024\n",
    "\n",
    "prompt = generate_prompt(system=system, user=user(node.text, NUM_QUESTIONS))\n",
    "gen = generate_output(prompt=prompt, temperature=TEMPERATURE, max_new_tokens=MAX_NEW_TOKENS)\n",
    "candidates = parse_output(original_question=node.text, prompt=prompt, output=gen, num_paraphrases=NUM_QUESTIONS)\n",
    "pprint.pprint(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [6:14:24<00:00, 280.81s/it]  \n"
     ]
    }
   ],
   "source": [
    "system = \"\"\"You are a helpful assistant creating a list of diverse FAQ-style questions from given facts.\n",
    "Only generate questions that can be answered by the given facts, without any external knowledge.\n",
    "Use casual language.\n",
    "Prefer short questions.\n",
    "Order the generated paraphrases in a numbered list.\"\"\"\n",
    "\n",
    "def user(answer_text: str, num_paraphrases: int) -> str:\n",
    "    return f'Generate {num_paraphrases} short and diverse FAQ-style questions from the fact: \"{answer_text}\"'\n",
    "\n",
    "def user_ner(answer_text: str, ner: str, num_paraphrases: int) -> str:\n",
    "    return f'Generate {num_paraphrases} short and diverse FAQ-style questions about \"{ner}\" from the fact: \"{answer_text}\"'\n",
    "\n",
    "\n",
    "NUM_QUESTIONS = 10\n",
    "NUM_QUESTIONS_PER_SENTENCE = 3\n",
    "TEMPERATURE = 0.7\n",
    "MAX_NEW_TOKENS = 1024\n",
    "generated_data = {}\n",
    "\n",
    "for node in tqdm(human_data_train.nodes_by_type[NodeType.INFO]):\n",
    "    # use dict indexed by generated text to filter out duplicates\n",
    "    all_generations = {}\n",
    "    \n",
    "    # extract NERs\n",
    "    named_entities = extract_ner_sentences(node)\n",
    "\n",
    "    # Generate questions with NER sentences only, make asking about NER a requirement\n",
    "    for entity, sentence in named_entities:\n",
    "        prompt = generate_prompt(system=system, user=user_ner(node.text, entity, NUM_QUESTIONS_PER_SENTENCE))\n",
    "        gen = generate_output(prompt=prompt, temperature=TEMPERATURE, max_new_tokens=MAX_NEW_TOKENS)\n",
    "        candidates = parse_output(original_question=node.text, prompt=prompt, output=gen, num_paraphrases=NUM_QUESTIONS_PER_SENTENCE)\n",
    "        for candidate_idx, candidate in enumerate(candidates):\n",
    "            key = str(time.time()).replace(\".\", \"\")\n",
    "            cleaned_candidate = candidate.replace(f\"{candidate_idx+1}.\", \"\").strip()\n",
    "            all_generations[cleaned_candidate] = {\n",
    "                \"context\": \"ner\",\n",
    "                \"entity\": entity,\n",
    "                \"dialog_node_key\": node.key,\n",
    "                \"key\": key,\n",
    "                \"text\": cleaned_candidate\n",
    "            }\n",
    "\n",
    "    # Generate questions with whole context\n",
    "    num_node_level_questions = max(NUM_QUESTIONS_PER_SENTENCE, NUM_QUESTIONS - len(named_entities) * NUM_QUESTIONS_PER_SENTENCE)\n",
    "    prompt = generate_prompt(system=system, user=user(node.text, num_node_level_questions))\n",
    "    gen = generate_output(prompt=prompt, temperature=TEMPERATURE, max_new_tokens=MAX_NEW_TOKENS)\n",
    "    candidates = parse_output(original_question=node.text, prompt=prompt, output=gen, num_paraphrases=NUM_QUESTIONS_PER_SENTENCE)\n",
    "    for candidate_idx, candidate in enumerate(candidates):\n",
    "        key = str(time.time()).replace(\".\", \"\")\n",
    "        cleaned_candidate = candidate.replace(f\"{candidate_idx+1}.\", \"\").strip()\n",
    "        all_generations[cleaned_candidate] = {\n",
    "            \"context\": \"node\",\n",
    "            \"dialog_node_key\": node.key,\n",
    "            \"key\": key,\n",
    "            \"text\": cleaned_candidate\n",
    "        }\n",
    "    \n",
    "    # add filtered questions to generated dataset\n",
    "    for text in all_generations:\n",
    "        entry = all_generations[text]\n",
    "        generated_data[entry[\"key\"]] = entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "cleaned_data = {}\n",
    "for key in generated_data:\n",
    "    node = human_data_train.nodes_by_key[generated_data[key]['dialog_node_key']]\n",
    "    cleaned_data[key] = generated_data[key]\n",
    "    for i in range (1, NUM_QUESTIONS+1):\n",
    "        cleaned_data[key]['text'] = cleaned_data[key]['text'].replace(f\"{i}.\", \"\").strip()\n",
    "    cleaned_data[key][\"node_text\"] = node.text\n",
    "    cleaned_data[key][\"node_type\"] = node.node_type.value\n",
    "\n",
    "with open(\"resources/en/generated/train_questions_v3.json\", \"w\") as f:\n",
    "    json.dump(cleaned_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
