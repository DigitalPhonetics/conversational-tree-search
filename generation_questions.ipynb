{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from typing import List\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:0'\n",
    "import torch\n",
    "a= torch.zeros(1,1,device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !GITHUB_ACTIONS=true pip install auto-gptq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs/scratch/users/vaethdk/adviser_reisekosten/.env/lib64/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"TheBloke/upstage-llama-30b-instruct-2048-GPTQ\"\n",
    "model_basename = \"gptq_model-4bit--1g\"\n",
    "\n",
    "use_triton = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\n",
    "                                          use_fast=True,\n",
    "                                          cache_dir=\"/mount/arbeitsdaten/asr-2/vaethdk/resources/weights/\",)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension not installed.\n",
      "The safetensors archive passed at /mount/arbeitsdaten/asr-2/vaethdk/resources/weights/TheBloke--upstage-llama-30b-instruct-2048-GPTQ/gptq_model-4bit-32g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n",
      "skip module injection for FusedLlamaMLPForQuantizedModel not support integrate without triton yet.\n"
     ]
    }
   ],
   "source": [
    "model = AutoGPTQForCausalLM.from_quantized(\"/mount/arbeitsdaten/asr-2/vaethdk/resources/weights/TheBloke--upstage-llama-30b-instruct-2048-GPTQ\",\n",
    "        # model_basename=model_basename,\n",
    "        # revision=\"gptq-4bit-32g-actorder_True\",\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=False,\n",
    "        device=\"cuda:0\",\n",
    "        use_triton=use_triton,\n",
    "        quantize_config=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System:\n",
    "{System}\n",
    "\n",
    "### User:\n",
    "{User}\n",
    "\n",
    "### Assistant:\n",
    "{Assistant}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> \n",
      "### System:\n",
      "You are a helpful assistant creating a list of FAQ-style questions from given facts.\n",
      "Only generate questions that can be answered by the given facts, without any external knowledge.\n",
      "Remove information from the generated questions: countries, cities, named entities and other nouns.\n",
      "Use casual language.\n",
      "Order the generated paraphrases in a numbered list.\n",
      "\n",
      "### User:\n",
      "Generate 10 FAQ-style questions from the fact: \"In the US, you are entitled to 30$ per day, minus any free meals which you choose to decline.\"\n",
      "\n",
      "### Assistant:\n",
      "1. What is the daily allowance for meals in the US?\n",
      "2. Can you receive more than 30$ per day for meals in the US?\n",
      "3. Are you allowed to decline free meals in the US?\n",
      "4. How does declining free meals affect your daily meal allowance in the US?\n",
      "5. Is there a specific amount you can receive for meals in the US?\n",
      "6. Can you receive less than 30$ per day for meals in the US?\n",
      "7. How does the daily meal allowance work in the US?\n",
      "8. Is there a limit to the number of meals you can receive in the US?\n",
      "9. Can you receive more than one meal per day in the US?\n",
      "10. How does declining free meals affect your daily meal allowance in the US?</s>\n"
     ]
    }
   ],
   "source": [
    "system = \"\"\"You are a helpful assistant creating a list of FAQ-style questions from given facts.\n",
    "Only generate questions that can be answered by the given facts, without any external knowledge.\n",
    "Remove some information, especially nouns and named entities, between generated questions.\n",
    "Use casual language.\n",
    "Order the generated paraphrases in a numbered list.\"\"\"\n",
    "user = 'Generate 10 FAQ-style questions from the fact: \"In the US, you are entitled to 30$ per day, minus any free meals which you choose to decline.\"'\n",
    "\n",
    "\n",
    "prompt = f\"\"\"\n",
    "### System:\n",
    "{system}\n",
    "\n",
    "### User:\n",
    "{user}\n",
    "\n",
    "### Assistant:\"\"\"\n",
    "\n",
    "set_seed(42)\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Question Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset import GraphDataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(system: str, user: str) -> str:\n",
    "    return f\"\"\"\n",
    "    ### System:\n",
    "    {system}\n",
    "\n",
    "    ### User:\n",
    "    {user}\n",
    "\n",
    "    ### Assistant:\"\"\"\n",
    "\n",
    "def generate_output(prompt: str, temperature: float = 0.7, max_new_tokens: int = 512) -> torch.FloatTensor:\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\n",
    "    output = model.generate(inputs=input_ids, temperature=temperature, max_new_tokens=max_new_tokens)\n",
    "    return tokenizer.decode(output[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- not using synonyms\n",
      "===== Dataset Statistics =====\n",
      "- files:  resources/en/train_graph.json resources/en/train_answers.json\n",
      "- synonyms: False\n",
      "- depth: 20  - degree: 13\n",
      "- answers: 73\n",
      "- questions: 279\n"
     ]
    }
   ],
   "source": [
    "train = GraphDataset('resources/en/train_graph.json', 'resources/en/train_answers.json', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that we don't have any answer synonyms\n",
    "for node in train.node_list:\n",
    "    for question in node.questions:\n",
    "        train.question_list.remove(question)\n",
    "        del train.questions_by_key[question.key]\n",
    "    node.questions.clear()\n",
    "assert len(train.question_list) == 0\n",
    "assert len(train.questions_by_key) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_output(original_question: str, prompt: str, output: str, num_paraphrases: int) -> List[str]:\n",
    "    # remove prompt from output first (ends at ### ASSISTANT: )\n",
    "    questions = []\n",
    "    cleaned = output[len(prompt):]\n",
    "    \n",
    "    if not \"1.\" in cleaned: \n",
    "        print(\"NO LIST FOR QUESTION\", original_question)\n",
    "        return questions\n",
    "    \n",
    "    for i in range(1, num_paraphrases+1):\n",
    "        if not f\"{i}.\" in cleaned: \n",
    "            print(f\" - NO {i}. CANDIDATE FOR QUESTION\", original_question)\n",
    "            continue\n",
    "\n",
    "        start_idx = cleaned.find(f\"{i}.\") # find i. line\n",
    "        end_idx = cleaned.find(\"\\n\", start_idx) # read until line end \n",
    "        if i == num_paraphrases and end_idx == -1:\n",
    "            # last line might not have line break\n",
    "            end_idx = len(cleaned)\n",
    "        if start_idx == -1 or end_idx == -1:\n",
    "            print(f\" - INDEX PROBLEM FOR {i}. CANDIDATE: ({start_idx}, {end_idx})\")\n",
    "            continue\n",
    "        # parse answer\n",
    "        questions.append(cleaned[start_idx:end_idx].replace(\"</s>\").strip())\n",
    "\n",
    "        cleaned = cleaned[end_idx:] # remove i. line\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 18/123 [56:29<4:41:16, 160.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - INDEX PROBLEM FOR 3. CANDIDATE: (1, -1)\n",
      " - NO 4. CANDIDATE FOR QUESTION What would you like to know about? Information Before the start of a Business trip An Emergency during your Business trip Information about the Country risk categories\n",
      " - NO 5. CANDIDATE FOR QUESTION What would you like to know about? Information Before the start of a Business trip An Emergency during your Business trip Information about the Country risk categories\n",
      " - NO 6. CANDIDATE FOR QUESTION What would you like to know about? Information Before the start of a Business trip An Emergency during your Business trip Information about the Country risk categories\n",
      " - NO 7. CANDIDATE FOR QUESTION What would you like to know about? Information Before the start of a Business trip An Emergency during your Business trip Information about the Country risk categories\n",
      " - NO 8. CANDIDATE FOR QUESTION What would you like to know about? Information Before the start of a Business trip An Emergency during your Business trip Information about the Country risk categories\n",
      " - NO 9. CANDIDATE FOR QUESTION What would you like to know about? Information Before the start of a Business trip An Emergency during your Business trip Information about the Country risk categories\n",
      " - NO 10. CANDIDATE FOR QUESTION What would you like to know about? Information Before the start of a Business trip An Emergency during your Business trip Information about the Country risk categories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 123/123 [7:02:31<00:00, 206.11s/it]  \n"
     ]
    }
   ],
   "source": [
    "from data.dataset import NodeType, Question\n",
    "import time\n",
    "\n",
    "system = \"\"\"You are a helpful assistant creating a list of FAQ-style questions from given facts.\n",
    "Only generate questions that can be answered by the given facts, without any external knowledge.\n",
    "Remove some information, especially nouns and named entities, between generated questions.\n",
    "Use casual language.\n",
    "Order the generated paraphrases in a numbered list.\"\"\"\n",
    "\n",
    "def user(answer_text: str, num_paraphrases: int) -> str:\n",
    "    return f'Generate {num_paraphrases} FAQ-style questions from the fact: \"{answer_text}\"'\n",
    "\n",
    "NUM_QUESTIONS = 10\n",
    "TEMPERATURE = 0.7\n",
    "MAX_NEW_TOKENS = 1024\n",
    "generated_data = {}\n",
    "\n",
    "for node in tqdm(train.node_list):\n",
    "    if node.node_type in [NodeType.INFO, NodeType.QUESTION]:\n",
    "        prompt = generate_prompt(system=system, user=user(node.text, NUM_QUESTIONS))\n",
    "        gen = generate_output(prompt=prompt, temperature=TEMPERATURE, max_new_tokens=MAX_NEW_TOKENS)\n",
    "        candidates = parse_output(original_question=node.text, prompt=prompt, output=gen, num_paraphrases=NUM_QUESTIONS)\n",
    "        for candidate in candidates:\n",
    "            key = str(time.time()).replace(\".\", \"\")\n",
    "            generated_data[key] = {\n",
    "                \"dialog_node_key\": node.key,\n",
    "                \"key\": key,\n",
    "                \"text\": candidate,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "cleaned_data = {}\n",
    "for key in generated_data:\n",
    "    node = train.nodes_by_key[generated_data[key]['dialog_node_key']]\n",
    "    cleaned_data[key] = generated_data[key]\n",
    "    for i in range (1, NUM_QUESTIONS+1):\n",
    "        cleaned_data[key]['text'] = cleaned_data[key]['text'].replace(f\"{i}.\", \"\").strip()\n",
    "    cleaned_data[key][\"node_text\"] = node.text\n",
    "    cleaned_data[key][\"node_type\"] = node.node_type.value\n",
    "\n",
    "with open(\"resources/en/generated//train_questions.json\", \"w\") as f:\n",
    "    json.dump(cleaned_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
